{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "## Project: Write an Algorithm for a Dog Identification App \n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, some template code has already been provided for you, and you will need to implement additional functionality to successfully complete this project. You will not need to modify the included code beyond what is requested. Sections that begin with **'(IMPLEMENTATION)'** in the header indicate that the following block of code will require additional functionality which you must provide. Instructions will be provided for each section, and the specifics of the implementation are marked in the code block with a 'TODO' statement. Please be sure to read the instructions carefully! \n",
    "\n",
    "> **Note**: Once you have completed all of the code implementations, you need to finalize your work by exporting the Jupyter Notebook as an HTML document. Before exporting the notebook to html, all of the code cells need to have been run so that reviewers can see the final implementation and output. You can then export the notebook by using the menu above and navigating to **File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission.\n",
    "\n",
    "In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a **'Question X'** header. Carefully read each question and provide thorough answers in the following text boxes that begin with **'Answer:'**. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide.\n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut.  Markdown cells can be edited by double-clicking the cell to enter edit mode.\n",
    "\n",
    "The rubric contains _optional_ \"Stand Out Suggestions\" for enhancing the project beyond the minimum requirements. If you decide to pursue the \"Stand Out Suggestions\", you should include the code in this Jupyter notebook.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "### Why We're Here \n",
    "\n",
    "In this notebook, you will make the first steps towards developing an algorithm that could be used as part of a mobile or web app.  At the end of this project, your code will accept any user-supplied image as input.  If a dog is detected in the image, it will provide an estimate of the dog's breed.  If a human is detected, it will provide an estimate of the dog breed that is most resembling.  The image below displays potential sample output of your finished project (... but we expect that each student's algorithm will behave differently!). \n",
    "\n",
    "![Sample Dog Output](images/sample_dog_output.png)\n",
    "\n",
    "In this real-world setting, you will need to piece together a series of models to perform different tasks; for instance, the algorithm that detects humans in an image will be different from the CNN that infers dog breed.  There are many points of possible failure, and no perfect algorithm exists.  Your imperfect solution will nonetheless create a fun user experience!\n",
    "\n",
    "### The Road Ahead\n",
    "\n",
    "We break the notebook into separate steps.  Feel free to use the links below to navigate the notebook.\n",
    "\n",
    "* [Step 0](#step0): Import Datasets\n",
    "* [Step 1](#step1): Detect Humans\n",
    "* [Step 2](#step2): Detect Dogs\n",
    "* [Step 3](#step3): Create a CNN to Classify Dog Breeds (from Scratch)\n",
    "* [Step 4](#step4): Create a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "* [Step 5](#step5): Write your Algorithm\n",
    "* [Step 6](#step6): Test Your Algorithm\n",
    "\n",
    "---\n",
    "<a id='step0'></a>\n",
    "## Step 0: Import Datasets\n",
    "\n",
    "Make sure that you've downloaded the required human and dog datasets:\n",
    "\n",
    "**Note: if you are using the Udacity workspace, you *DO NOT* need to re-download these - they can be found in the `/data` folder as noted in the cell below.**\n",
    "\n",
    "* Download the [dog dataset](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip).  Unzip the folder and place it in this project's home directory, at the location `/dog_images`. \n",
    "\n",
    "* Download the [human dataset](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/lfw.zip).  Unzip the folder and place it in the home directory, at location `/lfw`.  \n",
    "\n",
    "*Note: If you are using a Windows machine, you are encouraged to use [7zip](http://www.7-zip.org/) to extract the folder.*\n",
    "\n",
    "In the code cell below, we save the file paths for both the human (LFW) dataset and dog dataset in the numpy arrays `human_files` and `dog_files`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13233 total human images.\n",
      "There are 8351 total dog images.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# load filenames for human and dog images\n",
    "human_files = np.array(glob(\"lfw/*/*\"))\n",
    "dog_files = np.array(glob(\"dogImages/*/*/*\"))\n",
    "\n",
    "# print number of images in each dataset\n",
    "print('There are %d total human images.' % len(human_files))\n",
    "print('There are %d total dog images.' % len(dog_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "## Step 1: Detect Humans\n",
    "\n",
    "In this section, we use OpenCV's implementation of [Haar feature-based cascade classifiers](http://docs.opencv.org/trunk/d7/d8b/tutorial_py_face_detection.html) to detect human faces in images.  \n",
    "\n",
    "OpenCV provides many pre-trained face detectors, stored as XML files on [github](https://github.com/opencv/opencv/tree/master/data/haarcascades).  We have downloaded one of these detectors and stored it in the `haarcascades` directory.  In the next code cell, we demonstrate how to use this detector to find human faces in a sample image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of faces detected: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2                \n",
    "import matplotlib.pyplot as plt                        \n",
    "%matplotlib inline                               \n",
    "\n",
    "# extract pre-trained face detector\n",
    "face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_alt.xml')\n",
    "\n",
    "# load color (BGR) image\n",
    "img = cv2.imread(human_files[0])\n",
    "# convert BGR image to grayscale\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# find faces in image\n",
    "faces = face_cascade.detectMultiScale(gray)\n",
    "\n",
    "# print number of faces detected in the image\n",
    "print('Number of faces detected:', len(faces))\n",
    "\n",
    "# get bounding box for each detected face\n",
    "for (x,y,w,h) in faces:\n",
    "    # add bounding box to color image\n",
    "    cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    \n",
    "# convert BGR image to RGB for plotting\n",
    "cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# display the image, along with bounding box\n",
    "plt.imshow(cv_rgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using any of the face detectors, it is standard procedure to convert the images to grayscale.  The `detectMultiScale` function executes the classifier stored in `face_cascade` and takes the grayscale image as a parameter.  \n",
    "\n",
    "In the above code, `faces` is a numpy array of detected faces, where each row corresponds to a detected face.  Each detected face is a 1D array with four entries that specifies the bounding box of the detected face.  The first two entries in the array (extracted in the above code as `x` and `y`) specify the horizontal and vertical positions of the top left corner of the bounding box.  The last two entries in the array (extracted here as `w` and `h`) specify the width and height of the box.\n",
    "\n",
    "### Write a Human Face Detector\n",
    "\n",
    "We can use this procedure to write a function that returns `True` if a human face is detected in an image and `False` otherwise.  This function, aptly named `face_detector`, takes a string-valued file path to an image as input and appears in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns \"True\" if face is detected in image stored at img_path\n",
    "def face_detector(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray)\n",
    "    return len(faces) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Assess the Human Face Detector\n",
    "\n",
    "__Question 1:__ Use the code cell below to test the performance of the `face_detector` function.  \n",
    "- What percentage of the first 100 images in `human_files` have a detected human face?  \n",
    "- What percentage of the first 100 images in `dog_files` have a detected human face? \n",
    "\n",
    "Ideally, we would like 100% of human images with a detected face and 0% of dog images with a detected face.  You will see that our algorithm falls short of this goal, but still gives acceptable performance.  We extract the file paths for the first 100 images from each of the datasets and store them in the numpy arrays `human_files_short` and `dog_files_short`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__ \n",
    "- Human Faces detected in human images  97.0 %\n",
    "- Human Faces detected in dog images    18.0 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human Faces detected in human images  97.0 %\n",
      "Human Faces detected in dog images    18.0 %\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "human_files_short = human_files[:100]\n",
    "dog_files_short = dog_files[:100]\n",
    "\n",
    "#-#-# Do NOT modify the code above this line. #-#-#\n",
    "\n",
    "## TODO: Test the performance of the face_detector algorithm \n",
    "## on the images in human_files_short and dog_files_short.\n",
    "\n",
    "face_count = 0\n",
    "dog_count  = 0\n",
    "for i in range(len(human_files_short)):\n",
    "    if face_detector(human_files_short[i]):\n",
    "        face_count += 1\n",
    "    if face_detector(dog_files_short[i]):\n",
    "        dog_count  += 1\n",
    "        \n",
    "print (\"Human Faces detected in human images \", face_count/len(human_files_short)*100.0, \"%\")\n",
    "print (\"Human Faces detected in dog images   \", dog_count/len(dog_files_short)*100.0, \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We suggest the face detector from OpenCV as a potential way to detect human images in your algorithm, but you are free to explore other approaches, especially approaches that make use of deep learning :).  Please use the code cell below to design and test your own face detection algorithm.  If you decide to pursue this _optional_ task, report performance on `human_files_short` and `dog_files_short`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### (Optional) \n",
    "### TODO: Test performance of anotherface detection algorithm.\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step2'></a>\n",
    "## Step 2: Detect Dogs\n",
    "\n",
    "In this section, we use a [pre-trained model](http://pytorch.org/docs/master/torchvision/models.html) to detect dogs in images.  \n",
    "\n",
    "### Obtain Pre-trained VGG-16 Model\n",
    "\n",
    "The code cell below downloads the VGG-16 model, along with weights that have been trained on [ImageNet](http://www.image-net.org/), a very large, very popular dataset used for image classification and other vision tasks.  ImageNet contains over 10 million URLs, each linking to an image containing an object from one of [1000 categories](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "# define VGG16 model\n",
    "VGG16 = models.vgg16(pretrained=True)\n",
    "\n",
    "# check if CUDA is available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# move model to GPU if CUDA is available\n",
    "if use_cuda:\n",
    "    VGG16 = VGG16.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an image, this pre-trained VGG-16 model returns a prediction (derived from the 1000 possible categories in ImageNet) for the object that is contained in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Making Predictions with a Pre-trained Model\n",
    "\n",
    "In the next code cell, you will write a function that accepts a path to an image (such as `'dogImages/train/001.Affenpinscher/Affenpinscher_00001.jpg'`) as input and returns the index corresponding to the ImageNet class that is predicted by the pre-trained VGG-16 model.  The output should always be an integer between 0 and 999, inclusive.\n",
    "\n",
    "Before writing the function, make sure that you take the time to learn  how to appropriately pre-process tensors for pre-trained models in the [PyTorch documentation](http://pytorch.org/docs/stable/torchvision/models.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "maxsize = 224\n",
    "\n",
    "def VGG16_predict(img_path):\n",
    "    '''\n",
    "    Use pre-trained VGG-16 model to obtain index corresponding to \n",
    "    predicted ImageNet class for image at specified path\n",
    "    \n",
    "    Args:\n",
    "        img_path: path to an image\n",
    "        \n",
    "    Returns:\n",
    "        Index corresponding to VGG-16 model's prediction\n",
    "    '''\n",
    "    \n",
    "    ## TODO: Complete the function.\n",
    "    ## Load and pre-process an image from the given img_path\n",
    "    ## Return the *index* of the predicted class for that image\n",
    "    \n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    if max(image.size) > maxsize:\n",
    "        size = maxsize\n",
    "    else:\n",
    "        size = max(image.size)\n",
    "        \n",
    "    in_transform = transforms.Compose([\n",
    "                        transforms.RandomResizedCrop(size),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                                             (0.229, 0.224, 0.225))])\n",
    "        \n",
    "    image = in_transform(image)[:3,:,:].unsqueeze(0)\n",
    "    if (use_cuda):\n",
    "        image = image.to(\"cuda\")\n",
    "    \n",
    "    probabilities = VGG16(image)\n",
    "    \n",
    "    index = torch.max(probabilities,1)\n",
    "    \n",
    "    return index[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Write a Dog Detector\n",
    "\n",
    "While looking at the [dictionary](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a), you will notice that the categories corresponding to dogs appear in an uninterrupted sequence and correspond to dictionary keys 151-268, inclusive, to include all categories from `'Chihuahua'` to `'Mexican hairless'`.  Thus, in order to check to see if an image is predicted to contain a dog by the pre-trained VGG-16 model, we need only check if the pre-trained model predicts an index between 151 and 268 (inclusive).\n",
    "\n",
    "Use these ideas to complete the `dog_detector` function below, which returns `True` if a dog is detected in an image (and `False` if not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### returns \"True\" if a dog is detected in the image stored at img_path\n",
    "def dog_detector(img_path):\n",
    "    ## TODO: Complete the function.\n",
    "    \n",
    "    index = VGG16_predict(img_path)\n",
    "    \n",
    "    return ((index >= 151) and (index <= 268))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Assess the Dog Detector\n",
    "\n",
    "__Question 2:__ Use the code cell below to test the performance of your `dog_detector` function.  \n",
    "- What percentage of the images in `human_files_short` have a detected dog?  \n",
    "- What percentage of the images in `dog_files_short` have a detected dog?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__ \n",
    "- Dog Faces detected in human images  0.0 %\n",
    "- Dog Faces detected in dog images    96.0 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dog Faces detected in human images  0.0 %\n",
      "Dog Faces detected in dog images    91.0 %\n"
     ]
    }
   ],
   "source": [
    "### TODO: Test the performance of the dog_detector function\n",
    "### on the images in human_files_short and dog_files_short.\n",
    "\n",
    "face_count = 0\n",
    "dog_count  = 0\n",
    "for i in range(len(human_files_short)):\n",
    "    if dog_detector(human_files_short[i]):\n",
    "        face_count += 1\n",
    "    if dog_detector(dog_files_short[i]):\n",
    "        dog_count  += 1\n",
    "        \n",
    "print (\"Dog Faces detected in human images \", face_count/len(human_files_short)*100.0, \"%\")\n",
    "print (\"Dog Faces detected in dog images   \", dog_count/len(dog_files_short)*100.0, \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We suggest VGG-16 as a potential network to detect dog images in your algorithm, but you are free to explore other pre-trained networks (such as [Inception-v3](http://pytorch.org/docs/master/torchvision/models.html#inception-v3), [ResNet-50](http://pytorch.org/docs/master/torchvision/models.html#id3), etc).  Please use the code cell below to test other pre-trained PyTorch models.  If you decide to pursue this _optional_ task, report performance on `human_files_short` and `dog_files_short`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### (Optional) \n",
    "### TODO: Report the performance of another pre-trained network.\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step3'></a>\n",
    "## Step 3: Create a CNN to Classify Dog Breeds (from Scratch)\n",
    "\n",
    "Now that we have functions for detecting humans and dogs in images, we need a way to predict breed from images.  In this step, you will create a CNN that classifies dog breeds.  You must create your CNN _from scratch_ (so, you can't use transfer learning _yet_!), and you must attain a test accuracy of at least 10%.  In Step 4 of this notebook, you will have the opportunity to use transfer learning to create a CNN that attains greatly improved accuracy.\n",
    "\n",
    "We mention that the task of assigning breed to dogs from images is considered exceptionally challenging.  To see why, consider that *even a human* would have trouble distinguishing between a Brittany and a Welsh Springer Spaniel.  \n",
    "\n",
    "Brittany | Welsh Springer Spaniel\n",
    "- | - \n",
    "<img src=\"images/Brittany_02625.jpg\" width=\"100\"> | <img src=\"images/Welsh_springer_spaniel_08203.jpg\" width=\"200\">\n",
    "\n",
    "It is not difficult to find other dog breed pairs with minimal inter-class variation (for instance, Curly-Coated Retrievers and American Water Spaniels).  \n",
    "\n",
    "Curly-Coated Retriever | American Water Spaniel\n",
    "- | -\n",
    "<img src=\"images/Curly-coated_retriever_03896.jpg\" width=\"200\"> | <img src=\"images/American_water_spaniel_00648.jpg\" width=\"200\">\n",
    "\n",
    "\n",
    "Likewise, recall that labradors come in yellow, chocolate, and black.  Your vision-based algorithm will have to conquer this high intra-class variation to determine how to classify all of these different shades as the same breed.  \n",
    "\n",
    "Yellow Labrador | Chocolate Labrador | Black Labrador\n",
    "- | -\n",
    "<img src=\"images/Labrador_retriever_06457.jpg\" width=\"150\"> | <img src=\"images/Labrador_retriever_06455.jpg\" width=\"240\"> | <img src=\"images/Labrador_retriever_06449.jpg\" width=\"220\">\n",
    "\n",
    "We also mention that random chance presents an exceptionally low bar: setting aside the fact that the classes are slightly imabalanced, a random guess will provide a correct answer roughly 1 in 133 times, which corresponds to an accuracy of less than 1%.  \n",
    "\n",
    "Remember that the practice is far ahead of the theory in deep learning.  Experiment with many different architectures, and trust your intuition.  And, of course, have fun!\n",
    "\n",
    "### (IMPLEMENTATION) Specify Data Loaders for the Dog Dataset\n",
    "\n",
    "Use the code cell below to write three separate [data loaders](http://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for the training, validation, and test datasets of dog images (located at `dog_images/train`, `dog_images/valid`, and `dog_images/test`, respectively).  You may find [this documentation on custom datasets](http://pytorch.org/docs/stable/torchvision/datasets.html) to be a useful resource.  If you are interested in augmenting your training and/or validation data, check out the wide variety of [transforms](http://pytorch.org/docs/stable/torchvision/transforms.html?highlight=transform)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on device  cuda \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "### TODO: Write data loaders for training, validation, and test sets\n",
    "## Specify appropriate transforms, and batch_sizes\n",
    "        \n",
    "in_transform = transforms.Compose([\n",
    "                    transforms.RandomAffine(10,translate=(0.2,0.2)),\n",
    "                    transforms.RandomCrop(256,pad_if_needed=True,padding_mode='edge'),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "train_data = \"/data/dog_images/train\"\n",
    "valid_data = \"/data/dog_images/valid\"\n",
    "test_data  = \"/data/dog_images/test\"\n",
    "\n",
    "train_loader = datasets.ImageFolder(train_data, transform=in_transform)\n",
    "valid_loader = datasets.ImageFolder(train_data, transform=in_transform)\n",
    "test_loader  = datasets.ImageFolder(test_data,  transform=in_transform)\n",
    "\n",
    "loaders_scratch = {'train': train_loader, 'valid': valid_loader, 'test': test_loader}\n",
    "\n",
    "# check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print (\"running on device \", device, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:** Describe your chosen procedure for preprocessing the data. \n",
    "- How does your code resize the images (by cropping, stretching, etc)?  What size did you pick for the input tensor, and why?\n",
    "- Did you decide to augment the dataset?  If so, how (through translations, flips, rotations, etc)?  If not, why not?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:\n",
    "- Code crops images to 256x256, close to the size recommended for VDD16. Normalize to same values as VDD16.  They seemed like good values.  Won't be too big for excessive computation time, yet still big enough for detail.  It's a power of 2 so sould scale well in the convolutional layers.  Normalization will keep values around -1 to 1 and should help with training.\n",
    "- Added Random Affine to transforms.  This should help with training since it will randomly rotate/translate the picture to minimize local minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Model Architecture\n",
    "\n",
    "Create a CNN to classify dog breed.  Use the template in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define the CNN architecture\n",
    "class Net(nn.Module):\n",
    "    ### TODO: choose an architecture, and complete the class\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        ## Define layers of a CNN\n",
    "        self.conv1 = nn.Conv2d(3,  16, 3, padding=1)   # 3x256x256  => 256x256x16\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)   # 128x128x16 => 128x128x32\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)   # 64x64x32   => 64x64x64\n",
    "       \n",
    "        self.pool = nn.MaxPool2d(2, 2)   # max pooling layer, divide by two\n",
    "        \n",
    "        # output layers\n",
    "        self.out1 = nn.Linear(32*32*64, 500) # 32x32x64 => 500\n",
    "        self.out2 = nn.Linear(500, 133)      # 500      => 133\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ## Define forward behavior\n",
    "        # add sequence of convolutional and max pooling layers\n",
    "        x = x.view(1, 3, 256, 256)\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        # flatten image input\n",
    "        x = x.view(-1, 32*32*64)\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add 1st hidden layer, with relu activation function\n",
    "        x = F.relu(self.out1(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add 2nd hidden layer, with relu activation function\n",
    "        x = self.out2(x)\n",
    "        return x\n",
    "\n",
    "#-#-# You so NOT have to modify the code below this line. #-#-#\n",
    "## Yes, I do if I want to be able to understand cuda vs cpu ##\n",
    "\n",
    "# instantiate the CNN\n",
    "model_scratch = Net().to(device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 4:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__ \n",
    "- Wanted to increase the number of features in each convolutional layer as the pooling decreases the image size.  This should help distinguish between the 118 different dog breed types.\n",
    "- In the fully connected layers, I wanted to have more outputs than inputs, with a final result os 118 for the dog breeds.\n",
    "- Copied some of the above variables down here so I would not have to rerun everything each time I switch between cpu and cuda, or when I start again the next day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Specify Loss Function and Optimizer\n",
    "\n",
    "Use the next code cell to specify a [loss function](http://pytorch.org/docs/stable/nn.html#loss-functions) and [optimizer](http://pytorch.org/docs/stable/optim.html).  Save the chosen loss function as `criterion_scratch`, and the optimizer as `optimizer_scratch` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "### TODO: select loss function\n",
    "criterion_scratch = nn.CrossEntropyLoss().to(device=device)\n",
    "\n",
    "### TODO: select optimizer\n",
    "optimizer_scratch = optim.SGD(model_scratch.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Train and Validate the Model\n",
    "\n",
    "Train and validate your model in the code cell below.  [Save the final model parameters](http://pytorch.org/docs/master/notes/serialization.html) at filepath `'model_scratch.pt'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:     1\n",
      "batch_idx:     0, train_loss: 10.633940\n",
      "batch_idx:  1000, train_loss: 1.014638\n",
      "batch_idx:  2000, train_loss: 0.978182\n",
      "batch_idx:  3000, train_loss: 0.981838\n",
      "batch_idx:  4000, train_loss: 0.911053\n",
      "batch_idx:  5000, train_loss: 0.903607\n",
      "batch_idx:  6000, train_loss: 0.919145\n",
      "final train_loss: 0.965437\n",
      "batch_idx:     0, valid_loss: 9.761345\n",
      "batch_idx:  1000, valid_loss: 11.571132\n",
      "batch_idx:  2000, valid_loss: 13.357228\n",
      "batch_idx:  3000, valid_loss: 13.964989\n",
      "batch_idx:  4000, valid_loss: 14.291295\n",
      "batch_idx:  5000, valid_loss: 14.510395\n",
      "batch_idx:  6000, valid_loss: 14.463644\n",
      "Training Loss: 0.965437 \tValidation Loss: 14.069354\n",
      "Training loss decreased (inf --> 14.069354).  Saving model ...\n",
      "epoch:     2\n",
      "batch_idx:     0, train_loss: 9.120950\n",
      "batch_idx:  1000, train_loss: 0.736442\n",
      "batch_idx:  2000, train_loss: 0.771611\n",
      "batch_idx:  3000, train_loss: 0.849800\n",
      "batch_idx:  4000, train_loss: 0.913629\n",
      "batch_idx:  5000, train_loss: 0.964417\n",
      "batch_idx:  6000, train_loss: 0.994361\n",
      "final train_loss: 1.025812\n",
      "batch_idx:     0, valid_loss: 12.000197\n",
      "batch_idx:  1000, valid_loss: 12.512852\n",
      "batch_idx:  2000, valid_loss: 12.599704\n",
      "batch_idx:  3000, valid_loss: 12.572932\n",
      "batch_idx:  4000, valid_loss: 12.583817\n",
      "batch_idx:  5000, valid_loss: 12.545469\n",
      "batch_idx:  6000, valid_loss: 12.425238\n",
      "Training Loss: 1.025812 \tValidation Loss: 12.017447\n",
      "Training loss decreased (14.069354 --> 12.017447).  Saving model ...\n",
      "epoch:     3\n",
      "batch_idx:     0, train_loss: 15.158987\n",
      "batch_idx:  1000, train_loss: 0.846463\n",
      "batch_idx:  2000, train_loss: 0.843888\n",
      "batch_idx:  3000, train_loss: 0.806953\n",
      "batch_idx:  4000, train_loss: 0.829778\n",
      "batch_idx:  5000, train_loss: 0.891728\n",
      "batch_idx:  6000, train_loss: 0.902405\n",
      "final train_loss: 0.983868\n",
      "batch_idx:     0, valid_loss: 8.459162\n",
      "batch_idx:  1000, valid_loss: 10.168962\n",
      "batch_idx:  2000, valid_loss: 10.305519\n",
      "batch_idx:  3000, valid_loss: 10.322689\n",
      "batch_idx:  4000, valid_loss: 10.349607\n",
      "batch_idx:  5000, valid_loss: 10.350015\n",
      "batch_idx:  6000, valid_loss: 10.211919\n",
      "Training Loss: 0.983868 \tValidation Loss: 9.808920\n",
      "Training loss decreased (12.017447 --> 9.808920).  Saving model ...\n",
      "epoch:     4\n",
      "batch_idx:     0, train_loss: 10.911314\n",
      "batch_idx:  1000, train_loss: 0.844377\n",
      "batch_idx:  2000, train_loss: 0.887536\n",
      "batch_idx:  3000, train_loss: 0.980561\n",
      "batch_idx:  4000, train_loss: 1.041763\n",
      "batch_idx:  5000, train_loss: 1.147073\n",
      "batch_idx:  6000, train_loss: 1.258215\n",
      "final train_loss: 1.306977\n",
      "batch_idx:     0, valid_loss: 9.065515\n",
      "batch_idx:  1000, valid_loss: 10.039058\n",
      "batch_idx:  2000, valid_loss: 10.064499\n",
      "batch_idx:  3000, valid_loss: 10.057201\n",
      "batch_idx:  4000, valid_loss: 10.098476\n",
      "batch_idx:  5000, valid_loss: 10.047271\n",
      "batch_idx:  6000, valid_loss: 9.817361\n",
      "Training Loss: 1.306977 \tValidation Loss: 9.439337\n",
      "Training loss decreased (9.808920 --> 9.439337).  Saving model ...\n",
      "epoch:     5\n",
      "batch_idx:     0, train_loss: 11.542598\n",
      "batch_idx:  1000, train_loss: 1.380156\n",
      "batch_idx:  2000, train_loss: 1.377388\n",
      "batch_idx:  3000, train_loss: 1.313854\n",
      "batch_idx:  4000, train_loss: 1.178943\n",
      "batch_idx:  5000, train_loss: 1.144112\n",
      "batch_idx:  6000, train_loss: 1.199643\n",
      "final train_loss: 1.258813\n",
      "batch_idx:     0, valid_loss: 9.395759\n",
      "batch_idx:  1000, valid_loss: 10.020700\n",
      "batch_idx:  2000, valid_loss: 10.068546\n",
      "batch_idx:  3000, valid_loss: 10.103570\n",
      "batch_idx:  4000, valid_loss: 10.153839\n",
      "batch_idx:  5000, valid_loss: 10.153122\n",
      "batch_idx:  6000, valid_loss: 9.922606\n",
      "Training Loss: 1.258813 \tValidation Loss: 9.545573\n",
      "epoch:     6\n",
      "batch_idx:     0, train_loss: 11.838968\n",
      "batch_idx:  1000, train_loss: 1.425972\n",
      "batch_idx:  2000, train_loss: 1.452628\n",
      "batch_idx:  3000, train_loss: 1.301222\n",
      "batch_idx:  4000, train_loss: 1.215164\n",
      "batch_idx:  5000, train_loss: 1.209897\n",
      "batch_idx:  6000, train_loss: 1.105521\n",
      "final train_loss: 1.041807\n",
      "batch_idx:     0, valid_loss: 7.439349\n",
      "batch_idx:  1000, valid_loss: 13.600679\n",
      "batch_idx:  2000, valid_loss: 16.400240\n",
      "batch_idx:  3000, valid_loss: 17.773989\n",
      "batch_idx:  4000, valid_loss: 18.923172\n",
      "batch_idx:  5000, valid_loss: 19.627623\n",
      "batch_idx:  6000, valid_loss: 19.641039\n",
      "Training Loss: 1.041807 \tValidation Loss: 18.762230\n",
      "epoch:     7\n",
      "batch_idx:     0, train_loss: 7.383774\n",
      "batch_idx:  1000, train_loss: 0.337557\n",
      "batch_idx:  2000, train_loss: 0.351284\n",
      "batch_idx:  3000, train_loss: 0.347049\n",
      "batch_idx:  4000, train_loss: 0.357911\n",
      "batch_idx:  5000, train_loss: 0.365920\n",
      "batch_idx:  6000, train_loss: 0.389096\n",
      "final train_loss: 0.415688\n",
      "batch_idx:     0, valid_loss: 8.111320\n",
      "batch_idx:  1000, valid_loss: 13.135756\n",
      "batch_idx:  2000, valid_loss: 14.809037\n",
      "batch_idx:  3000, valid_loss: 15.572607\n",
      "batch_idx:  4000, valid_loss: 16.062958\n",
      "batch_idx:  5000, valid_loss: 16.389185\n",
      "batch_idx:  6000, valid_loss: 16.239475\n",
      "Training Loss: 0.415688 \tValidation Loss: 15.511027\n",
      "epoch:     8\n",
      "batch_idx:     0, train_loss: 9.936811\n",
      "batch_idx:  1000, train_loss: 0.430947\n",
      "batch_idx:  2000, train_loss: 0.459589\n",
      "batch_idx:  3000, train_loss: 0.481926\n",
      "batch_idx:  4000, train_loss: 0.501889\n",
      "batch_idx:  5000, train_loss: 0.687931\n",
      "batch_idx:  6000, train_loss: 0.796839\n",
      "final train_loss: 0.911863\n",
      "batch_idx:     0, valid_loss: 9.755730\n",
      "batch_idx:  1000, valid_loss: 10.074904\n",
      "batch_idx:  2000, valid_loss: 9.901139\n",
      "batch_idx:  3000, valid_loss: 9.970255\n",
      "batch_idx:  4000, valid_loss: 10.020506\n",
      "batch_idx:  5000, valid_loss: 10.024089\n",
      "batch_idx:  6000, valid_loss: 9.812532\n",
      "Training Loss: 0.911863 \tValidation Loss: 9.466266\n",
      "epoch:     9\n",
      "batch_idx:     0, train_loss: 4.382505\n",
      "batch_idx:  1000, train_loss: 0.849241\n",
      "batch_idx:  2000, train_loss: 1.033104\n",
      "batch_idx:  3000, train_loss: 1.180627\n",
      "batch_idx:  4000, train_loss: 1.101146\n",
      "batch_idx:  5000, train_loss: 1.037364\n",
      "batch_idx:  6000, train_loss: 0.945409\n",
      "final train_loss: 0.897833\n",
      "batch_idx:     0, valid_loss: 7.745805\n",
      "batch_idx:  1000, valid_loss: 14.727030\n",
      "batch_idx:  2000, valid_loss: 17.032907\n",
      "batch_idx:  3000, valid_loss: 18.204676\n",
      "batch_idx:  4000, valid_loss: 19.203072\n",
      "batch_idx:  5000, valid_loss: 19.850021\n",
      "batch_idx:  6000, valid_loss: 19.860031\n",
      "Training Loss: 0.897833 \tValidation Loss: 18.965424\n",
      "epoch:    10\n",
      "batch_idx:     0, train_loss: 8.200120\n",
      "batch_idx:  1000, train_loss: 0.348550\n",
      "batch_idx:  2000, train_loss: 0.363650\n",
      "batch_idx:  3000, train_loss: 0.388041\n",
      "batch_idx:  4000, train_loss: 0.383277\n",
      "batch_idx:  5000, train_loss: 0.374753\n",
      "batch_idx:  6000, train_loss: 0.380020\n",
      "final train_loss: 0.388237\n",
      "batch_idx:     0, valid_loss: 8.142816\n",
      "batch_idx:  1000, valid_loss: 14.411700\n",
      "batch_idx:  2000, valid_loss: 16.522308\n",
      "batch_idx:  3000, valid_loss: 17.740738\n",
      "batch_idx:  4000, valid_loss: 18.582706\n",
      "batch_idx:  5000, valid_loss: 19.075069\n",
      "batch_idx:  6000, valid_loss: 19.008574\n",
      "Training Loss: 0.388237 \tValidation Loss: 18.130379\n",
      "epoch:    11\n",
      "batch_idx:     0, train_loss: 9.619674\n",
      "batch_idx:  1000, train_loss: 0.360579\n",
      "batch_idx:  2000, train_loss: 0.380897\n",
      "batch_idx:  3000, train_loss: 0.389410\n",
      "batch_idx:  4000, train_loss: 0.406783\n",
      "batch_idx:  5000, train_loss: 0.414237\n",
      "batch_idx:  6000, train_loss: 0.459263\n",
      "final train_loss: 0.510312\n",
      "batch_idx:     0, valid_loss: 12.227746\n",
      "batch_idx:  1000, valid_loss: 13.517399\n",
      "batch_idx:  2000, valid_loss: 14.629452\n",
      "batch_idx:  3000, valid_loss: 15.215502\n",
      "batch_idx:  4000, valid_loss: 15.563530\n",
      "batch_idx:  5000, valid_loss: 15.719918\n",
      "batch_idx:  6000, valid_loss: 15.612285\n",
      "Training Loss: 0.510312 \tValidation Loss: 15.184542\n",
      "epoch:    12\n",
      "batch_idx:     0, train_loss: 13.249789\n",
      "batch_idx:  1000, train_loss: 0.552761\n",
      "batch_idx:  2000, train_loss: 0.547454\n",
      "batch_idx:  3000, train_loss: 0.470065\n",
      "batch_idx:  4000, train_loss: 0.432221\n",
      "batch_idx:  5000, train_loss: 0.419344\n",
      "batch_idx:  6000, train_loss: 0.419246\n",
      "final train_loss: 0.436630\n",
      "batch_idx:     0, valid_loss: 11.055817\n",
      "batch_idx:  1000, valid_loss: 15.710813\n",
      "batch_idx:  2000, valid_loss: 17.324417\n",
      "batch_idx:  3000, valid_loss: 18.399862\n",
      "batch_idx:  4000, valid_loss: 19.049816\n",
      "batch_idx:  5000, valid_loss: 19.375889\n",
      "batch_idx:  6000, valid_loss: 19.078487\n",
      "Training Loss: 0.436630 \tValidation Loss: 18.250040\n",
      "epoch:    13\n",
      "batch_idx:     0, train_loss: 6.735988\n",
      "batch_idx:  1000, train_loss: 0.432811\n",
      "batch_idx:  2000, train_loss: 0.425999\n",
      "batch_idx:  3000, train_loss: 0.454101\n",
      "batch_idx:  4000, train_loss: 0.477910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  5000, train_loss: 0.506766\n",
      "batch_idx:  6000, train_loss: 0.564415\n",
      "final train_loss: 0.599064\n",
      "batch_idx:     0, valid_loss: 11.915812\n",
      "batch_idx:  1000, valid_loss: 16.487221\n",
      "batch_idx:  2000, valid_loss: 17.483427\n",
      "batch_idx:  3000, valid_loss: 17.736103\n",
      "batch_idx:  4000, valid_loss: 18.105663\n",
      "batch_idx:  5000, valid_loss: 18.196455\n",
      "batch_idx:  6000, valid_loss: 17.867443\n",
      "Training Loss: 0.599064 \tValidation Loss: 17.182980\n",
      "epoch:    14\n",
      "batch_idx:     0, train_loss: 4.216806\n",
      "batch_idx:  1000, train_loss: 0.748145\n",
      "batch_idx:  2000, train_loss: 0.763564\n",
      "batch_idx:  3000, train_loss: 0.850661\n",
      "batch_idx:  4000, train_loss: 0.757600\n",
      "batch_idx:  5000, train_loss: 0.672688\n",
      "batch_idx:  6000, train_loss: 0.631766\n",
      "final train_loss: 0.615836\n",
      "batch_idx:     0, valid_loss: 7.259271\n",
      "batch_idx:  1000, valid_loss: 14.417112\n",
      "batch_idx:  2000, valid_loss: 17.391836\n",
      "batch_idx:  3000, valid_loss: 18.857258\n",
      "batch_idx:  4000, valid_loss: 19.718985\n",
      "batch_idx:  5000, valid_loss: 20.249964\n",
      "batch_idx:  6000, valid_loss: 20.077019\n",
      "Training Loss: 0.615836 \tValidation Loss: 19.189573\n",
      "epoch:    15\n",
      "batch_idx:     0, train_loss: 8.539822\n",
      "batch_idx:  1000, train_loss: 0.360896\n",
      "batch_idx:  2000, train_loss: 0.383666\n",
      "batch_idx:  3000, train_loss: 0.391071\n",
      "batch_idx:  4000, train_loss: 0.379744\n",
      "batch_idx:  5000, train_loss: 0.370431\n",
      "batch_idx:  6000, train_loss: 0.381281\n",
      "final train_loss: 0.395925\n",
      "batch_idx:     0, valid_loss: 7.717012\n",
      "batch_idx:  1000, valid_loss: 15.546832\n",
      "batch_idx:  2000, valid_loss: 17.900885\n",
      "batch_idx:  3000, valid_loss: 19.360964\n",
      "batch_idx:  4000, valid_loss: 20.383516\n",
      "batch_idx:  5000, valid_loss: 21.038597\n",
      "batch_idx:  6000, valid_loss: 20.878206\n",
      "Training Loss: 0.395925 \tValidation Loss: 19.932079\n",
      "epoch:    16\n",
      "batch_idx:     0, train_loss: 8.445622\n",
      "batch_idx:  1000, train_loss: 0.353266\n",
      "batch_idx:  2000, train_loss: 0.371690\n",
      "batch_idx:  3000, train_loss: 0.399636\n",
      "batch_idx:  4000, train_loss: 0.451297\n",
      "batch_idx:  5000, train_loss: 0.442765\n",
      "batch_idx:  6000, train_loss: 0.435279\n",
      "final train_loss: 0.440016\n",
      "batch_idx:     0, valid_loss: 8.537938\n",
      "batch_idx:  1000, valid_loss: 16.359432\n",
      "batch_idx:  2000, valid_loss: 19.092876\n",
      "batch_idx:  3000, valid_loss: 20.605337\n",
      "batch_idx:  4000, valid_loss: 21.674885\n",
      "batch_idx:  5000, valid_loss: 22.436125\n",
      "batch_idx:  6000, valid_loss: 22.292297\n",
      "Training Loss: 0.440016 \tValidation Loss: 21.339384\n",
      "epoch:    17\n",
      "batch_idx:     0, train_loss: 10.395422\n",
      "batch_idx:  1000, train_loss: 0.333322\n",
      "batch_idx:  2000, train_loss: 0.373652\n",
      "batch_idx:  3000, train_loss: 0.346128\n",
      "batch_idx:  4000, train_loss: 0.348210\n",
      "batch_idx:  5000, train_loss: 0.346857\n",
      "batch_idx:  6000, train_loss: 0.350220\n",
      "final train_loss: 0.362207\n",
      "batch_idx:     0, valid_loss: 6.930882\n",
      "batch_idx:  1000, valid_loss: 14.443759\n",
      "batch_idx:  2000, valid_loss: 16.930101\n",
      "batch_idx:  3000, valid_loss: 18.384922\n",
      "batch_idx:  4000, valid_loss: 19.324873\n",
      "batch_idx:  5000, valid_loss: 19.918726\n",
      "batch_idx:  6000, valid_loss: 19.840281\n",
      "Training Loss: 0.362207 \tValidation Loss: 19.018600\n",
      "epoch:    18\n",
      "batch_idx:     0, train_loss: 7.412265\n",
      "batch_idx:  1000, train_loss: 0.290539\n",
      "batch_idx:  2000, train_loss: 0.310389\n",
      "batch_idx:  3000, train_loss: 0.316073\n",
      "batch_idx:  4000, train_loss: 0.320305\n",
      "batch_idx:  5000, train_loss: 0.321064\n",
      "batch_idx:  6000, train_loss: 0.336235\n",
      "final train_loss: 0.356541\n",
      "batch_idx:     0, valid_loss: 9.287033\n",
      "batch_idx:  1000, valid_loss: 14.830501\n",
      "batch_idx:  2000, valid_loss: 17.628893\n",
      "batch_idx:  3000, valid_loss: 19.266741\n",
      "batch_idx:  4000, valid_loss: 20.292002\n",
      "batch_idx:  5000, valid_loss: 20.899979\n",
      "batch_idx:  6000, valid_loss: 20.744511\n",
      "Training Loss: 0.356541 \tValidation Loss: 19.928888\n",
      "epoch:    19\n",
      "batch_idx:     0, train_loss: 11.106829\n",
      "batch_idx:  1000, train_loss: 0.367588\n",
      "batch_idx:  2000, train_loss: 0.405858\n",
      "batch_idx:  3000, train_loss: 0.423085\n",
      "batch_idx:  4000, train_loss: 0.456981\n",
      "batch_idx:  5000, train_loss: 0.510307\n",
      "batch_idx:  6000, train_loss: 0.633436\n",
      "final train_loss: 0.702806\n",
      "batch_idx:     0, valid_loss: 12.155935\n",
      "batch_idx:  1000, valid_loss: 13.128585\n",
      "batch_idx:  2000, valid_loss: 13.270828\n",
      "batch_idx:  3000, valid_loss: 13.290483\n",
      "batch_idx:  4000, valid_loss: 13.339103\n",
      "batch_idx:  5000, valid_loss: 13.367985\n",
      "batch_idx:  6000, valid_loss: 13.372952\n",
      "Training Loss: 0.702806 \tValidation Loss: 13.122549\n",
      "epoch:    20\n",
      "batch_idx:     0, train_loss: 15.930161\n",
      "batch_idx:  1000, train_loss: 0.827979\n",
      "batch_idx:  2000, train_loss: 0.851804\n",
      "batch_idx:  3000, train_loss: 0.831399\n",
      "batch_idx:  4000, train_loss: 0.826800\n",
      "batch_idx:  5000, train_loss: 0.807027\n",
      "batch_idx:  6000, train_loss: 0.792279\n",
      "final train_loss: 0.805419\n",
      "batch_idx:     0, valid_loss: 10.163176\n",
      "batch_idx:  1000, valid_loss: 12.829297\n",
      "batch_idx:  2000, valid_loss: 15.088315\n",
      "batch_idx:  3000, valid_loss: 15.895077\n",
      "batch_idx:  4000, valid_loss: 16.266314\n",
      "batch_idx:  5000, valid_loss: 16.558130\n",
      "batch_idx:  6000, valid_loss: 16.605202\n",
      "Training Loss: 0.805419 \tValidation Loss: 16.064039\n",
      "epoch:    21\n",
      "batch_idx:     0, train_loss: 10.551292\n",
      "batch_idx:  1000, train_loss: 0.599014\n",
      "batch_idx:  2000, train_loss: 0.632634\n",
      "batch_idx:  3000, train_loss: 0.658930\n",
      "batch_idx:  4000, train_loss: 0.676889\n",
      "batch_idx:  5000, train_loss: 0.717876\n",
      "batch_idx:  6000, train_loss: 0.665635\n",
      "final train_loss: 0.645858\n",
      "batch_idx:     0, valid_loss: 6.618010\n",
      "batch_idx:  1000, valid_loss: 14.528204\n",
      "batch_idx:  2000, valid_loss: 17.180025\n",
      "batch_idx:  3000, valid_loss: 18.590252\n",
      "batch_idx:  4000, valid_loss: 19.471567\n",
      "batch_idx:  5000, valid_loss: 20.037790\n",
      "batch_idx:  6000, valid_loss: 20.034185\n",
      "Training Loss: 0.645858 \tValidation Loss: 19.225183\n",
      "epoch:    22\n",
      "batch_idx:     0, train_loss: 7.962861\n",
      "batch_idx:  1000, train_loss: 0.334687\n",
      "batch_idx:  2000, train_loss: 0.350583\n",
      "batch_idx:  3000, train_loss: 0.352335\n",
      "batch_idx:  4000, train_loss: 0.361066\n",
      "batch_idx:  5000, train_loss: 0.356944\n",
      "batch_idx:  6000, train_loss: 0.365425\n",
      "final train_loss: 0.382987\n",
      "batch_idx:     0, valid_loss: 6.776628\n",
      "batch_idx:  1000, valid_loss: 14.121125\n",
      "batch_idx:  2000, valid_loss: 17.129101\n",
      "batch_idx:  3000, valid_loss: 18.636297\n",
      "batch_idx:  4000, valid_loss: 19.709808\n",
      "batch_idx:  5000, valid_loss: 20.347330\n",
      "batch_idx:  6000, valid_loss: 20.237644\n",
      "Training Loss: 0.382987 \tValidation Loss: 19.361486\n",
      "epoch:    23\n",
      "batch_idx:     0, train_loss: 7.299997\n",
      "batch_idx:  1000, train_loss: 0.344488\n",
      "batch_idx:  2000, train_loss: 0.375339\n",
      "batch_idx:  3000, train_loss: 0.376933\n",
      "batch_idx:  4000, train_loss: 0.425071\n",
      "batch_idx:  5000, train_loss: 0.504315\n",
      "batch_idx:  6000, train_loss: 0.586769\n",
      "final train_loss: 0.637046\n",
      "batch_idx:     0, valid_loss: 11.984936\n",
      "batch_idx:  1000, valid_loss: 13.583502\n",
      "batch_idx:  2000, valid_loss: 13.973845\n",
      "batch_idx:  3000, valid_loss: 14.171697\n",
      "batch_idx:  4000, valid_loss: 14.274082\n",
      "batch_idx:  5000, valid_loss: 14.317289\n",
      "batch_idx:  6000, valid_loss: 14.372498\n",
      "Training Loss: 0.637046 \tValidation Loss: 13.953809\n",
      "epoch:    24\n",
      "batch_idx:     0, train_loss: 15.372928\n",
      "batch_idx:  1000, train_loss: 0.953598\n",
      "batch_idx:  2000, train_loss: 0.996860\n",
      "batch_idx:  3000, train_loss: 0.955469\n",
      "batch_idx:  4000, train_loss: 0.933269\n",
      "batch_idx:  5000, train_loss: 0.911192\n",
      "batch_idx:  6000, train_loss: 0.878198\n",
      "final train_loss: 0.833474\n",
      "batch_idx:     0, valid_loss: 6.934463\n",
      "batch_idx:  1000, valid_loss: 13.745331\n",
      "batch_idx:  2000, valid_loss: 16.895552\n",
      "batch_idx:  3000, valid_loss: 18.276531\n",
      "batch_idx:  4000, valid_loss: 19.242449\n",
      "batch_idx:  5000, valid_loss: 19.889032\n",
      "batch_idx:  6000, valid_loss: 20.020952\n",
      "Training Loss: 0.833474 \tValidation Loss: 19.271273\n",
      "epoch:    25\n",
      "batch_idx:     0, train_loss: 8.418606\n",
      "batch_idx:  1000, train_loss: 0.308703\n",
      "batch_idx:  2000, train_loss: 0.337379\n",
      "batch_idx:  3000, train_loss: 0.344971\n",
      "batch_idx:  4000, train_loss: 0.336057\n",
      "batch_idx:  5000, train_loss: 0.342426\n",
      "batch_idx:  6000, train_loss: 0.349219\n",
      "final train_loss: 0.356317\n",
      "batch_idx:     0, valid_loss: 6.530585\n",
      "batch_idx:  1000, valid_loss: 13.367184\n",
      "batch_idx:  2000, valid_loss: 16.446718\n",
      "batch_idx:  3000, valid_loss: 17.942999\n",
      "batch_idx:  4000, valid_loss: 18.913160\n",
      "batch_idx:  5000, valid_loss: 19.563520\n",
      "batch_idx:  6000, valid_loss: 19.696087\n",
      "Training Loss: 0.356317 \tValidation Loss: 18.941183\n",
      "epoch:    26\n",
      "batch_idx:     0, train_loss: 4.617036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  1000, train_loss: 0.320180\n",
      "batch_idx:  2000, train_loss: 0.338127\n",
      "batch_idx:  3000, train_loss: 0.345712\n",
      "batch_idx:  4000, train_loss: 0.338137\n",
      "batch_idx:  5000, train_loss: 0.340388\n",
      "batch_idx:  6000, train_loss: 0.357004\n",
      "final train_loss: 0.366381\n",
      "batch_idx:     0, valid_loss: 7.144929\n",
      "batch_idx:  1000, valid_loss: 15.331645\n",
      "batch_idx:  2000, valid_loss: 18.466839\n",
      "batch_idx:  3000, valid_loss: 19.929586\n",
      "batch_idx:  4000, valid_loss: 20.973005\n",
      "batch_idx:  5000, valid_loss: 21.610580\n",
      "batch_idx:  6000, valid_loss: 21.549429\n",
      "Training Loss: 0.366381 \tValidation Loss: 20.606956\n",
      "epoch:    27\n",
      "batch_idx:     0, train_loss: 4.457152\n",
      "batch_idx:  1000, train_loss: 0.331324\n",
      "batch_idx:  2000, train_loss: 0.364780\n",
      "batch_idx:  3000, train_loss: 0.370016\n",
      "batch_idx:  4000, train_loss: 0.387632\n",
      "batch_idx:  5000, train_loss: 0.459476\n",
      "batch_idx:  6000, train_loss: 0.540406\n",
      "final train_loss: 0.576915\n",
      "batch_idx:     0, valid_loss: 9.949520\n",
      "batch_idx:  1000, valid_loss: 13.117075\n",
      "batch_idx:  2000, valid_loss: 13.467640\n",
      "batch_idx:  3000, valid_loss: 13.683071\n",
      "batch_idx:  4000, valid_loss: 13.807931\n",
      "batch_idx:  5000, valid_loss: 13.870177\n",
      "batch_idx:  6000, valid_loss: 13.749674\n",
      "Training Loss: 0.576915 \tValidation Loss: 13.381854\n",
      "epoch:    28\n",
      "batch_idx:     0, train_loss: 6.612277\n",
      "batch_idx:  1000, train_loss: 0.818410\n",
      "batch_idx:  2000, train_loss: 0.780229\n",
      "batch_idx:  3000, train_loss: 0.741822\n",
      "batch_idx:  4000, train_loss: 0.647168\n",
      "batch_idx:  5000, train_loss: 0.589182\n",
      "batch_idx:  6000, train_loss: 0.557651\n",
      "final train_loss: 0.547547\n",
      "batch_idx:     0, valid_loss: 7.648259\n",
      "batch_idx:  1000, valid_loss: 15.356512\n",
      "batch_idx:  2000, valid_loss: 18.354591\n",
      "batch_idx:  3000, valid_loss: 19.748022\n",
      "batch_idx:  4000, valid_loss: 20.642876\n",
      "batch_idx:  5000, valid_loss: 21.123468\n",
      "batch_idx:  6000, valid_loss: 21.095922\n",
      "Training Loss: 0.547547 \tValidation Loss: 20.248659\n",
      "epoch:    29\n",
      "batch_idx:     0, train_loss: 5.671990\n",
      "batch_idx:  1000, train_loss: 0.331336\n",
      "batch_idx:  2000, train_loss: 0.347504\n",
      "batch_idx:  3000, train_loss: 0.356642\n",
      "batch_idx:  4000, train_loss: 0.370875\n",
      "batch_idx:  5000, train_loss: 0.383567\n",
      "batch_idx:  6000, train_loss: 0.438933\n",
      "final train_loss: 0.491672\n",
      "batch_idx:     0, valid_loss: 10.876148\n",
      "batch_idx:  1000, valid_loss: 11.935676\n",
      "batch_idx:  2000, valid_loss: 13.194664\n",
      "batch_idx:  3000, valid_loss: 13.677316\n",
      "batch_idx:  4000, valid_loss: 14.032333\n",
      "batch_idx:  5000, valid_loss: 14.185193\n",
      "batch_idx:  6000, valid_loss: 14.095370\n",
      "Training Loss: 0.491672 \tValidation Loss: 13.590988\n",
      "epoch:    30\n",
      "batch_idx:     0, train_loss: 7.136683\n",
      "batch_idx:  1000, train_loss: 0.627277\n",
      "batch_idx:  2000, train_loss: 0.700501\n",
      "batch_idx:  3000, train_loss: 0.725643\n",
      "batch_idx:  4000, train_loss: 0.759381\n",
      "batch_idx:  5000, train_loss: 0.756131\n",
      "batch_idx:  6000, train_loss: 0.806397\n",
      "final train_loss: 0.874975\n",
      "batch_idx:     0, valid_loss: 10.093553\n",
      "batch_idx:  1000, valid_loss: 12.648369\n",
      "batch_idx:  2000, valid_loss: 12.155127\n",
      "batch_idx:  3000, valid_loss: 11.891482\n",
      "batch_idx:  4000, valid_loss: 12.027859\n",
      "batch_idx:  5000, valid_loss: 12.070698\n",
      "batch_idx:  6000, valid_loss: 12.073594\n",
      "Training Loss: 0.874975 \tValidation Loss: 11.618102\n",
      "epoch:    31\n",
      "batch_idx:     0, train_loss: 12.523854\n",
      "batch_idx:  1000, train_loss: 1.010671\n",
      "batch_idx:  2000, train_loss: 1.036222\n",
      "batch_idx:  3000, train_loss: 1.064525\n",
      "batch_idx:  4000, train_loss: 1.166608\n",
      "batch_idx:  5000, train_loss: 1.181937\n",
      "batch_idx:  6000, train_loss: 1.119277\n",
      "final train_loss: 1.100390\n",
      "batch_idx:     0, valid_loss: 12.244167\n",
      "batch_idx:  1000, valid_loss: 15.531150\n",
      "batch_idx:  2000, valid_loss: 16.742922\n",
      "batch_idx:  3000, valid_loss: 17.149780\n",
      "batch_idx:  4000, valid_loss: 17.211962\n",
      "batch_idx:  5000, valid_loss: 17.353148\n",
      "batch_idx:  6000, valid_loss: 17.400202\n",
      "Training Loss: 1.100390 \tValidation Loss: 16.983404\n",
      "epoch:    32\n",
      "batch_idx:     0, train_loss: 12.675999\n",
      "batch_idx:  1000, train_loss: 0.841168\n",
      "batch_idx:  2000, train_loss: 1.038412\n",
      "batch_idx:  3000, train_loss: 1.058783\n",
      "batch_idx:  4000, train_loss: 0.971292\n",
      "batch_idx:  5000, train_loss: 0.915300\n",
      "batch_idx:  6000, train_loss: 0.888837\n",
      "final train_loss: 0.915906\n",
      "batch_idx:     0, valid_loss: 13.764271\n",
      "batch_idx:  1000, valid_loss: 14.073859\n",
      "batch_idx:  2000, valid_loss: 14.114902\n",
      "batch_idx:  3000, valid_loss: 14.095019\n",
      "batch_idx:  4000, valid_loss: 14.197662\n",
      "batch_idx:  5000, valid_loss: 14.212282\n",
      "batch_idx:  6000, valid_loss: 14.192128\n",
      "Training Loss: 0.915906 \tValidation Loss: 13.852478\n",
      "epoch:    33\n",
      "batch_idx:     0, train_loss: 17.418259\n",
      "batch_idx:  1000, train_loss: 1.047908\n",
      "batch_idx:  2000, train_loss: 1.103045\n",
      "batch_idx:  3000, train_loss: 1.098876\n",
      "batch_idx:  4000, train_loss: 1.058857\n",
      "batch_idx:  5000, train_loss: 1.101005\n",
      "batch_idx:  6000, train_loss: 1.073512\n",
      "final train_loss: 1.056395\n",
      "batch_idx:     0, valid_loss: 9.205640\n",
      "batch_idx:  1000, valid_loss: 11.516720\n",
      "batch_idx:  2000, valid_loss: 14.249563\n",
      "batch_idx:  3000, valid_loss: 15.075789\n",
      "batch_idx:  4000, valid_loss: 15.575857\n",
      "batch_idx:  5000, valid_loss: 15.944282\n",
      "batch_idx:  6000, valid_loss: 15.966883\n",
      "Training Loss: 1.056395 \tValidation Loss: 15.494524\n",
      "epoch:    34\n",
      "batch_idx:     0, train_loss: 6.591178\n",
      "batch_idx:  1000, train_loss: 0.650391\n",
      "batch_idx:  2000, train_loss: 0.761600\n",
      "batch_idx:  3000, train_loss: 0.775953\n",
      "batch_idx:  4000, train_loss: 0.731880\n",
      "batch_idx:  5000, train_loss: 0.729666\n",
      "batch_idx:  6000, train_loss: 0.717378\n",
      "final train_loss: 0.687618\n",
      "batch_idx:     0, valid_loss: 7.314155\n",
      "batch_idx:  1000, valid_loss: 15.619906\n",
      "batch_idx:  2000, valid_loss: 18.731276\n",
      "batch_idx:  3000, valid_loss: 20.202053\n",
      "batch_idx:  4000, valid_loss: 21.342762\n",
      "batch_idx:  5000, valid_loss: 22.158381\n",
      "batch_idx:  6000, valid_loss: 22.365765\n",
      "Training Loss: 0.687618 \tValidation Loss: 21.463539\n",
      "epoch:    35\n",
      "batch_idx:     0, train_loss: 8.049143\n",
      "batch_idx:  1000, train_loss: 0.298099\n",
      "batch_idx:  2000, train_loss: 0.327718\n",
      "batch_idx:  3000, train_loss: 0.330293\n",
      "batch_idx:  4000, train_loss: 0.343036\n",
      "batch_idx:  5000, train_loss: 0.353979\n",
      "batch_idx:  6000, train_loss: 0.367190\n",
      "final train_loss: 0.375568\n",
      "batch_idx:     0, valid_loss: 7.459112\n",
      "batch_idx:  1000, valid_loss: 14.692692\n",
      "batch_idx:  2000, valid_loss: 17.806675\n",
      "batch_idx:  3000, valid_loss: 19.158369\n",
      "batch_idx:  4000, valid_loss: 20.110561\n",
      "batch_idx:  5000, valid_loss: 20.769022\n",
      "batch_idx:  6000, valid_loss: 20.920155\n",
      "Training Loss: 0.375568 \tValidation Loss: 20.088991\n",
      "epoch:    36\n",
      "batch_idx:     0, train_loss: 8.558450\n",
      "batch_idx:  1000, train_loss: 0.323754\n",
      "batch_idx:  2000, train_loss: 0.346904\n",
      "batch_idx:  3000, train_loss: 0.349578\n",
      "batch_idx:  4000, train_loss: 0.364366\n",
      "batch_idx:  5000, train_loss: 0.371219\n",
      "batch_idx:  6000, train_loss: 0.383704\n",
      "final train_loss: 0.389314\n",
      "batch_idx:     0, valid_loss: 6.973509\n",
      "batch_idx:  1000, valid_loss: 14.473652\n",
      "batch_idx:  2000, valid_loss: 17.762403\n",
      "batch_idx:  3000, valid_loss: 19.432550\n",
      "batch_idx:  4000, valid_loss: 20.535709\n",
      "batch_idx:  5000, valid_loss: 21.322853\n",
      "batch_idx:  6000, valid_loss: 21.528236\n",
      "Training Loss: 0.389314 \tValidation Loss: 20.624229\n",
      "epoch:    37\n",
      "batch_idx:     0, train_loss: 4.459410\n",
      "batch_idx:  1000, train_loss: 0.320360\n",
      "batch_idx:  2000, train_loss: 0.340286\n",
      "batch_idx:  3000, train_loss: 0.340142\n",
      "batch_idx:  4000, train_loss: 0.352062\n",
      "batch_idx:  5000, train_loss: 0.345855\n",
      "batch_idx:  6000, train_loss: 0.347277\n",
      "final train_loss: 0.357203\n",
      "batch_idx:     0, valid_loss: 6.319505\n",
      "batch_idx:  1000, valid_loss: 12.939694\n",
      "batch_idx:  2000, valid_loss: 16.587509\n",
      "batch_idx:  3000, valid_loss: 18.435902\n",
      "batch_idx:  4000, valid_loss: 19.623856\n",
      "batch_idx:  5000, valid_loss: 20.583260\n",
      "batch_idx:  6000, valid_loss: 20.918430\n",
      "Training Loss: 0.357203 \tValidation Loss: 20.071970\n",
      "epoch:    38\n",
      "batch_idx:     0, train_loss: 5.597509\n",
      "batch_idx:  1000, train_loss: 0.322154\n",
      "batch_idx:  2000, train_loss: 0.335875\n",
      "batch_idx:  3000, train_loss: 0.337839\n",
      "batch_idx:  4000, train_loss: 0.348151\n",
      "batch_idx:  5000, train_loss: 0.355743\n",
      "batch_idx:  6000, train_loss: 0.369590\n",
      "final train_loss: 0.378865\n",
      "batch_idx:     0, valid_loss: 6.793940\n",
      "batch_idx:  1000, valid_loss: 14.134431\n",
      "batch_idx:  2000, valid_loss: 17.653139\n",
      "batch_idx:  3000, valid_loss: 19.414755\n",
      "batch_idx:  4000, valid_loss: 20.729261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  5000, valid_loss: 21.555384\n",
      "batch_idx:  6000, valid_loss: 21.690317\n",
      "Training Loss: 0.378865 \tValidation Loss: 20.802219\n",
      "epoch:    39\n",
      "batch_idx:     0, train_loss: 8.401537\n",
      "batch_idx:  1000, train_loss: 0.323269\n",
      "batch_idx:  2000, train_loss: 0.352604\n",
      "batch_idx:  3000, train_loss: 0.366412\n",
      "batch_idx:  4000, train_loss: 0.423649\n",
      "batch_idx:  5000, train_loss: 0.482085\n",
      "batch_idx:  6000, train_loss: 0.598176\n",
      "final train_loss: 0.740039\n",
      "batch_idx:     0, valid_loss: 9.990647\n",
      "batch_idx:  1000, valid_loss: 9.769981\n",
      "batch_idx:  2000, valid_loss: 9.822302\n",
      "batch_idx:  3000, valid_loss: 9.812899\n",
      "batch_idx:  4000, valid_loss: 9.856185\n",
      "batch_idx:  5000, valid_loss: 9.868742\n",
      "batch_idx:  6000, valid_loss: 9.715224\n",
      "Training Loss: 0.740039 \tValidation Loss: 9.369958\n",
      "Training loss decreased (9.439337 --> 9.369958).  Saving model ...\n",
      "epoch:    40\n",
      "batch_idx:     0, train_loss: 11.912059\n",
      "batch_idx:  1000, train_loss: 1.340399\n",
      "batch_idx:  2000, train_loss: 1.400946\n",
      "batch_idx:  3000, train_loss: 1.559325\n",
      "batch_idx:  4000, train_loss: 1.639590\n",
      "batch_idx:  5000, train_loss: 1.607915\n",
      "batch_idx:  6000, train_loss: 1.572931\n",
      "final train_loss: 1.454592\n",
      "batch_idx:     0, valid_loss: 5.750441\n",
      "batch_idx:  1000, valid_loss: 12.966660\n",
      "batch_idx:  2000, valid_loss: 16.223276\n",
      "batch_idx:  3000, valid_loss: 17.783726\n",
      "batch_idx:  4000, valid_loss: 19.075693\n",
      "batch_idx:  5000, valid_loss: 19.949009\n",
      "batch_idx:  6000, valid_loss: 20.285109\n",
      "Training Loss: 1.454592 \tValidation Loss: 19.566282\n",
      "epoch:    41\n",
      "batch_idx:     0, train_loss: 6.523550\n",
      "batch_idx:  1000, train_loss: 0.304673\n",
      "batch_idx:  2000, train_loss: 0.325306\n",
      "batch_idx:  3000, train_loss: 0.331271\n",
      "batch_idx:  4000, train_loss: 0.351375\n",
      "batch_idx:  5000, train_loss: 0.365762\n",
      "batch_idx:  6000, train_loss: 0.367338\n",
      "final train_loss: 0.377898\n",
      "batch_idx:     0, valid_loss: 6.622378\n",
      "batch_idx:  1000, valid_loss: 13.875033\n",
      "batch_idx:  2000, valid_loss: 17.855539\n",
      "batch_idx:  3000, valid_loss: 19.686419\n",
      "batch_idx:  4000, valid_loss: 20.956932\n",
      "batch_idx:  5000, valid_loss: 21.908669\n",
      "batch_idx:  6000, valid_loss: 22.142881\n",
      "Training Loss: 0.377898 \tValidation Loss: 21.232138\n",
      "epoch:    42\n",
      "batch_idx:     0, train_loss: 7.235249\n",
      "batch_idx:  1000, train_loss: 0.329445\n",
      "batch_idx:  2000, train_loss: 0.346644\n",
      "batch_idx:  3000, train_loss: 0.353658\n",
      "batch_idx:  4000, train_loss: 0.365128\n",
      "batch_idx:  5000, train_loss: 0.383373\n",
      "batch_idx:  6000, train_loss: 0.410698\n",
      "final train_loss: 0.459856\n",
      "batch_idx:     0, valid_loss: 11.294603\n",
      "batch_idx:  1000, valid_loss: 12.835057\n",
      "batch_idx:  2000, valid_loss: 14.188374\n",
      "batch_idx:  3000, valid_loss: 14.640094\n",
      "batch_idx:  4000, valid_loss: 14.908331\n",
      "batch_idx:  5000, valid_loss: 15.071957\n",
      "batch_idx:  6000, valid_loss: 14.919744\n",
      "Training Loss: 0.459856 \tValidation Loss: 14.437538\n",
      "epoch:    43\n",
      "batch_idx:     0, train_loss: 12.611363\n",
      "batch_idx:  1000, train_loss: 0.722253\n",
      "batch_idx:  2000, train_loss: 0.866480\n",
      "batch_idx:  3000, train_loss: 0.888128\n",
      "batch_idx:  4000, train_loss: 0.922162\n",
      "batch_idx:  5000, train_loss: 0.961745\n",
      "batch_idx:  6000, train_loss: 0.937935\n",
      "final train_loss: 0.889558\n",
      "batch_idx:     0, valid_loss: 7.672928\n",
      "batch_idx:  1000, valid_loss: 14.312143\n",
      "batch_idx:  2000, valid_loss: 17.501833\n",
      "batch_idx:  3000, valid_loss: 19.331848\n",
      "batch_idx:  4000, valid_loss: 20.569702\n",
      "batch_idx:  5000, valid_loss: 21.426201\n",
      "batch_idx:  6000, valid_loss: 21.524994\n",
      "Training Loss: 0.889558 \tValidation Loss: 20.581909\n",
      "epoch:    44\n",
      "batch_idx:     0, train_loss: 8.859048\n",
      "batch_idx:  1000, train_loss: 0.336933\n",
      "batch_idx:  2000, train_loss: 0.348480\n",
      "batch_idx:  3000, train_loss: 0.356500\n",
      "batch_idx:  4000, train_loss: 0.366697\n",
      "batch_idx:  5000, train_loss: 0.395578\n",
      "batch_idx:  6000, train_loss: 0.420042\n",
      "final train_loss: 0.454716\n",
      "batch_idx:     0, valid_loss: 14.473242\n",
      "batch_idx:  1000, valid_loss: 17.576529\n",
      "batch_idx:  2000, valid_loss: 17.904715\n",
      "batch_idx:  3000, valid_loss: 18.023375\n",
      "batch_idx:  4000, valid_loss: 18.061518\n",
      "batch_idx:  5000, valid_loss: 18.113974\n",
      "batch_idx:  6000, valid_loss: 17.855501\n",
      "Training Loss: 0.454716 \tValidation Loss: 17.289194\n",
      "epoch:    45\n",
      "batch_idx:     0, train_loss: 17.449476\n",
      "batch_idx:  1000, train_loss: 1.428455\n",
      "batch_idx:  2000, train_loss: 1.235766\n",
      "batch_idx:  3000, train_loss: 1.189542\n",
      "batch_idx:  4000, train_loss: 1.251393\n",
      "batch_idx:  5000, train_loss: 1.227293\n",
      "batch_idx:  6000, train_loss: 1.222439\n",
      "final train_loss: 1.243649\n",
      "batch_idx:     0, valid_loss: 12.068125\n",
      "batch_idx:  1000, valid_loss: 11.202577\n",
      "batch_idx:  2000, valid_loss: 12.425538\n",
      "batch_idx:  3000, valid_loss: 13.003037\n",
      "batch_idx:  4000, valid_loss: 13.260437\n",
      "batch_idx:  5000, valid_loss: 13.465253\n",
      "batch_idx:  6000, valid_loss: 13.442872\n",
      "Training Loss: 1.243649 \tValidation Loss: 13.088410\n",
      "epoch:    46\n",
      "batch_idx:     0, train_loss: 8.411608\n",
      "batch_idx:  1000, train_loss: 0.970574\n",
      "batch_idx:  2000, train_loss: 0.954784\n",
      "batch_idx:  3000, train_loss: 0.947993\n",
      "batch_idx:  4000, train_loss: 0.936753\n",
      "batch_idx:  5000, train_loss: 0.959736\n",
      "batch_idx:  6000, train_loss: 0.961763\n",
      "final train_loss: 0.953378\n",
      "batch_idx:     0, valid_loss: 6.086793\n",
      "batch_idx:  1000, valid_loss: 14.508293\n",
      "batch_idx:  2000, valid_loss: 16.489910\n",
      "batch_idx:  3000, valid_loss: 17.267597\n",
      "batch_idx:  4000, valid_loss: 17.693501\n",
      "batch_idx:  5000, valid_loss: 17.954218\n",
      "batch_idx:  6000, valid_loss: 17.598330\n",
      "Training Loss: 0.953378 \tValidation Loss: 16.922106\n",
      "epoch:    47\n",
      "batch_idx:     0, train_loss: 7.843058\n",
      "batch_idx:  1000, train_loss: 0.561109\n",
      "batch_idx:  2000, train_loss: 0.665173\n",
      "batch_idx:  3000, train_loss: 0.689415\n",
      "batch_idx:  4000, train_loss: 0.836556\n",
      "batch_idx:  5000, train_loss: 0.887537\n",
      "batch_idx:  6000, train_loss: 0.912420\n",
      "final train_loss: 0.971314\n",
      "batch_idx:     0, valid_loss: 14.452271\n",
      "batch_idx:  1000, valid_loss: 14.767632\n",
      "batch_idx:  2000, valid_loss: 14.957541\n",
      "batch_idx:  3000, valid_loss: 15.228133\n",
      "batch_idx:  4000, valid_loss: 15.375886\n",
      "batch_idx:  5000, valid_loss: 15.515345\n",
      "batch_idx:  6000, valid_loss: 15.331573\n",
      "Training Loss: 0.971314 \tValidation Loss: 14.791194\n",
      "epoch:    48\n",
      "batch_idx:     0, train_loss: 17.144302\n",
      "batch_idx:  1000, train_loss: 0.925199\n",
      "batch_idx:  2000, train_loss: 0.945827\n",
      "batch_idx:  3000, train_loss: 0.957402\n",
      "batch_idx:  4000, train_loss: 0.907319\n",
      "batch_idx:  5000, train_loss: 1.001662\n",
      "batch_idx:  6000, train_loss: 1.000866\n",
      "final train_loss: 1.015827\n",
      "batch_idx:     0, valid_loss: 14.057293\n",
      "batch_idx:  1000, valid_loss: 17.533487\n",
      "batch_idx:  2000, valid_loss: 17.621378\n",
      "batch_idx:  3000, valid_loss: 17.752707\n",
      "batch_idx:  4000, valid_loss: 17.798460\n",
      "batch_idx:  5000, valid_loss: 17.857475\n",
      "batch_idx:  6000, valid_loss: 17.680155\n",
      "Training Loss: 1.015827 \tValidation Loss: 17.076614\n",
      "epoch:    49\n",
      "batch_idx:     0, train_loss: 8.341614\n",
      "batch_idx:  1000, train_loss: 0.727514\n",
      "batch_idx:  2000, train_loss: 0.836699\n",
      "batch_idx:  3000, train_loss: 0.858717\n",
      "batch_idx:  4000, train_loss: 0.996286\n",
      "batch_idx:  5000, train_loss: 1.176277\n",
      "batch_idx:  6000, train_loss: 1.218967\n",
      "final train_loss: 1.178298\n",
      "batch_idx:     0, valid_loss: 10.663333\n",
      "batch_idx:  1000, valid_loss: 17.641685\n",
      "batch_idx:  2000, valid_loss: 18.761858\n",
      "batch_idx:  3000, valid_loss: 19.123590\n",
      "batch_idx:  4000, valid_loss: 19.332922\n",
      "batch_idx:  5000, valid_loss: 19.472452\n",
      "batch_idx:  6000, valid_loss: 19.137875\n",
      "Training Loss: 1.178298 \tValidation Loss: 18.468596\n",
      "epoch:    50\n",
      "batch_idx:     0, train_loss: 11.198534\n",
      "batch_idx:  1000, train_loss: 0.553546\n",
      "batch_idx:  2000, train_loss: 0.600903\n",
      "batch_idx:  3000, train_loss: 0.813123\n",
      "batch_idx:  4000, train_loss: 0.930541\n",
      "batch_idx:  5000, train_loss: 1.061013\n",
      "batch_idx:  6000, train_loss: 1.018930\n",
      "final train_loss: 0.961573\n",
      "batch_idx:     0, valid_loss: 6.490613\n",
      "batch_idx:  1000, valid_loss: 13.716461\n",
      "batch_idx:  2000, valid_loss: 16.910248\n",
      "batch_idx:  3000, valid_loss: 18.591705\n",
      "batch_idx:  4000, valid_loss: 19.710154\n",
      "batch_idx:  5000, valid_loss: 20.458977\n",
      "batch_idx:  6000, valid_loss: 20.707270\n",
      "Training Loss: 0.961573 \tValidation Loss: 19.955894\n",
      "epoch:    51\n",
      "batch_idx:     0, train_loss: 5.604041\n",
      "batch_idx:  1000, train_loss: 0.309804\n",
      "batch_idx:  2000, train_loss: 0.323283\n",
      "batch_idx:  3000, train_loss: 0.328673\n",
      "batch_idx:  4000, train_loss: 0.348173\n",
      "batch_idx:  5000, train_loss: 0.365657\n",
      "batch_idx:  6000, train_loss: 0.378719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final train_loss: 0.411299\n",
      "batch_idx:     0, valid_loss: 7.432005\n",
      "batch_idx:  1000, valid_loss: 13.937018\n",
      "batch_idx:  2000, valid_loss: 16.706505\n",
      "batch_idx:  3000, valid_loss: 17.579870\n",
      "batch_idx:  4000, valid_loss: 18.052496\n",
      "batch_idx:  5000, valid_loss: 18.338457\n",
      "batch_idx:  6000, valid_loss: 18.071186\n",
      "Training Loss: 0.411299 \tValidation Loss: 17.401209\n",
      "epoch:    52\n",
      "batch_idx:     0, train_loss: 7.853501\n",
      "batch_idx:  1000, train_loss: 0.486240\n",
      "batch_idx:  2000, train_loss: 0.533136\n",
      "batch_idx:  3000, train_loss: 0.582683\n",
      "batch_idx:  4000, train_loss: 0.587575\n",
      "batch_idx:  5000, train_loss: 0.639329\n",
      "batch_idx:  6000, train_loss: 0.676458\n",
      "final train_loss: 0.717805\n",
      "batch_idx:     0, valid_loss: 10.500446\n",
      "batch_idx:  1000, valid_loss: 13.248517\n",
      "batch_idx:  2000, valid_loss: 14.615256\n",
      "batch_idx:  3000, valid_loss: 15.021646\n",
      "batch_idx:  4000, valid_loss: 15.242042\n",
      "batch_idx:  5000, valid_loss: 15.387865\n",
      "batch_idx:  6000, valid_loss: 15.343539\n",
      "Training Loss: 0.717805 \tValidation Loss: 14.804027\n",
      "epoch:    53\n",
      "batch_idx:     0, train_loss: 7.714308\n",
      "batch_idx:  1000, train_loss: 0.449827\n",
      "batch_idx:  2000, train_loss: 0.485943\n",
      "batch_idx:  3000, train_loss: 0.558619\n",
      "batch_idx:  4000, train_loss: 0.563824\n",
      "batch_idx:  5000, train_loss: 0.619114\n",
      "batch_idx:  6000, train_loss: 0.618589\n",
      "final train_loss: 0.599961\n",
      "batch_idx:     0, valid_loss: 7.450060\n",
      "batch_idx:  1000, valid_loss: 13.740542\n",
      "batch_idx:  2000, valid_loss: 17.589901\n",
      "batch_idx:  3000, valid_loss: 19.697565\n",
      "batch_idx:  4000, valid_loss: 20.859247\n",
      "batch_idx:  5000, valid_loss: 21.657103\n",
      "batch_idx:  6000, valid_loss: 21.853909\n",
      "Training Loss: 0.599961 \tValidation Loss: 20.962574\n",
      "epoch:    54\n",
      "batch_idx:     0, train_loss: 7.886555\n",
      "batch_idx:  1000, train_loss: 0.335963\n",
      "batch_idx:  2000, train_loss: 0.339890\n",
      "batch_idx:  3000, train_loss: 0.342380\n",
      "batch_idx:  4000, train_loss: 0.354171\n",
      "batch_idx:  5000, train_loss: 0.368390\n",
      "batch_idx:  6000, train_loss: 0.375121\n",
      "final train_loss: 0.383509\n",
      "batch_idx:     0, valid_loss: 7.921241\n",
      "batch_idx:  1000, valid_loss: 13.771308\n",
      "batch_idx:  2000, valid_loss: 17.363281\n",
      "batch_idx:  3000, valid_loss: 19.062992\n",
      "batch_idx:  4000, valid_loss: 20.123505\n",
      "batch_idx:  5000, valid_loss: 20.976210\n",
      "batch_idx:  6000, valid_loss: 21.081690\n",
      "Training Loss: 0.383509 \tValidation Loss: 20.222282\n",
      "epoch:    55\n",
      "batch_idx:     0, train_loss: 8.284427\n",
      "batch_idx:  1000, train_loss: 0.326502\n",
      "batch_idx:  2000, train_loss: 0.353197\n",
      "batch_idx:  3000, train_loss: 0.352116\n",
      "batch_idx:  4000, train_loss: 0.366335\n",
      "batch_idx:  5000, train_loss: 0.376573\n",
      "batch_idx:  6000, train_loss: 0.386668\n",
      "final train_loss: 0.394150\n",
      "batch_idx:     0, valid_loss: 8.115438\n",
      "batch_idx:  1000, valid_loss: 13.542023\n",
      "batch_idx:  2000, valid_loss: 17.125395\n",
      "batch_idx:  3000, valid_loss: 18.916948\n",
      "batch_idx:  4000, valid_loss: 19.963312\n",
      "batch_idx:  5000, valid_loss: 20.747156\n",
      "batch_idx:  6000, valid_loss: 20.932426\n",
      "Training Loss: 0.394150 \tValidation Loss: 20.125044\n",
      "epoch:    56\n",
      "batch_idx:     0, train_loss: 7.726473\n",
      "batch_idx:  1000, train_loss: 0.313664\n",
      "batch_idx:  2000, train_loss: 0.333993\n",
      "batch_idx:  3000, train_loss: 0.336867\n",
      "batch_idx:  4000, train_loss: 0.353845\n",
      "batch_idx:  5000, train_loss: 0.363598\n",
      "batch_idx:  6000, train_loss: 0.371186\n",
      "final train_loss: 0.379929\n",
      "batch_idx:     0, valid_loss: 7.044689\n",
      "batch_idx:  1000, valid_loss: 13.290574\n",
      "batch_idx:  2000, valid_loss: 16.845768\n",
      "batch_idx:  3000, valid_loss: 18.491049\n",
      "batch_idx:  4000, valid_loss: 19.578537\n",
      "batch_idx:  5000, valid_loss: 20.307407\n",
      "batch_idx:  6000, valid_loss: 20.400311\n",
      "Training Loss: 0.379929 \tValidation Loss: 19.582081\n",
      "epoch:    57\n",
      "batch_idx:     0, train_loss: 6.977914\n",
      "batch_idx:  1000, train_loss: 0.341793\n",
      "batch_idx:  2000, train_loss: 0.339041\n",
      "batch_idx:  3000, train_loss: 0.326713\n",
      "batch_idx:  4000, train_loss: 0.333107\n",
      "batch_idx:  5000, train_loss: 0.339754\n",
      "batch_idx:  6000, train_loss: 0.358737\n",
      "final train_loss: 0.374208\n",
      "batch_idx:     0, valid_loss: 7.165380\n",
      "batch_idx:  1000, valid_loss: 13.781311\n",
      "batch_idx:  2000, valid_loss: 17.199080\n",
      "batch_idx:  3000, valid_loss: 18.655460\n",
      "batch_idx:  4000, valid_loss: 19.422653\n",
      "batch_idx:  5000, valid_loss: 19.937632\n",
      "batch_idx:  6000, valid_loss: 19.731308\n",
      "Training Loss: 0.374208 \tValidation Loss: 18.922434\n",
      "epoch:    58\n",
      "batch_idx:     0, train_loss: 6.684764\n",
      "batch_idx:  1000, train_loss: 0.393559\n",
      "batch_idx:  2000, train_loss: 0.433676\n",
      "batch_idx:  3000, train_loss: 0.442588\n",
      "batch_idx:  4000, train_loss: 0.467546\n",
      "batch_idx:  5000, train_loss: 0.478608\n",
      "batch_idx:  6000, train_loss: 0.522002\n",
      "final train_loss: 0.589140\n",
      "batch_idx:     0, valid_loss: 13.992562\n",
      "batch_idx:  1000, valid_loss: 14.099016\n",
      "batch_idx:  2000, valid_loss: 14.255302\n",
      "batch_idx:  3000, valid_loss: 14.209438\n",
      "batch_idx:  4000, valid_loss: 14.289215\n",
      "batch_idx:  5000, valid_loss: 14.323459\n",
      "batch_idx:  6000, valid_loss: 14.299181\n",
      "Training Loss: 0.589140 \tValidation Loss: 13.998442\n",
      "epoch:    59\n",
      "batch_idx:     0, train_loss: 18.312054\n",
      "batch_idx:  1000, train_loss: 0.948776\n",
      "batch_idx:  2000, train_loss: 0.950249\n",
      "batch_idx:  3000, train_loss: 0.931361\n",
      "batch_idx:  4000, train_loss: 0.956711\n",
      "batch_idx:  5000, train_loss: 0.974425\n",
      "batch_idx:  6000, train_loss: 0.950051\n",
      "final train_loss: 1.019891\n",
      "batch_idx:     0, valid_loss: 12.131656\n",
      "batch_idx:  1000, valid_loss: 12.590345\n",
      "batch_idx:  2000, valid_loss: 12.735670\n",
      "batch_idx:  3000, valid_loss: 12.754024\n",
      "batch_idx:  4000, valid_loss: 12.718077\n",
      "batch_idx:  5000, valid_loss: 12.686907\n",
      "batch_idx:  6000, valid_loss: 12.610325\n",
      "Training Loss: 1.019891 \tValidation Loss: 12.346212\n",
      "epoch:    60\n",
      "batch_idx:     0, train_loss: 15.291295\n",
      "batch_idx:  1000, train_loss: 0.933641\n",
      "batch_idx:  2000, train_loss: 1.111529\n",
      "batch_idx:  3000, train_loss: 1.113353\n",
      "batch_idx:  4000, train_loss: 1.069771\n",
      "batch_idx:  5000, train_loss: 1.063622\n",
      "batch_idx:  6000, train_loss: 1.066147\n",
      "final train_loss: 1.074646\n",
      "batch_idx:     0, valid_loss: 14.414763\n",
      "batch_idx:  1000, valid_loss: 13.611755\n",
      "batch_idx:  2000, valid_loss: 13.724260\n",
      "batch_idx:  3000, valid_loss: 13.723034\n",
      "batch_idx:  4000, valid_loss: 13.769541\n",
      "batch_idx:  5000, valid_loss: 13.805547\n",
      "batch_idx:  6000, valid_loss: 13.590665\n",
      "Training Loss: 1.074646 \tValidation Loss: 13.041224\n",
      "epoch:    61\n",
      "batch_idx:     0, train_loss: 12.363233\n",
      "batch_idx:  1000, train_loss: 1.242479\n",
      "batch_idx:  2000, train_loss: 1.069412\n",
      "batch_idx:  3000, train_loss: 1.006622\n",
      "batch_idx:  4000, train_loss: 1.031099\n",
      "batch_idx:  5000, train_loss: 1.110909\n",
      "batch_idx:  6000, train_loss: 1.295460\n",
      "final train_loss: 1.410142\n",
      "batch_idx:     0, valid_loss: 9.378925\n",
      "batch_idx:  1000, valid_loss: 10.412180\n",
      "batch_idx:  2000, valid_loss: 10.431014\n",
      "batch_idx:  3000, valid_loss: 10.459226\n",
      "batch_idx:  4000, valid_loss: 10.447053\n",
      "batch_idx:  5000, valid_loss: 10.203253\n",
      "batch_idx:  6000, valid_loss: 9.861489\n",
      "Training Loss: 1.410142 \tValidation Loss: 9.461263\n",
      "epoch:    62\n",
      "batch_idx:     0, train_loss: 12.425099\n",
      "batch_idx:  1000, train_loss: 1.307704\n",
      "batch_idx:  2000, train_loss: 0.990642\n",
      "batch_idx:  3000, train_loss: 0.889723\n",
      "batch_idx:  4000, train_loss: 1.067967\n",
      "batch_idx:  5000, train_loss: 1.267207\n",
      "batch_idx:  6000, train_loss: 1.349276\n",
      "final train_loss: 1.423786\n",
      "batch_idx:     0, valid_loss: 10.737593\n",
      "batch_idx:  1000, valid_loss: 10.708305\n",
      "batch_idx:  2000, valid_loss: 10.771879\n",
      "batch_idx:  3000, valid_loss: 10.800408\n",
      "batch_idx:  4000, valid_loss: 10.831837\n",
      "batch_idx:  5000, valid_loss: 10.751082\n",
      "batch_idx:  6000, valid_loss: 10.445878\n",
      "Training Loss: 1.423786 \tValidation Loss: 10.058962\n",
      "epoch:    63\n",
      "batch_idx:     0, train_loss: 4.767966\n",
      "batch_idx:  1000, train_loss: 1.451626\n",
      "batch_idx:  2000, train_loss: 1.453003\n",
      "batch_idx:  3000, train_loss: 1.174188\n",
      "batch_idx:  4000, train_loss: 1.070254\n",
      "batch_idx:  5000, train_loss: 1.142257\n",
      "batch_idx:  6000, train_loss: 1.163286\n",
      "final train_loss: 1.245879\n",
      "batch_idx:     0, valid_loss: 9.428080\n",
      "batch_idx:  1000, valid_loss: 9.972668\n",
      "batch_idx:  2000, valid_loss: 10.039341\n",
      "batch_idx:  3000, valid_loss: 10.071377\n",
      "batch_idx:  4000, valid_loss: 10.066615\n",
      "batch_idx:  5000, valid_loss: 10.049714\n",
      "batch_idx:  6000, valid_loss: 9.826542\n",
      "Training Loss: 1.245879 \tValidation Loss: 9.480865\n",
      "epoch:    64\n",
      "batch_idx:     0, train_loss: 11.394794\n",
      "batch_idx:  1000, train_loss: 1.496502\n",
      "batch_idx:  2000, train_loss: 1.567550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  3000, train_loss: 1.505845\n",
      "batch_idx:  4000, train_loss: 1.319196\n",
      "batch_idx:  5000, train_loss: 1.158626\n",
      "batch_idx:  6000, train_loss: 1.092356\n",
      "final train_loss: 1.041124\n",
      "batch_idx:     0, valid_loss: 11.041579\n",
      "batch_idx:  1000, valid_loss: 15.180614\n",
      "batch_idx:  2000, valid_loss: 16.002794\n",
      "batch_idx:  3000, valid_loss: 16.303673\n",
      "batch_idx:  4000, valid_loss: 16.465427\n",
      "batch_idx:  5000, valid_loss: 16.517878\n",
      "batch_idx:  6000, valid_loss: 16.125486\n",
      "Training Loss: 1.041124 \tValidation Loss: 15.601315\n",
      "epoch:    65\n",
      "batch_idx:     0, train_loss: 14.094395\n",
      "batch_idx:  1000, train_loss: 0.926826\n",
      "batch_idx:  2000, train_loss: 0.982009\n",
      "batch_idx:  3000, train_loss: 1.073463\n",
      "batch_idx:  4000, train_loss: 1.160406\n",
      "batch_idx:  5000, train_loss: 1.277279\n",
      "batch_idx:  6000, train_loss: 1.250036\n",
      "final train_loss: 1.269241\n",
      "batch_idx:     0, valid_loss: 9.568274\n",
      "batch_idx:  1000, valid_loss: 11.077453\n",
      "batch_idx:  2000, valid_loss: 11.985144\n",
      "batch_idx:  3000, valid_loss: 12.307068\n",
      "batch_idx:  4000, valid_loss: 12.490644\n",
      "batch_idx:  5000, valid_loss: 12.510234\n",
      "batch_idx:  6000, valid_loss: 12.272173\n",
      "Training Loss: 1.269241 \tValidation Loss: 11.759778\n",
      "epoch:    66\n",
      "batch_idx:     0, train_loss: 12.376981\n",
      "batch_idx:  1000, train_loss: 0.803432\n",
      "batch_idx:  2000, train_loss: 0.861308\n",
      "batch_idx:  3000, train_loss: 0.881023\n",
      "batch_idx:  4000, train_loss: 0.967674\n",
      "batch_idx:  5000, train_loss: 1.119237\n",
      "batch_idx:  6000, train_loss: 1.203435\n",
      "final train_loss: 1.288391\n",
      "batch_idx:     0, valid_loss: 9.937853\n",
      "batch_idx:  1000, valid_loss: 9.582743\n",
      "batch_idx:  2000, valid_loss: 9.584565\n",
      "batch_idx:  3000, valid_loss: 9.606819\n",
      "batch_idx:  4000, valid_loss: 9.637606\n",
      "batch_idx:  5000, valid_loss: 9.621001\n",
      "batch_idx:  6000, valid_loss: 9.401350\n",
      "Training Loss: 1.288391 \tValidation Loss: 9.032110\n",
      "Training loss decreased (9.369958 --> 9.032110).  Saving model ...\n",
      "epoch:    67\n",
      "batch_idx:     0, train_loss: 4.605638\n",
      "batch_idx:  1000, train_loss: 1.416589\n",
      "batch_idx:  2000, train_loss: 1.169340\n",
      "batch_idx:  3000, train_loss: 1.139399\n",
      "batch_idx:  4000, train_loss: 1.158651\n",
      "batch_idx:  5000, train_loss: 1.233931\n",
      "batch_idx:  6000, train_loss: 1.378096\n",
      "final train_loss: 1.502079\n",
      "batch_idx:     0, valid_loss: 10.931342\n",
      "batch_idx:  1000, valid_loss: 10.887362\n",
      "batch_idx:  2000, valid_loss: 11.490637\n",
      "batch_idx:  3000, valid_loss: 11.658045\n",
      "batch_idx:  4000, valid_loss: 11.617437\n",
      "batch_idx:  5000, valid_loss: 11.510441\n",
      "batch_idx:  6000, valid_loss: 11.209228\n",
      "Training Loss: 1.502079 \tValidation Loss: 10.757290\n",
      "epoch:    68\n",
      "batch_idx:     0, train_loss: 13.558310\n",
      "batch_idx:  1000, train_loss: 0.660760\n",
      "batch_idx:  2000, train_loss: 0.839456\n",
      "batch_idx:  3000, train_loss: 0.890336\n",
      "batch_idx:  4000, train_loss: 0.942158\n",
      "batch_idx:  5000, train_loss: 1.041594\n",
      "batch_idx:  6000, train_loss: 0.955440\n",
      "final train_loss: 0.907028\n",
      "batch_idx:     0, valid_loss: 6.958017\n",
      "batch_idx:  1000, valid_loss: 14.346814\n",
      "batch_idx:  2000, valid_loss: 17.523630\n",
      "batch_idx:  3000, valid_loss: 19.314665\n",
      "batch_idx:  4000, valid_loss: 20.322651\n",
      "batch_idx:  5000, valid_loss: 20.997604\n",
      "batch_idx:  6000, valid_loss: 20.925272\n",
      "Training Loss: 0.907028 \tValidation Loss: 20.055864\n",
      "epoch:    69\n",
      "batch_idx:     0, train_loss: 5.802794\n",
      "batch_idx:  1000, train_loss: 0.349274\n",
      "batch_idx:  2000, train_loss: 0.354164\n",
      "batch_idx:  3000, train_loss: 0.352337\n",
      "batch_idx:  4000, train_loss: 0.366263\n",
      "batch_idx:  5000, train_loss: 0.383640\n",
      "batch_idx:  6000, train_loss: 0.388732\n",
      "final train_loss: 0.396770\n",
      "batch_idx:     0, valid_loss: 6.156102\n",
      "batch_idx:  1000, valid_loss: 14.830297\n",
      "batch_idx:  2000, valid_loss: 17.745758\n",
      "batch_idx:  3000, valid_loss: 19.464760\n",
      "batch_idx:  4000, valid_loss: 20.469841\n",
      "batch_idx:  5000, valid_loss: 21.059841\n",
      "batch_idx:  6000, valid_loss: 20.858883\n",
      "Training Loss: 0.396770 \tValidation Loss: 19.987999\n",
      "epoch:    70\n",
      "batch_idx:     0, train_loss: 6.619723\n",
      "batch_idx:  1000, train_loss: 0.349556\n",
      "batch_idx:  2000, train_loss: 0.344454\n",
      "batch_idx:  3000, train_loss: 0.352600\n",
      "batch_idx:  4000, train_loss: 0.364466\n",
      "batch_idx:  5000, train_loss: 0.404745\n",
      "batch_idx:  6000, train_loss: 0.429152\n",
      "final train_loss: 0.452483\n",
      "batch_idx:     0, valid_loss: 6.851217\n",
      "batch_idx:  1000, valid_loss: 12.813013\n",
      "batch_idx:  2000, valid_loss: 15.232768\n",
      "batch_idx:  3000, valid_loss: 16.238230\n",
      "batch_idx:  4000, valid_loss: 16.651722\n",
      "batch_idx:  5000, valid_loss: 16.755417\n",
      "batch_idx:  6000, valid_loss: 16.400612\n",
      "Training Loss: 0.452483 \tValidation Loss: 15.762467\n",
      "epoch:    71\n",
      "batch_idx:     0, train_loss: 6.754313\n",
      "batch_idx:  1000, train_loss: 0.501915\n",
      "batch_idx:  2000, train_loss: 0.650465\n",
      "batch_idx:  3000, train_loss: 0.827940\n",
      "batch_idx:  4000, train_loss: 0.905602\n",
      "batch_idx:  5000, train_loss: 1.026344\n",
      "batch_idx:  6000, train_loss: 1.158861\n",
      "final train_loss: 1.215300\n",
      "batch_idx:     0, valid_loss: 11.563426\n",
      "batch_idx:  1000, valid_loss: 12.375237\n",
      "batch_idx:  2000, valid_loss: 13.220254\n",
      "batch_idx:  3000, valid_loss: 13.550248\n",
      "batch_idx:  4000, valid_loss: 13.739456\n",
      "batch_idx:  5000, valid_loss: 13.848036\n",
      "batch_idx:  6000, valid_loss: 13.749029\n",
      "Training Loss: 1.215300 \tValidation Loss: 13.194124\n",
      "epoch:    72\n",
      "batch_idx:     0, train_loss: 14.478257\n",
      "batch_idx:  1000, train_loss: 0.874853\n",
      "batch_idx:  2000, train_loss: 1.093964\n",
      "batch_idx:  3000, train_loss: 1.105536\n",
      "batch_idx:  4000, train_loss: 1.071793\n",
      "batch_idx:  5000, train_loss: 1.008887\n",
      "batch_idx:  6000, train_loss: 1.009540\n",
      "final train_loss: 1.059138\n",
      "batch_idx:     0, valid_loss: 11.739729\n",
      "batch_idx:  1000, valid_loss: 15.600368\n",
      "batch_idx:  2000, valid_loss: 15.829677\n",
      "batch_idx:  3000, valid_loss: 15.891707\n",
      "batch_idx:  4000, valid_loss: 15.971610\n",
      "batch_idx:  5000, valid_loss: 16.007631\n",
      "batch_idx:  6000, valid_loss: 15.860779\n",
      "Training Loss: 1.059138 \tValidation Loss: 15.321693\n",
      "epoch:    73\n",
      "batch_idx:     0, train_loss: 13.923553\n",
      "batch_idx:  1000, train_loss: 1.307527\n",
      "batch_idx:  2000, train_loss: 1.625387\n",
      "batch_idx:  3000, train_loss: 1.466899\n",
      "batch_idx:  4000, train_loss: 1.439623\n",
      "batch_idx:  5000, train_loss: 1.368434\n",
      "batch_idx:  6000, train_loss: 1.306549\n",
      "final train_loss: 1.330210\n",
      "batch_idx:     0, valid_loss: 9.374769\n",
      "batch_idx:  1000, valid_loss: 9.411760\n",
      "batch_idx:  2000, valid_loss: 9.354211\n",
      "batch_idx:  3000, valid_loss: 9.347916\n",
      "batch_idx:  4000, valid_loss: 9.382683\n",
      "batch_idx:  5000, valid_loss: 9.386274\n",
      "batch_idx:  6000, valid_loss: 9.274553\n",
      "Training Loss: 1.330210 \tValidation Loss: 8.972283\n",
      "Training loss decreased (9.032110 --> 8.972283).  Saving model ...\n",
      "epoch:    74\n",
      "batch_idx:     0, train_loss: 11.317358\n",
      "batch_idx:  1000, train_loss: 1.699175\n",
      "batch_idx:  2000, train_loss: 1.796748\n",
      "batch_idx:  3000, train_loss: 1.738682\n",
      "batch_idx:  4000, train_loss: 1.716168\n",
      "batch_idx:  5000, train_loss: 1.762822\n",
      "batch_idx:  6000, train_loss: 1.738994\n",
      "final train_loss: 1.826322\n",
      "batch_idx:     0, valid_loss: 9.761194\n",
      "batch_idx:  1000, valid_loss: 9.883505\n",
      "batch_idx:  2000, valid_loss: 9.938064\n",
      "batch_idx:  3000, valid_loss: 9.970667\n",
      "batch_idx:  4000, valid_loss: 10.024817\n",
      "batch_idx:  5000, valid_loss: 9.910854\n",
      "batch_idx:  6000, valid_loss: 9.579038\n",
      "Training Loss: 1.826322 \tValidation Loss: 9.190899\n",
      "epoch:    75\n",
      "batch_idx:     0, train_loss: 12.387470\n",
      "batch_idx:  1000, train_loss: 2.163049\n",
      "batch_idx:  2000, train_loss: 1.715344\n",
      "batch_idx:  3000, train_loss: 1.503721\n",
      "batch_idx:  4000, train_loss: 1.501203\n",
      "batch_idx:  5000, train_loss: 1.540785\n",
      "batch_idx:  6000, train_loss: 1.627655\n",
      "final train_loss: 1.690814\n",
      "batch_idx:     0, valid_loss: 10.177626\n",
      "batch_idx:  1000, valid_loss: 10.215176\n",
      "batch_idx:  2000, valid_loss: 10.304701\n",
      "batch_idx:  3000, valid_loss: 10.289209\n",
      "batch_idx:  4000, valid_loss: 10.178407\n",
      "batch_idx:  5000, valid_loss: 10.010824\n",
      "batch_idx:  6000, valid_loss: 9.707425\n",
      "Training Loss: 1.690814 \tValidation Loss: 9.333051\n",
      "epoch:    76\n",
      "batch_idx:     0, train_loss: 11.199505\n",
      "batch_idx:  1000, train_loss: 1.575611\n",
      "batch_idx:  2000, train_loss: 1.596106\n",
      "batch_idx:  3000, train_loss: 1.528078\n",
      "batch_idx:  4000, train_loss: 1.605190\n",
      "batch_idx:  5000, train_loss: 1.598122\n",
      "batch_idx:  6000, train_loss: 1.407551\n",
      "final train_loss: 1.311562\n",
      "batch_idx:     0, valid_loss: 6.703632\n",
      "batch_idx:  1000, valid_loss: 14.317627\n",
      "batch_idx:  2000, valid_loss: 17.767614\n",
      "batch_idx:  3000, valid_loss: 19.253965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  4000, valid_loss: 20.088739\n",
      "batch_idx:  5000, valid_loss: 20.608150\n",
      "batch_idx:  6000, valid_loss: 20.468855\n",
      "Training Loss: 1.311562 \tValidation Loss: 19.616318\n",
      "epoch:    77\n",
      "batch_idx:     0, train_loss: 7.120411\n",
      "batch_idx:  1000, train_loss: 0.332116\n",
      "batch_idx:  2000, train_loss: 0.344770\n",
      "batch_idx:  3000, train_loss: 0.357642\n",
      "batch_idx:  4000, train_loss: 0.364562\n",
      "batch_idx:  5000, train_loss: 0.385492\n",
      "batch_idx:  6000, train_loss: 0.421033\n",
      "final train_loss: 0.445464\n",
      "batch_idx:     0, valid_loss: 8.287199\n",
      "batch_idx:  1000, valid_loss: 12.020298\n",
      "batch_idx:  2000, valid_loss: 15.015246\n",
      "batch_idx:  3000, valid_loss: 16.208565\n",
      "batch_idx:  4000, valid_loss: 16.794836\n",
      "batch_idx:  5000, valid_loss: 17.050018\n",
      "batch_idx:  6000, valid_loss: 16.719975\n",
      "Training Loss: 0.445464 \tValidation Loss: 16.065683\n",
      "epoch:    78\n",
      "batch_idx:     0, train_loss: 5.442532\n",
      "batch_idx:  1000, train_loss: 0.510644\n",
      "batch_idx:  2000, train_loss: 0.519972\n",
      "batch_idx:  3000, train_loss: 0.551641\n",
      "batch_idx:  4000, train_loss: 0.632439\n",
      "batch_idx:  5000, train_loss: 0.652864\n",
      "batch_idx:  6000, train_loss: 0.683232\n",
      "final train_loss: 0.660199\n",
      "batch_idx:     0, valid_loss: 6.656869\n",
      "batch_idx:  1000, valid_loss: 12.708221\n",
      "batch_idx:  2000, valid_loss: 16.246721\n",
      "batch_idx:  3000, valid_loss: 18.081491\n",
      "batch_idx:  4000, valid_loss: 19.049511\n",
      "batch_idx:  5000, valid_loss: 19.835331\n",
      "batch_idx:  6000, valid_loss: 19.860525\n",
      "Training Loss: 0.660199 \tValidation Loss: 19.042370\n",
      "epoch:    79\n",
      "batch_idx:     0, train_loss: 8.028047\n",
      "batch_idx:  1000, train_loss: 0.308356\n",
      "batch_idx:  2000, train_loss: 0.323200\n",
      "batch_idx:  3000, train_loss: 0.333257\n",
      "batch_idx:  4000, train_loss: 0.344397\n",
      "batch_idx:  5000, train_loss: 0.359060\n",
      "batch_idx:  6000, train_loss: 0.389318\n",
      "final train_loss: 0.397421\n",
      "batch_idx:     0, valid_loss: 6.434761\n",
      "batch_idx:  1000, valid_loss: 14.344660\n",
      "batch_idx:  2000, valid_loss: 18.054245\n",
      "batch_idx:  3000, valid_loss: 19.658373\n",
      "batch_idx:  4000, valid_loss: 20.577919\n",
      "batch_idx:  5000, valid_loss: 21.170843\n",
      "batch_idx:  6000, valid_loss: 21.060179\n",
      "Training Loss: 0.397421 \tValidation Loss: 20.183037\n",
      "epoch:    80\n",
      "batch_idx:     0, train_loss: 5.066273\n",
      "batch_idx:  1000, train_loss: 0.324951\n",
      "batch_idx:  2000, train_loss: 0.332254\n",
      "batch_idx:  3000, train_loss: 0.342073\n",
      "batch_idx:  4000, train_loss: 0.351862\n",
      "batch_idx:  5000, train_loss: 0.375960\n",
      "batch_idx:  6000, train_loss: 0.405975\n",
      "final train_loss: 0.409849\n",
      "batch_idx:     0, valid_loss: 6.419971\n",
      "batch_idx:  1000, valid_loss: 13.901312\n",
      "batch_idx:  2000, valid_loss: 17.986687\n",
      "batch_idx:  3000, valid_loss: 19.961435\n",
      "batch_idx:  4000, valid_loss: 21.022131\n",
      "batch_idx:  5000, valid_loss: 21.716845\n",
      "batch_idx:  6000, valid_loss: 21.652924\n",
      "Training Loss: 0.409849 \tValidation Loss: 20.761410\n",
      "epoch:    81\n",
      "batch_idx:     0, train_loss: 4.704314\n",
      "batch_idx:  1000, train_loss: 0.298763\n",
      "batch_idx:  2000, train_loss: 0.315191\n",
      "batch_idx:  3000, train_loss: 0.322072\n",
      "batch_idx:  4000, train_loss: 0.333987\n",
      "batch_idx:  5000, train_loss: 0.358783\n",
      "batch_idx:  6000, train_loss: 0.384376\n",
      "final train_loss: 0.394485\n",
      "batch_idx:     0, valid_loss: 6.713636\n",
      "batch_idx:  1000, valid_loss: 14.556845\n",
      "batch_idx:  2000, valid_loss: 18.743973\n",
      "batch_idx:  3000, valid_loss: 20.739491\n",
      "batch_idx:  4000, valid_loss: 21.901037\n",
      "batch_idx:  5000, valid_loss: 22.687527\n",
      "batch_idx:  6000, valid_loss: 22.539085\n",
      "Training Loss: 0.394485 \tValidation Loss: 21.640224\n",
      "epoch:    82\n",
      "batch_idx:     0, train_loss: 6.809234\n",
      "batch_idx:  1000, train_loss: 0.324275\n",
      "batch_idx:  2000, train_loss: 0.338436\n",
      "batch_idx:  3000, train_loss: 0.345452\n",
      "batch_idx:  4000, train_loss: 0.358955\n",
      "batch_idx:  5000, train_loss: 0.387670\n",
      "batch_idx:  6000, train_loss: 0.423332\n",
      "final train_loss: 0.425789\n",
      "batch_idx:     0, valid_loss: 7.018436\n",
      "batch_idx:  1000, valid_loss: 13.666177\n",
      "batch_idx:  2000, valid_loss: 17.183191\n",
      "batch_idx:  3000, valid_loss: 18.836905\n",
      "batch_idx:  4000, valid_loss: 19.704498\n",
      "batch_idx:  5000, valid_loss: 20.315290\n",
      "batch_idx:  6000, valid_loss: 20.296873\n",
      "Training Loss: 0.425789 \tValidation Loss: 19.524830\n",
      "epoch:    83\n",
      "batch_idx:     0, train_loss: 4.443539\n",
      "batch_idx:  1000, train_loss: 0.306238\n",
      "batch_idx:  2000, train_loss: 0.323618\n",
      "batch_idx:  3000, train_loss: 0.334100\n",
      "batch_idx:  4000, train_loss: 0.348268\n",
      "batch_idx:  5000, train_loss: 0.360833\n",
      "batch_idx:  6000, train_loss: 0.429447\n",
      "final train_loss: 0.429820\n",
      "batch_idx:     0, valid_loss: 6.832461\n",
      "batch_idx:  1000, valid_loss: 13.853311\n",
      "batch_idx:  2000, valid_loss: 17.321270\n",
      "batch_idx:  3000, valid_loss: 18.808001\n",
      "batch_idx:  4000, valid_loss: 19.671673\n",
      "batch_idx:  5000, valid_loss: 20.284803\n",
      "batch_idx:  6000, valid_loss: 20.232689\n",
      "Training Loss: 0.429820 \tValidation Loss: 19.441456\n",
      "epoch:    84\n",
      "batch_idx:     0, train_loss: 6.696292\n",
      "batch_idx:  1000, train_loss: 0.319619\n",
      "batch_idx:  2000, train_loss: 0.326667\n",
      "batch_idx:  3000, train_loss: 0.335941\n",
      "batch_idx:  4000, train_loss: 0.345861\n",
      "batch_idx:  5000, train_loss: 0.361738\n",
      "batch_idx:  6000, train_loss: 0.422828\n",
      "final train_loss: 0.426339\n",
      "batch_idx:     0, valid_loss: 7.045774\n",
      "batch_idx:  1000, valid_loss: 14.688703\n",
      "batch_idx:  2000, valid_loss: 18.952925\n",
      "batch_idx:  3000, valid_loss: 20.916098\n",
      "batch_idx:  4000, valid_loss: 21.948187\n",
      "batch_idx:  5000, valid_loss: 22.601694\n",
      "batch_idx:  6000, valid_loss: 22.437662\n",
      "Training Loss: 0.426339 \tValidation Loss: 21.554626\n",
      "epoch:    85\n",
      "batch_idx:     0, train_loss: 3.949379\n",
      "batch_idx:  1000, train_loss: 0.306659\n",
      "batch_idx:  2000, train_loss: 0.321281\n",
      "batch_idx:  3000, train_loss: 0.334685\n",
      "batch_idx:  4000, train_loss: 0.354547\n",
      "batch_idx:  5000, train_loss: 0.376764\n",
      "batch_idx:  6000, train_loss: 0.429056\n",
      "final train_loss: 0.527008\n",
      "batch_idx:     0, valid_loss: 6.590199\n",
      "batch_idx:  1000, valid_loss: 10.932029\n",
      "batch_idx:  2000, valid_loss: 16.388752\n",
      "batch_idx:  3000, valid_loss: 19.017406\n",
      "batch_idx:  4000, valid_loss: 20.534925\n",
      "batch_idx:  5000, valid_loss: 21.694960\n",
      "batch_idx:  6000, valid_loss: 22.164846\n",
      "Training Loss: 0.527008 \tValidation Loss: 21.269663\n",
      "epoch:    86\n",
      "batch_idx:     0, train_loss: 6.119543\n",
      "batch_idx:  1000, train_loss: 0.279116\n",
      "batch_idx:  2000, train_loss: 0.301533\n",
      "batch_idx:  3000, train_loss: 0.313215\n",
      "batch_idx:  4000, train_loss: 0.324251\n",
      "batch_idx:  5000, train_loss: 0.344270\n",
      "batch_idx:  6000, train_loss: 0.376187\n",
      "final train_loss: 0.396522\n",
      "batch_idx:     0, valid_loss: 6.720872\n",
      "batch_idx:  1000, valid_loss: 12.778274\n",
      "batch_idx:  2000, valid_loss: 15.918583\n",
      "batch_idx:  3000, valid_loss: 17.076851\n",
      "batch_idx:  4000, valid_loss: 17.593235\n",
      "batch_idx:  5000, valid_loss: 17.930798\n",
      "batch_idx:  6000, valid_loss: 18.143457\n",
      "Training Loss: 0.396522 \tValidation Loss: 17.488825\n",
      "epoch:    87\n",
      "batch_idx:     0, train_loss: 6.956540\n",
      "batch_idx:  1000, train_loss: 0.395251\n",
      "batch_idx:  2000, train_loss: 0.457070\n",
      "batch_idx:  3000, train_loss: 0.567248\n",
      "batch_idx:  4000, train_loss: 0.702306\n",
      "batch_idx:  5000, train_loss: 0.796270\n",
      "batch_idx:  6000, train_loss: 0.847095\n",
      "final train_loss: 0.826033\n",
      "batch_idx:     0, valid_loss: 11.471008\n",
      "batch_idx:  1000, valid_loss: 16.616581\n",
      "batch_idx:  2000, valid_loss: 17.898054\n",
      "batch_idx:  3000, valid_loss: 18.191977\n",
      "batch_idx:  4000, valid_loss: 18.273233\n",
      "batch_idx:  5000, valid_loss: 18.313709\n",
      "batch_idx:  6000, valid_loss: 17.905050\n",
      "Training Loss: 0.826033 \tValidation Loss: 17.199593\n",
      "epoch:    88\n",
      "batch_idx:     0, train_loss: 10.188557\n",
      "batch_idx:  1000, train_loss: 0.542214\n",
      "batch_idx:  2000, train_loss: 0.648891\n",
      "batch_idx:  3000, train_loss: 0.713510\n",
      "batch_idx:  4000, train_loss: 0.968388\n",
      "batch_idx:  5000, train_loss: 1.163151\n",
      "batch_idx:  6000, train_loss: 1.240101\n",
      "final train_loss: 1.312839\n",
      "batch_idx:     0, valid_loss: 7.476431\n",
      "batch_idx:  1000, valid_loss: 8.639076\n",
      "batch_idx:  2000, valid_loss: 9.216039\n",
      "batch_idx:  3000, valid_loss: 9.440040\n",
      "batch_idx:  4000, valid_loss: 9.520980\n",
      "batch_idx:  5000, valid_loss: 9.517306\n",
      "batch_idx:  6000, valid_loss: 9.595750\n",
      "Training Loss: 1.312839 \tValidation Loss: 9.467716\n",
      "epoch:    89\n",
      "batch_idx:     0, train_loss: 8.631877\n",
      "batch_idx:  1000, train_loss: 1.450864\n",
      "batch_idx:  2000, train_loss: 1.593543\n",
      "batch_idx:  3000, train_loss: 1.640349\n",
      "batch_idx:  4000, train_loss: 1.582136\n",
      "batch_idx:  5000, train_loss: 1.527723\n",
      "batch_idx:  6000, train_loss: 1.531801\n",
      "final train_loss: 1.514581\n",
      "batch_idx:     0, valid_loss: 9.629178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  1000, valid_loss: 9.308616\n",
      "batch_idx:  2000, valid_loss: 9.299588\n",
      "batch_idx:  3000, valid_loss: 9.338302\n",
      "batch_idx:  4000, valid_loss: 9.351686\n",
      "batch_idx:  5000, valid_loss: 9.307673\n",
      "batch_idx:  6000, valid_loss: 9.339553\n",
      "Training Loss: 1.514581 \tValidation Loss: 9.093248\n",
      "epoch:    90\n",
      "batch_idx:     0, train_loss: 11.136989\n",
      "batch_idx:  1000, train_loss: 1.470454\n",
      "batch_idx:  2000, train_loss: 1.373357\n",
      "batch_idx:  3000, train_loss: 1.327893\n",
      "batch_idx:  4000, train_loss: 1.396127\n",
      "batch_idx:  5000, train_loss: 1.530256\n",
      "batch_idx:  6000, train_loss: 1.657260\n",
      "final train_loss: 1.686203\n",
      "batch_idx:     0, valid_loss: 9.717545\n",
      "batch_idx:  1000, valid_loss: 9.925369\n",
      "batch_idx:  2000, valid_loss: 9.938516\n",
      "batch_idx:  3000, valid_loss: 9.954814\n",
      "batch_idx:  4000, valid_loss: 9.968889\n",
      "batch_idx:  5000, valid_loss: 9.975702\n",
      "batch_idx:  6000, valid_loss: 9.882918\n",
      "Training Loss: 1.686203 \tValidation Loss: 9.547972\n",
      "epoch:    91\n",
      "batch_idx:     0, train_loss: 4.654214\n",
      "batch_idx:  1000, train_loss: 1.550253\n",
      "batch_idx:  2000, train_loss: 1.497225\n",
      "batch_idx:  3000, train_loss: 1.586774\n",
      "batch_idx:  4000, train_loss: 1.480348\n",
      "batch_idx:  5000, train_loss: 1.460412\n",
      "batch_idx:  6000, train_loss: 1.484568\n",
      "final train_loss: 1.509863\n",
      "batch_idx:     0, valid_loss: 5.846249\n",
      "batch_idx:  1000, valid_loss: 6.813857\n",
      "batch_idx:  2000, valid_loss: 7.684979\n",
      "batch_idx:  3000, valid_loss: 8.208658\n",
      "batch_idx:  4000, valid_loss: 8.492238\n",
      "batch_idx:  5000, valid_loss: 8.856122\n",
      "batch_idx:  6000, valid_loss: 8.732799\n",
      "Training Loss: 1.509863 \tValidation Loss: 8.297111\n",
      "Training loss decreased (8.972283 --> 8.297111).  Saving model ...\n",
      "epoch:    92\n",
      "batch_idx:     0, train_loss: 5.553777\n",
      "batch_idx:  1000, train_loss: 0.299833\n",
      "batch_idx:  2000, train_loss: 0.292578\n",
      "batch_idx:  3000, train_loss: 0.298323\n",
      "batch_idx:  4000, train_loss: 0.314425\n",
      "batch_idx:  5000, train_loss: 0.317546\n",
      "batch_idx:  6000, train_loss: 0.332908\n",
      "final train_loss: 0.358721\n",
      "batch_idx:     0, valid_loss: 5.219268\n",
      "batch_idx:  1000, valid_loss: 7.604368\n",
      "batch_idx:  2000, valid_loss: 8.580178\n",
      "batch_idx:  3000, valid_loss: 9.138700\n",
      "batch_idx:  4000, valid_loss: 9.415295\n",
      "batch_idx:  5000, valid_loss: 9.801826\n",
      "batch_idx:  6000, valid_loss: 9.711494\n",
      "Training Loss: 0.358721 \tValidation Loss: 9.294367\n",
      "epoch:    93\n",
      "batch_idx:     0, train_loss: 4.756515\n",
      "batch_idx:  1000, train_loss: 0.317444\n",
      "batch_idx:  2000, train_loss: 0.313510\n",
      "batch_idx:  3000, train_loss: 0.314462\n",
      "batch_idx:  4000, train_loss: 0.326515\n",
      "batch_idx:  5000, train_loss: 0.338288\n",
      "batch_idx:  6000, train_loss: 0.358906\n",
      "final train_loss: 0.386965\n",
      "batch_idx:     0, valid_loss: 3.747766\n",
      "batch_idx:  1000, valid_loss: 7.605625\n",
      "batch_idx:  2000, valid_loss: 8.747596\n",
      "batch_idx:  3000, valid_loss: 9.354899\n",
      "batch_idx:  4000, valid_loss: 9.652273\n",
      "batch_idx:  5000, valid_loss: 10.052692\n",
      "batch_idx:  6000, valid_loss: 9.979362\n",
      "Training Loss: 0.386965 \tValidation Loss: 9.576402\n",
      "epoch:    94\n",
      "batch_idx:     0, train_loss: 3.627859\n",
      "batch_idx:  1000, train_loss: 0.527899\n",
      "batch_idx:  2000, train_loss: 0.419392\n",
      "batch_idx:  3000, train_loss: 0.381751\n",
      "batch_idx:  4000, train_loss: 0.373380\n",
      "batch_idx:  5000, train_loss: 0.370788\n",
      "batch_idx:  6000, train_loss: 0.380908\n",
      "final train_loss: 0.407610\n",
      "batch_idx:     0, valid_loss: 4.401739\n",
      "batch_idx:  1000, valid_loss: 6.457993\n",
      "batch_idx:  2000, valid_loss: 7.953916\n",
      "batch_idx:  3000, valid_loss: 8.679108\n",
      "batch_idx:  4000, valid_loss: 9.053477\n",
      "batch_idx:  5000, valid_loss: 9.501376\n",
      "batch_idx:  6000, valid_loss: 9.491821\n",
      "Training Loss: 0.407610 \tValidation Loss: 9.151642\n",
      "epoch:    95\n",
      "batch_idx:     0, train_loss: 4.486878\n",
      "batch_idx:  1000, train_loss: 0.668174\n",
      "batch_idx:  2000, train_loss: 0.565300\n",
      "batch_idx:  3000, train_loss: 0.657562\n",
      "batch_idx:  4000, train_loss: 0.831973\n",
      "batch_idx:  5000, train_loss: 0.980678\n",
      "batch_idx:  6000, train_loss: 1.014502\n",
      "final train_loss: 1.118515\n",
      "batch_idx:     0, valid_loss: 9.660976\n",
      "batch_idx:  1000, valid_loss: 9.828702\n",
      "batch_idx:  2000, valid_loss: 9.859636\n",
      "batch_idx:  3000, valid_loss: 9.911834\n",
      "batch_idx:  4000, valid_loss: 9.932495\n",
      "batch_idx:  5000, valid_loss: 9.934149\n",
      "batch_idx:  6000, valid_loss: 9.760737\n",
      "Training Loss: 1.118515 \tValidation Loss: 9.418171\n",
      "epoch:    96\n",
      "batch_idx:     0, train_loss: 12.157742\n",
      "batch_idx:  1000, train_loss: 1.421115\n",
      "batch_idx:  2000, train_loss: 1.432580\n",
      "batch_idx:  3000, train_loss: 1.464761\n",
      "batch_idx:  4000, train_loss: 1.576512\n",
      "batch_idx:  5000, train_loss: 1.613856\n",
      "batch_idx:  6000, train_loss: 1.560783\n",
      "final train_loss: 1.637846\n",
      "batch_idx:     0, valid_loss: 10.326960\n",
      "batch_idx:  1000, valid_loss: 10.520391\n",
      "batch_idx:  2000, valid_loss: 10.598455\n",
      "batch_idx:  3000, valid_loss: 10.612177\n",
      "batch_idx:  4000, valid_loss: 10.649789\n",
      "batch_idx:  5000, valid_loss: 10.481690\n",
      "batch_idx:  6000, valid_loss: 10.201208\n",
      "Training Loss: 1.637846 \tValidation Loss: 9.816435\n",
      "epoch:    97\n",
      "batch_idx:     0, train_loss: 13.036751\n",
      "batch_idx:  1000, train_loss: 0.925969\n",
      "batch_idx:  2000, train_loss: 0.738633\n",
      "batch_idx:  3000, train_loss: 0.830185\n",
      "batch_idx:  4000, train_loss: 0.955165\n",
      "batch_idx:  5000, train_loss: 1.046491\n",
      "batch_idx:  6000, train_loss: 1.089072\n",
      "final train_loss: 1.122931\n",
      "batch_idx:     0, valid_loss: 10.820569\n",
      "batch_idx:  1000, valid_loss: 12.223156\n",
      "batch_idx:  2000, valid_loss: 14.842558\n",
      "batch_idx:  3000, valid_loss: 15.613455\n",
      "batch_idx:  4000, valid_loss: 15.917038\n",
      "batch_idx:  5000, valid_loss: 16.081095\n",
      "batch_idx:  6000, valid_loss: 15.981400\n",
      "Training Loss: 1.122931 \tValidation Loss: 15.400746\n",
      "epoch:    98\n",
      "batch_idx:     0, train_loss: 13.454880\n",
      "batch_idx:  1000, train_loss: 0.786543\n",
      "batch_idx:  2000, train_loss: 1.188710\n",
      "batch_idx:  3000, train_loss: 1.255219\n",
      "batch_idx:  4000, train_loss: 1.563556\n",
      "batch_idx:  5000, train_loss: 1.855895\n",
      "batch_idx:  6000, train_loss: 1.795019\n",
      "final train_loss: 1.776004\n",
      "batch_idx:     0, valid_loss: 6.907108\n",
      "batch_idx:  1000, valid_loss: 9.031008\n",
      "batch_idx:  2000, valid_loss: 9.852790\n",
      "batch_idx:  3000, valid_loss: 10.121208\n",
      "batch_idx:  4000, valid_loss: 10.188252\n",
      "batch_idx:  5000, valid_loss: 10.231176\n",
      "batch_idx:  6000, valid_loss: 10.278108\n",
      "Training Loss: 1.776004 \tValidation Loss: 9.969815\n",
      "epoch:    99\n",
      "batch_idx:     0, train_loss: 8.740380\n",
      "batch_idx:  1000, train_loss: 1.563519\n",
      "batch_idx:  2000, train_loss: 1.661922\n",
      "batch_idx:  3000, train_loss: 1.822415\n",
      "batch_idx:  4000, train_loss: 1.974011\n",
      "batch_idx:  5000, train_loss: 1.937498\n",
      "batch_idx:  6000, train_loss: 1.980986\n",
      "final train_loss: 2.042965\n",
      "batch_idx:     0, valid_loss: 8.856686\n",
      "batch_idx:  1000, valid_loss: 9.745530\n",
      "batch_idx:  2000, valid_loss: 9.774741\n",
      "batch_idx:  3000, valid_loss: 9.787655\n",
      "batch_idx:  4000, valid_loss: 9.657390\n",
      "batch_idx:  5000, valid_loss: 9.450257\n",
      "batch_idx:  6000, valid_loss: 9.135903\n",
      "Training Loss: 2.042965 \tValidation Loss: 8.765762\n",
      "epoch:   100\n",
      "batch_idx:     0, train_loss: 10.216051\n",
      "batch_idx:  1000, train_loss: 2.214791\n",
      "batch_idx:  2000, train_loss: 2.337861\n",
      "batch_idx:  3000, train_loss: 2.478432\n",
      "batch_idx:  4000, train_loss: 2.499157\n",
      "batch_idx:  5000, train_loss: 2.361843\n",
      "batch_idx:  6000, train_loss: 2.340122\n",
      "final train_loss: 2.414893\n",
      "batch_idx:     0, valid_loss: 6.805881\n",
      "batch_idx:  1000, valid_loss: 6.843451\n",
      "batch_idx:  2000, valid_loss: 6.880488\n",
      "batch_idx:  3000, valid_loss: 6.901064\n",
      "batch_idx:  4000, valid_loss: 6.780652\n",
      "batch_idx:  5000, valid_loss: 6.658003\n",
      "batch_idx:  6000, valid_loss: 6.369954\n",
      "Training Loss: 2.414893 \tValidation Loss: 6.093254\n",
      "Training loss decreased (8.297111 --> 6.093254).  Saving model ...\n",
      "epoch:   101\n",
      "batch_idx:     0, train_loss: 8.141867\n",
      "batch_idx:  1000, train_loss: 2.690763\n",
      "batch_idx:  2000, train_loss: 2.188090\n",
      "batch_idx:  3000, train_loss: 2.037104\n",
      "batch_idx:  4000, train_loss: 2.085042\n",
      "batch_idx:  5000, train_loss: 2.076359\n",
      "batch_idx:  6000, train_loss: 1.985411\n",
      "final train_loss: 1.979037\n",
      "batch_idx:     0, valid_loss: 8.470219\n",
      "batch_idx:  1000, valid_loss: 9.057123\n",
      "batch_idx:  2000, valid_loss: 10.848057\n",
      "batch_idx:  3000, valid_loss: 11.472142\n",
      "batch_idx:  4000, valid_loss: 11.637402\n",
      "batch_idx:  5000, valid_loss: 11.697101\n",
      "batch_idx:  6000, valid_loss: 11.185077\n",
      "Training Loss: 1.979037 \tValidation Loss: 10.689777\n",
      "epoch:   102\n",
      "batch_idx:     0, train_loss: 10.146782\n",
      "batch_idx:  1000, train_loss: 1.545457\n",
      "batch_idx:  2000, train_loss: 1.317557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  3000, train_loss: 1.462822\n",
      "batch_idx:  4000, train_loss: 1.445030\n",
      "batch_idx:  5000, train_loss: 1.412455\n",
      "batch_idx:  6000, train_loss: 1.503350\n",
      "final train_loss: 1.559587\n",
      "batch_idx:     0, valid_loss: 9.251874\n",
      "batch_idx:  1000, valid_loss: 9.935673\n",
      "batch_idx:  2000, valid_loss: 9.997881\n",
      "batch_idx:  3000, valid_loss: 10.076008\n",
      "batch_idx:  4000, valid_loss: 10.152212\n",
      "batch_idx:  5000, valid_loss: 10.069358\n",
      "batch_idx:  6000, valid_loss: 9.835522\n",
      "Training Loss: 1.559587 \tValidation Loss: 9.465801\n",
      "epoch:   103\n",
      "batch_idx:     0, train_loss: 12.395994\n",
      "batch_idx:  1000, train_loss: 1.890268\n",
      "batch_idx:  2000, train_loss: 2.035394\n",
      "batch_idx:  3000, train_loss: 1.905062\n",
      "batch_idx:  4000, train_loss: 1.906365\n",
      "batch_idx:  5000, train_loss: 1.958287\n",
      "batch_idx:  6000, train_loss: 1.994651\n",
      "final train_loss: 1.946187\n",
      "batch_idx:     0, valid_loss: 8.032546\n",
      "batch_idx:  1000, valid_loss: 8.051233\n",
      "batch_idx:  2000, valid_loss: 9.715311\n",
      "batch_idx:  3000, valid_loss: 10.176108\n",
      "batch_idx:  4000, valid_loss: 10.359340\n",
      "batch_idx:  5000, valid_loss: 10.402641\n",
      "batch_idx:  6000, valid_loss: 10.366996\n",
      "Training Loss: 1.946187 \tValidation Loss: 10.013694\n",
      "epoch:   104\n",
      "batch_idx:     0, train_loss: 4.703288\n",
      "batch_idx:  1000, train_loss: 1.237337\n",
      "batch_idx:  2000, train_loss: 1.885816\n",
      "batch_idx:  3000, train_loss: 1.903151\n",
      "batch_idx:  4000, train_loss: 1.940773\n",
      "batch_idx:  5000, train_loss: 2.004131\n",
      "batch_idx:  6000, train_loss: 1.984917\n",
      "final train_loss: 2.080761\n",
      "batch_idx:     0, valid_loss: 8.775199\n",
      "batch_idx:  1000, valid_loss: 9.335303\n",
      "batch_idx:  2000, valid_loss: 9.325752\n",
      "batch_idx:  3000, valid_loss: 9.308375\n",
      "batch_idx:  4000, valid_loss: 9.285567\n",
      "batch_idx:  5000, valid_loss: 9.279615\n",
      "batch_idx:  6000, valid_loss: 9.101021\n",
      "Training Loss: 2.080761 \tValidation Loss: 8.732471\n",
      "epoch:   105\n",
      "batch_idx:     0, train_loss: 10.177786\n",
      "batch_idx:  1000, train_loss: 2.144624\n",
      "batch_idx:  2000, train_loss: 1.945560\n",
      "batch_idx:  3000, train_loss: 1.806297\n",
      "batch_idx:  4000, train_loss: 1.824436\n",
      "batch_idx:  5000, train_loss: 1.895584\n",
      "batch_idx:  6000, train_loss: 1.880874\n",
      "final train_loss: 1.874038\n",
      "batch_idx:     0, valid_loss: 8.489688\n",
      "batch_idx:  1000, valid_loss: 7.843584\n",
      "batch_idx:  2000, valid_loss: 10.005125\n",
      "batch_idx:  3000, valid_loss: 10.669900\n",
      "batch_idx:  4000, valid_loss: 10.973320\n",
      "batch_idx:  5000, valid_loss: 11.072131\n",
      "batch_idx:  6000, valid_loss: 11.092397\n",
      "Training Loss: 1.874038 \tValidation Loss: 10.733088\n",
      "epoch:   106\n",
      "batch_idx:     0, train_loss: 4.677449\n",
      "batch_idx:  1000, train_loss: 1.600542\n",
      "batch_idx:  2000, train_loss: 1.848691\n",
      "batch_idx:  3000, train_loss: 1.971480\n",
      "batch_idx:  4000, train_loss: 1.941667\n",
      "batch_idx:  5000, train_loss: 2.061574\n",
      "batch_idx:  6000, train_loss: 2.100148\n",
      "final train_loss: 2.110011\n",
      "batch_idx:     0, valid_loss: 9.247724\n",
      "batch_idx:  1000, valid_loss: 10.312602\n",
      "batch_idx:  2000, valid_loss: 10.192432\n",
      "batch_idx:  3000, valid_loss: 10.159925\n",
      "batch_idx:  4000, valid_loss: 10.063339\n",
      "batch_idx:  5000, valid_loss: 9.911901\n",
      "batch_idx:  6000, valid_loss: 9.607985\n",
      "Training Loss: 2.110011 \tValidation Loss: 9.212882\n",
      "epoch:   107\n",
      "batch_idx:     0, train_loss: 12.727688\n",
      "batch_idx:  1000, train_loss: 2.222469\n",
      "batch_idx:  2000, train_loss: 2.388563\n",
      "batch_idx:  3000, train_loss: 2.023074\n",
      "batch_idx:  4000, train_loss: 2.086148\n",
      "batch_idx:  5000, train_loss: 2.084999\n",
      "batch_idx:  6000, train_loss: 1.972928\n",
      "final train_loss: 1.999951\n",
      "batch_idx:     0, valid_loss: 9.993504\n",
      "batch_idx:  1000, valid_loss: 10.145173\n",
      "batch_idx:  2000, valid_loss: 10.113663\n",
      "batch_idx:  3000, valid_loss: 10.129427\n",
      "batch_idx:  4000, valid_loss: 10.166049\n",
      "batch_idx:  5000, valid_loss: 10.120154\n",
      "batch_idx:  6000, valid_loss: 9.811042\n",
      "Training Loss: 1.999951 \tValidation Loss: 9.439647\n",
      "epoch:   108\n",
      "batch_idx:     0, train_loss: 11.389117\n",
      "batch_idx:  1000, train_loss: 1.981150\n",
      "batch_idx:  2000, train_loss: 2.032526\n",
      "batch_idx:  3000, train_loss: 1.945244\n",
      "batch_idx:  4000, train_loss: 1.794247\n",
      "batch_idx:  5000, train_loss: 1.697172\n",
      "batch_idx:  6000, train_loss: 1.707458\n",
      "final train_loss: 1.830583\n",
      "batch_idx:     0, valid_loss: 9.202252\n",
      "batch_idx:  1000, valid_loss: 10.046394\n",
      "batch_idx:  2000, valid_loss: 9.938315\n",
      "batch_idx:  3000, valid_loss: 9.738206\n",
      "batch_idx:  4000, valid_loss: 9.572008\n",
      "batch_idx:  5000, valid_loss: 9.403667\n",
      "batch_idx:  6000, valid_loss: 9.104604\n",
      "Training Loss: 1.830583 \tValidation Loss: 8.731506\n",
      "epoch:   109\n",
      "batch_idx:     0, train_loss: 12.589279\n",
      "batch_idx:  1000, train_loss: 2.444610\n",
      "batch_idx:  2000, train_loss: 1.900471\n",
      "batch_idx:  3000, train_loss: 1.638666\n",
      "batch_idx:  4000, train_loss: 1.537549\n",
      "batch_idx:  5000, train_loss: 1.500229\n",
      "batch_idx:  6000, train_loss: 1.540097\n",
      "final train_loss: 1.595997\n",
      "batch_idx:     0, valid_loss: 10.421669\n",
      "batch_idx:  1000, valid_loss: 10.385817\n",
      "batch_idx:  2000, valid_loss: 10.434496\n",
      "batch_idx:  3000, valid_loss: 10.451286\n",
      "batch_idx:  4000, valid_loss: 10.353062\n",
      "batch_idx:  5000, valid_loss: 10.150973\n",
      "batch_idx:  6000, valid_loss: 9.891947\n",
      "Training Loss: 1.595997 \tValidation Loss: 9.554606\n",
      "epoch:   110\n",
      "batch_idx:     0, train_loss: 4.587285\n",
      "batch_idx:  1000, train_loss: 1.725486\n",
      "batch_idx:  2000, train_loss: 1.602572\n",
      "batch_idx:  3000, train_loss: 1.518662\n",
      "batch_idx:  4000, train_loss: 1.601502\n",
      "batch_idx:  5000, train_loss: 1.737435\n",
      "batch_idx:  6000, train_loss: 1.876646\n",
      "final train_loss: 2.001035\n",
      "batch_idx:     0, valid_loss: 9.705341\n",
      "batch_idx:  1000, valid_loss: 10.078852\n",
      "batch_idx:  2000, valid_loss: 9.961955\n",
      "batch_idx:  3000, valid_loss: 9.730641\n",
      "batch_idx:  4000, valid_loss: 9.534206\n",
      "batch_idx:  5000, valid_loss: 9.329432\n",
      "batch_idx:  6000, valid_loss: 9.013170\n",
      "Training Loss: 2.001035 \tValidation Loss: 8.631795\n",
      "epoch:   111\n",
      "batch_idx:     0, train_loss: 11.340591\n",
      "batch_idx:  1000, train_loss: 1.765992\n",
      "batch_idx:  2000, train_loss: 1.756172\n",
      "batch_idx:  3000, train_loss: 1.685935\n",
      "batch_idx:  4000, train_loss: 1.528082\n",
      "batch_idx:  5000, train_loss: 1.646672\n",
      "batch_idx:  6000, train_loss: 1.762103\n",
      "final train_loss: 1.815658\n",
      "batch_idx:     0, valid_loss: 6.371023\n",
      "batch_idx:  1000, valid_loss: 10.403398\n",
      "batch_idx:  2000, valid_loss: 11.334419\n",
      "batch_idx:  3000, valid_loss: 11.452535\n",
      "batch_idx:  4000, valid_loss: 11.435829\n",
      "batch_idx:  5000, valid_loss: 11.317078\n",
      "batch_idx:  6000, valid_loss: 11.043656\n",
      "Training Loss: 1.815658 \tValidation Loss: 10.656820\n",
      "epoch:   112\n",
      "batch_idx:     0, train_loss: 4.496315\n",
      "batch_idx:  1000, train_loss: 1.363433\n",
      "batch_idx:  2000, train_loss: 1.465396\n",
      "batch_idx:  3000, train_loss: 1.466975\n",
      "batch_idx:  4000, train_loss: 1.515134\n",
      "batch_idx:  5000, train_loss: 1.670152\n",
      "batch_idx:  6000, train_loss: 1.833039\n",
      "final train_loss: 1.885219\n",
      "batch_idx:     0, valid_loss: 7.447670\n",
      "batch_idx:  1000, valid_loss: 10.479134\n",
      "batch_idx:  2000, valid_loss: 11.753158\n",
      "batch_idx:  3000, valid_loss: 11.981555\n",
      "batch_idx:  4000, valid_loss: 12.032318\n",
      "batch_idx:  5000, valid_loss: 11.952873\n",
      "batch_idx:  6000, valid_loss: 11.703576\n",
      "Training Loss: 1.885219 \tValidation Loss: 11.264336\n",
      "epoch:   113\n",
      "batch_idx:     0, train_loss: 9.553288\n",
      "batch_idx:  1000, train_loss: 1.233759\n",
      "batch_idx:  2000, train_loss: 0.768573\n",
      "batch_idx:  3000, train_loss: 0.620271\n",
      "batch_idx:  4000, train_loss: 0.555552\n",
      "batch_idx:  5000, train_loss: 0.515543\n",
      "batch_idx:  6000, train_loss: 0.505832\n",
      "final train_loss: 0.514727\n",
      "batch_idx:     0, valid_loss: 5.816439\n",
      "batch_idx:  1000, valid_loss: 13.031684\n",
      "batch_idx:  2000, valid_loss: 16.158840\n",
      "batch_idx:  3000, valid_loss: 17.107960\n",
      "batch_idx:  4000, valid_loss: 17.499777\n",
      "batch_idx:  5000, valid_loss: 17.817251\n",
      "batch_idx:  6000, valid_loss: 17.671469\n",
      "Training Loss: 0.514727 \tValidation Loss: 17.028830\n",
      "epoch:   114\n",
      "batch_idx:     0, train_loss: 7.548295\n",
      "batch_idx:  1000, train_loss: 0.428757\n",
      "batch_idx:  2000, train_loss: 0.363411\n",
      "batch_idx:  3000, train_loss: 0.344810\n",
      "batch_idx:  4000, train_loss: 0.349828\n",
      "batch_idx:  5000, train_loss: 0.350411\n",
      "batch_idx:  6000, train_loss: 0.364809\n",
      "final train_loss: 0.384180\n",
      "batch_idx:     0, valid_loss: 6.125343\n",
      "batch_idx:  1000, valid_loss: 13.815003\n",
      "batch_idx:  2000, valid_loss: 17.237720\n",
      "batch_idx:  3000, valid_loss: 18.431307\n",
      "batch_idx:  4000, valid_loss: 18.981195\n",
      "batch_idx:  5000, valid_loss: 19.253735\n",
      "batch_idx:  6000, valid_loss: 18.769093\n",
      "Training Loss: 0.384180 \tValidation Loss: 17.979357\n",
      "epoch:   115\n",
      "batch_idx:     0, train_loss: 7.166162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  1000, train_loss: 0.616154\n",
      "batch_idx:  2000, train_loss: 0.967452\n",
      "batch_idx:  3000, train_loss: 1.071401\n",
      "batch_idx:  4000, train_loss: 1.144935\n",
      "batch_idx:  5000, train_loss: 1.221904\n",
      "batch_idx:  6000, train_loss: 1.311555\n",
      "final train_loss: 1.292012\n",
      "batch_idx:     0, valid_loss: 6.536182\n",
      "batch_idx:  1000, valid_loss: 12.099361\n",
      "batch_idx:  2000, valid_loss: 14.330345\n",
      "batch_idx:  3000, valid_loss: 15.545397\n",
      "batch_idx:  4000, valid_loss: 16.458055\n",
      "batch_idx:  5000, valid_loss: 17.130789\n",
      "batch_idx:  6000, valid_loss: 16.946989\n",
      "Training Loss: 1.292012 \tValidation Loss: 16.315504\n",
      "epoch:   116\n",
      "batch_idx:     0, train_loss: 7.495428\n",
      "batch_idx:  1000, train_loss: 0.291090\n",
      "batch_idx:  2000, train_loss: 0.307916\n",
      "batch_idx:  3000, train_loss: 0.317403\n",
      "batch_idx:  4000, train_loss: 0.327488\n",
      "batch_idx:  5000, train_loss: 0.349589\n",
      "batch_idx:  6000, train_loss: 0.369562\n",
      "final train_loss: 0.384354\n",
      "batch_idx:     0, valid_loss: 6.052180\n",
      "batch_idx:  1000, valid_loss: 12.833752\n",
      "batch_idx:  2000, valid_loss: 15.304778\n",
      "batch_idx:  3000, valid_loss: 16.456856\n",
      "batch_idx:  4000, valid_loss: 17.151588\n",
      "batch_idx:  5000, valid_loss: 17.614159\n",
      "batch_idx:  6000, valid_loss: 17.448105\n",
      "Training Loss: 0.384354 \tValidation Loss: 16.780476\n",
      "epoch:   117\n",
      "batch_idx:     0, train_loss: 6.468360\n",
      "batch_idx:  1000, train_loss: 0.311286\n",
      "batch_idx:  2000, train_loss: 0.326418\n",
      "batch_idx:  3000, train_loss: 0.330563\n",
      "batch_idx:  4000, train_loss: 0.336483\n",
      "batch_idx:  5000, train_loss: 0.348796\n",
      "batch_idx:  6000, train_loss: 0.367183\n",
      "final train_loss: 0.392986\n",
      "batch_idx:     0, valid_loss: 7.010612\n",
      "batch_idx:  1000, valid_loss: 13.536466\n",
      "batch_idx:  2000, valid_loss: 16.943741\n",
      "batch_idx:  3000, valid_loss: 18.287245\n",
      "batch_idx:  4000, valid_loss: 18.949568\n",
      "batch_idx:  5000, valid_loss: 19.274101\n",
      "batch_idx:  6000, valid_loss: 18.870275\n",
      "Training Loss: 0.392986 \tValidation Loss: 18.039371\n",
      "epoch:   118\n",
      "batch_idx:     0, train_loss: 8.398676\n",
      "batch_idx:  1000, train_loss: 0.437262\n",
      "batch_idx:  2000, train_loss: 0.523818\n",
      "batch_idx:  3000, train_loss: 0.588776\n",
      "batch_idx:  4000, train_loss: 0.611444\n",
      "batch_idx:  5000, train_loss: 0.652007\n",
      "batch_idx:  6000, train_loss: 0.713465\n",
      "final train_loss: 0.799987\n",
      "batch_idx:     0, valid_loss: 12.273683\n",
      "batch_idx:  1000, valid_loss: 16.453915\n",
      "batch_idx:  2000, valid_loss: 16.954075\n",
      "batch_idx:  3000, valid_loss: 17.124273\n",
      "batch_idx:  4000, valid_loss: 17.261446\n",
      "batch_idx:  5000, valid_loss: 17.292343\n",
      "batch_idx:  6000, valid_loss: 16.853231\n",
      "Training Loss: 0.799987 \tValidation Loss: 16.203615\n",
      "epoch:   119\n",
      "batch_idx:     0, train_loss: 14.493695\n",
      "batch_idx:  1000, train_loss: 0.592164\n",
      "batch_idx:  2000, train_loss: 0.779865\n",
      "batch_idx:  3000, train_loss: 0.839187\n",
      "batch_idx:  4000, train_loss: 0.872907\n",
      "batch_idx:  5000, train_loss: 0.972595\n",
      "batch_idx:  6000, train_loss: 1.124014\n",
      "final train_loss: 1.248704\n",
      "batch_idx:     0, valid_loss: 9.672534\n",
      "batch_idx:  1000, valid_loss: 10.367332\n",
      "batch_idx:  2000, valid_loss: 10.710626\n",
      "batch_idx:  3000, valid_loss: 11.053767\n",
      "batch_idx:  4000, valid_loss: 11.206198\n",
      "batch_idx:  5000, valid_loss: 11.135535\n",
      "batch_idx:  6000, valid_loss: 10.896782\n",
      "Training Loss: 1.248704 \tValidation Loss: 10.503091\n",
      "epoch:   120\n",
      "batch_idx:     0, train_loss: 4.396517\n",
      "batch_idx:  1000, train_loss: 1.094133\n",
      "batch_idx:  2000, train_loss: 1.140764\n",
      "batch_idx:  3000, train_loss: 1.130620\n",
      "batch_idx:  4000, train_loss: 1.107043\n",
      "batch_idx:  5000, train_loss: 1.146823\n",
      "batch_idx:  6000, train_loss: 1.277957\n",
      "final train_loss: 1.390573\n",
      "batch_idx:     0, valid_loss: 8.025765\n",
      "batch_idx:  1000, valid_loss: 11.420351\n",
      "batch_idx:  2000, valid_loss: 12.094267\n",
      "batch_idx:  3000, valid_loss: 12.328135\n",
      "batch_idx:  4000, valid_loss: 12.425488\n",
      "batch_idx:  5000, valid_loss: 12.271055\n",
      "batch_idx:  6000, valid_loss: 11.932116\n",
      "Training Loss: 1.390573 \tValidation Loss: 11.492381\n",
      "epoch:   121\n",
      "batch_idx:     0, train_loss: 5.700672\n",
      "batch_idx:  1000, train_loss: 1.474432\n",
      "batch_idx:  2000, train_loss: 1.616616\n",
      "batch_idx:  3000, train_loss: 1.628910\n",
      "batch_idx:  4000, train_loss: 1.539389\n",
      "batch_idx:  5000, train_loss: 1.616584\n",
      "batch_idx:  6000, train_loss: 1.640016\n",
      "final train_loss: 1.663002\n",
      "batch_idx:     0, valid_loss: 9.965287\n",
      "batch_idx:  1000, valid_loss: 9.493965\n",
      "batch_idx:  2000, valid_loss: 9.509883\n",
      "batch_idx:  3000, valid_loss: 9.552913\n",
      "batch_idx:  4000, valid_loss: 9.610606\n",
      "batch_idx:  5000, valid_loss: 9.607434\n",
      "batch_idx:  6000, valid_loss: 9.527122\n",
      "Training Loss: 1.663002 \tValidation Loss: 9.204736\n",
      "epoch:   122\n",
      "batch_idx:     0, train_loss: 12.015620\n",
      "batch_idx:  1000, train_loss: 1.619021\n",
      "batch_idx:  2000, train_loss: 1.584731\n",
      "batch_idx:  3000, train_loss: 1.569940\n",
      "batch_idx:  4000, train_loss: 1.499313\n",
      "batch_idx:  5000, train_loss: 1.444498\n",
      "batch_idx:  6000, train_loss: 1.517667\n",
      "final train_loss: 1.562422\n",
      "batch_idx:     0, valid_loss: 8.893803\n",
      "batch_idx:  1000, valid_loss: 8.009074\n",
      "batch_idx:  2000, valid_loss: 8.648348\n",
      "batch_idx:  3000, valid_loss: 8.850739\n",
      "batch_idx:  4000, valid_loss: 9.004860\n",
      "batch_idx:  5000, valid_loss: 9.043381\n",
      "batch_idx:  6000, valid_loss: 9.098162\n",
      "Training Loss: 1.562422 \tValidation Loss: 9.002943\n",
      "epoch:   123\n",
      "batch_idx:     0, train_loss: 11.566239\n",
      "batch_idx:  1000, train_loss: 1.270009\n",
      "batch_idx:  2000, train_loss: 1.346434\n",
      "batch_idx:  3000, train_loss: 1.245860\n",
      "batch_idx:  4000, train_loss: 1.403196\n",
      "batch_idx:  5000, train_loss: 1.331096\n",
      "batch_idx:  6000, train_loss: 1.373076\n",
      "final train_loss: 1.414851\n",
      "batch_idx:     0, valid_loss: 10.002105\n",
      "batch_idx:  1000, valid_loss: 11.437017\n",
      "batch_idx:  2000, valid_loss: 11.499317\n",
      "batch_idx:  3000, valid_loss: 11.562648\n",
      "batch_idx:  4000, valid_loss: 11.648030\n",
      "batch_idx:  5000, valid_loss: 11.542801\n",
      "batch_idx:  6000, valid_loss: 11.225249\n",
      "Training Loss: 1.414851 \tValidation Loss: 10.787974\n",
      "epoch:   124\n",
      "batch_idx:     0, train_loss: 12.585501\n",
      "batch_idx:  1000, train_loss: 1.544019\n",
      "batch_idx:  2000, train_loss: 1.587995\n",
      "batch_idx:  3000, train_loss: 1.752670\n",
      "batch_idx:  4000, train_loss: 1.713701\n",
      "batch_idx:  5000, train_loss: 1.721998\n",
      "batch_idx:  6000, train_loss: 1.681189\n",
      "final train_loss: 1.682835\n",
      "batch_idx:     0, valid_loss: 7.182781\n",
      "batch_idx:  1000, valid_loss: 10.919881\n",
      "batch_idx:  2000, valid_loss: 11.630935\n",
      "batch_idx:  3000, valid_loss: 11.872821\n",
      "batch_idx:  4000, valid_loss: 12.045510\n",
      "batch_idx:  5000, valid_loss: 11.996355\n",
      "batch_idx:  6000, valid_loss: 11.693974\n",
      "Training Loss: 1.682835 \tValidation Loss: 11.267653\n",
      "epoch:   125\n",
      "batch_idx:     0, train_loss: 8.716956\n",
      "batch_idx:  1000, train_loss: 2.128246\n",
      "batch_idx:  2000, train_loss: 1.913615\n",
      "batch_idx:  3000, train_loss: 1.941098\n",
      "batch_idx:  4000, train_loss: 1.950148\n",
      "batch_idx:  5000, train_loss: 1.882545\n",
      "batch_idx:  6000, train_loss: 1.987606\n",
      "final train_loss: 1.967615\n",
      "batch_idx:     0, valid_loss: 9.777812\n",
      "batch_idx:  1000, valid_loss: 9.804877\n",
      "batch_idx:  2000, valid_loss: 9.836984\n",
      "batch_idx:  3000, valid_loss: 9.813381\n",
      "batch_idx:  4000, valid_loss: 9.818316\n",
      "batch_idx:  5000, valid_loss: 9.686788\n",
      "batch_idx:  6000, valid_loss: 9.469812\n",
      "Training Loss: 1.967615 \tValidation Loss: 9.087580\n",
      "epoch:   126\n",
      "batch_idx:     0, train_loss: 4.717124\n",
      "batch_idx:  1000, train_loss: 2.165726\n",
      "batch_idx:  2000, train_loss: 2.194946\n",
      "batch_idx:  3000, train_loss: 2.146294\n",
      "batch_idx:  4000, train_loss: 1.892223\n",
      "batch_idx:  5000, train_loss: 1.809090\n",
      "batch_idx:  6000, train_loss: 1.743230\n",
      "final train_loss: 1.743998\n",
      "batch_idx:     0, valid_loss: 14.473305\n",
      "batch_idx:  1000, valid_loss: 14.677829\n",
      "batch_idx:  2000, valid_loss: 14.699804\n",
      "batch_idx:  3000, valid_loss: 14.625805\n",
      "batch_idx:  4000, valid_loss: 14.661959\n",
      "batch_idx:  5000, valid_loss: 14.586844\n",
      "batch_idx:  6000, valid_loss: 14.207397\n",
      "Training Loss: 1.743998 \tValidation Loss: 13.676770\n",
      "epoch:   127\n",
      "batch_idx:     0, train_loss: 16.599031\n",
      "batch_idx:  1000, train_loss: 1.597234\n",
      "batch_idx:  2000, train_loss: 1.552795\n",
      "batch_idx:  3000, train_loss: 1.426312\n",
      "batch_idx:  4000, train_loss: 1.385787\n",
      "batch_idx:  5000, train_loss: 1.413144\n",
      "batch_idx:  6000, train_loss: 1.588021\n",
      "final train_loss: 1.637087\n",
      "batch_idx:     0, valid_loss: 10.185875\n",
      "batch_idx:  1000, valid_loss: 10.002643\n",
      "batch_idx:  2000, valid_loss: 10.002778\n",
      "batch_idx:  3000, valid_loss: 10.000631\n",
      "batch_idx:  4000, valid_loss: 10.082856\n",
      "batch_idx:  5000, valid_loss: 10.140450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  6000, valid_loss: 9.963094\n",
      "Training Loss: 1.637087 \tValidation Loss: 9.618376\n",
      "epoch:   128\n",
      "batch_idx:     0, train_loss: 12.651239\n",
      "batch_idx:  1000, train_loss: 1.560781\n",
      "batch_idx:  2000, train_loss: 1.738287\n",
      "batch_idx:  3000, train_loss: 1.738738\n",
      "batch_idx:  4000, train_loss: 1.649483\n",
      "batch_idx:  5000, train_loss: 1.705714\n",
      "batch_idx:  6000, train_loss: 1.762346\n",
      "final train_loss: 1.783987\n",
      "batch_idx:     0, valid_loss: 9.698171\n",
      "batch_idx:  1000, valid_loss: 10.358586\n",
      "batch_idx:  2000, valid_loss: 10.421602\n",
      "batch_idx:  3000, valid_loss: 10.417812\n",
      "batch_idx:  4000, valid_loss: 10.490446\n",
      "batch_idx:  5000, valid_loss: 10.527038\n",
      "batch_idx:  6000, valid_loss: 10.285384\n",
      "Training Loss: 1.783987 \tValidation Loss: 9.895941\n",
      "epoch:   129\n",
      "batch_idx:     0, train_loss: 12.734278\n",
      "batch_idx:  1000, train_loss: 1.983322\n",
      "batch_idx:  2000, train_loss: 2.084178\n",
      "batch_idx:  3000, train_loss: 1.771439\n",
      "batch_idx:  4000, train_loss: 1.670081\n",
      "batch_idx:  5000, train_loss: 1.589719\n",
      "batch_idx:  6000, train_loss: 1.495169\n",
      "final train_loss: 1.485007\n",
      "batch_idx:     0, valid_loss: 6.383047\n",
      "batch_idx:  1000, valid_loss: 6.880355\n",
      "batch_idx:  2000, valid_loss: 7.898677\n",
      "batch_idx:  3000, valid_loss: 8.464080\n",
      "batch_idx:  4000, valid_loss: 8.860133\n",
      "batch_idx:  5000, valid_loss: 9.202985\n",
      "batch_idx:  6000, valid_loss: 9.021633\n",
      "Training Loss: 1.485007 \tValidation Loss: 8.598561\n",
      "epoch:   130\n",
      "batch_idx:     0, train_loss: 6.197365\n",
      "batch_idx:  1000, train_loss: 0.643715\n",
      "batch_idx:  2000, train_loss: 0.532812\n",
      "batch_idx:  3000, train_loss: 0.521739\n",
      "batch_idx:  4000, train_loss: 0.526960\n",
      "batch_idx:  5000, train_loss: 0.554329\n",
      "batch_idx:  6000, train_loss: 0.598388\n",
      "final train_loss: 0.650834\n",
      "batch_idx:     0, valid_loss: 9.259329\n",
      "batch_idx:  1000, valid_loss: 11.021408\n",
      "batch_idx:  2000, valid_loss: 13.398509\n",
      "batch_idx:  3000, valid_loss: 14.540994\n",
      "batch_idx:  4000, valid_loss: 15.187410\n",
      "batch_idx:  5000, valid_loss: 15.441433\n",
      "batch_idx:  6000, valid_loss: 15.270839\n",
      "Training Loss: 0.650834 \tValidation Loss: 14.917280\n",
      "epoch:   131\n",
      "batch_idx:     0, train_loss: 11.671709\n",
      "batch_idx:  1000, train_loss: 0.750837\n",
      "batch_idx:  2000, train_loss: 0.632995\n",
      "batch_idx:  3000, train_loss: 0.639770\n",
      "batch_idx:  4000, train_loss: 0.660302\n",
      "batch_idx:  5000, train_loss: 0.776790\n",
      "batch_idx:  6000, train_loss: 0.901385\n",
      "final train_loss: 0.990520\n",
      "batch_idx:     0, valid_loss: 9.403019\n",
      "batch_idx:  1000, valid_loss: 10.209825\n",
      "batch_idx:  2000, valid_loss: 10.296060\n",
      "batch_idx:  3000, valid_loss: 10.341727\n",
      "batch_idx:  4000, valid_loss: 10.432521\n",
      "batch_idx:  5000, valid_loss: 10.491158\n",
      "batch_idx:  6000, valid_loss: 10.313215\n",
      "Training Loss: 0.990520 \tValidation Loss: 9.889667\n",
      "epoch:   132\n",
      "batch_idx:     0, train_loss: 11.972562\n",
      "batch_idx:  1000, train_loss: 1.516091\n",
      "batch_idx:  2000, train_loss: 1.310120\n",
      "batch_idx:  3000, train_loss: 1.203864\n",
      "batch_idx:  4000, train_loss: 1.198771\n",
      "batch_idx:  5000, train_loss: 1.176639\n",
      "batch_idx:  6000, train_loss: 1.229885\n",
      "final train_loss: 1.253161\n",
      "batch_idx:     0, valid_loss: 9.157076\n",
      "batch_idx:  1000, valid_loss: 9.851971\n",
      "batch_idx:  2000, valid_loss: 10.087933\n",
      "batch_idx:  3000, valid_loss: 10.142643\n",
      "batch_idx:  4000, valid_loss: 10.247159\n",
      "batch_idx:  5000, valid_loss: 10.132177\n",
      "batch_idx:  6000, valid_loss: 9.854514\n",
      "Training Loss: 1.253161 \tValidation Loss: 9.471158\n",
      "epoch:   133\n",
      "batch_idx:     0, train_loss: 10.789440\n",
      "batch_idx:  1000, train_loss: 1.647804\n",
      "batch_idx:  2000, train_loss: 1.669758\n",
      "batch_idx:  3000, train_loss: 1.547235\n",
      "batch_idx:  4000, train_loss: 1.379966\n",
      "batch_idx:  5000, train_loss: 1.366154\n",
      "batch_idx:  6000, train_loss: 1.416140\n",
      "final train_loss: 1.487257\n",
      "batch_idx:     0, valid_loss: 11.422914\n",
      "batch_idx:  1000, valid_loss: 12.949410\n",
      "batch_idx:  2000, valid_loss: 12.908636\n",
      "batch_idx:  3000, valid_loss: 12.746767\n",
      "batch_idx:  4000, valid_loss: 12.822948\n",
      "batch_idx:  5000, valid_loss: 12.604163\n",
      "batch_idx:  6000, valid_loss: 12.189641\n",
      "Training Loss: 1.487257 \tValidation Loss: 11.692602\n",
      "epoch:   134\n",
      "batch_idx:     0, train_loss: 16.662659\n",
      "batch_idx:  1000, train_loss: 1.402652\n",
      "batch_idx:  2000, train_loss: 1.165869\n",
      "batch_idx:  3000, train_loss: 1.241912\n",
      "batch_idx:  4000, train_loss: 1.404831\n",
      "batch_idx:  5000, train_loss: 1.426994\n",
      "batch_idx:  6000, train_loss: 1.571871\n",
      "final train_loss: 1.578801\n",
      "batch_idx:     0, valid_loss: 8.605569\n",
      "batch_idx:  1000, valid_loss: 11.476881\n",
      "batch_idx:  2000, valid_loss: 12.078057\n",
      "batch_idx:  3000, valid_loss: 12.152288\n",
      "batch_idx:  4000, valid_loss: 12.314517\n",
      "batch_idx:  5000, valid_loss: 12.247948\n",
      "batch_idx:  6000, valid_loss: 11.844785\n",
      "Training Loss: 1.578801 \tValidation Loss: 11.326894\n",
      "epoch:   135\n",
      "batch_idx:     0, train_loss: 4.174840\n",
      "batch_idx:  1000, train_loss: 1.332583\n",
      "batch_idx:  2000, train_loss: 1.453437\n",
      "batch_idx:  3000, train_loss: 1.211897\n",
      "batch_idx:  4000, train_loss: 1.054246\n",
      "batch_idx:  5000, train_loss: 1.079859\n",
      "batch_idx:  6000, train_loss: 1.111255\n",
      "final train_loss: 1.237689\n",
      "batch_idx:     0, valid_loss: 9.562800\n",
      "batch_idx:  1000, valid_loss: 10.287556\n",
      "batch_idx:  2000, valid_loss: 10.326695\n",
      "batch_idx:  3000, valid_loss: 10.377155\n",
      "batch_idx:  4000, valid_loss: 10.453755\n",
      "batch_idx:  5000, valid_loss: 10.479083\n",
      "batch_idx:  6000, valid_loss: 10.133731\n",
      "Training Loss: 1.237689 \tValidation Loss: 9.726500\n",
      "epoch:   136\n",
      "batch_idx:     0, train_loss: 4.644080\n",
      "batch_idx:  1000, train_loss: 1.382519\n",
      "batch_idx:  2000, train_loss: 1.483010\n",
      "batch_idx:  3000, train_loss: 1.459786\n",
      "batch_idx:  4000, train_loss: 1.343713\n",
      "batch_idx:  5000, train_loss: 1.359395\n",
      "batch_idx:  6000, train_loss: 1.387242\n",
      "final train_loss: 1.344933\n",
      "batch_idx:     0, valid_loss: 13.040769\n",
      "batch_idx:  1000, valid_loss: 13.002675\n",
      "batch_idx:  2000, valid_loss: 14.795771\n",
      "batch_idx:  3000, valid_loss: 15.539286\n",
      "batch_idx:  4000, valid_loss: 15.948800\n",
      "batch_idx:  5000, valid_loss: 16.012552\n",
      "batch_idx:  6000, valid_loss: 15.651937\n",
      "Training Loss: 1.344933 \tValidation Loss: 15.164601\n",
      "epoch:   137\n",
      "batch_idx:     0, train_loss: 17.374361\n",
      "batch_idx:  1000, train_loss: 1.422892\n",
      "batch_idx:  2000, train_loss: 1.285226\n",
      "batch_idx:  3000, train_loss: 1.379450\n",
      "batch_idx:  4000, train_loss: 1.474116\n",
      "batch_idx:  5000, train_loss: 1.519790\n",
      "batch_idx:  6000, train_loss: 1.560497\n",
      "final train_loss: 1.515244\n",
      "batch_idx:     0, valid_loss: 9.809969\n",
      "batch_idx:  1000, valid_loss: 12.393214\n",
      "batch_idx:  2000, valid_loss: 12.641433\n",
      "batch_idx:  3000, valid_loss: 12.698998\n",
      "batch_idx:  4000, valid_loss: 12.772656\n",
      "batch_idx:  5000, valid_loss: 12.811773\n",
      "batch_idx:  6000, valid_loss: 12.788312\n",
      "Training Loss: 1.515244 \tValidation Loss: 12.458517\n",
      "epoch:   138\n",
      "batch_idx:     0, train_loss: 6.360373\n",
      "batch_idx:  1000, train_loss: 1.210610\n",
      "batch_idx:  2000, train_loss: 1.361552\n",
      "batch_idx:  3000, train_loss: 1.286699\n",
      "batch_idx:  4000, train_loss: 1.130755\n",
      "batch_idx:  5000, train_loss: 1.241420\n",
      "batch_idx:  6000, train_loss: 1.246270\n",
      "final train_loss: 1.256494\n",
      "batch_idx:     0, valid_loss: 10.072465\n",
      "batch_idx:  1000, valid_loss: 10.878695\n",
      "batch_idx:  2000, valid_loss: 12.053623\n",
      "batch_idx:  3000, valid_loss: 12.431400\n",
      "batch_idx:  4000, valid_loss: 12.706693\n",
      "batch_idx:  5000, valid_loss: 12.740322\n",
      "batch_idx:  6000, valid_loss: 12.480024\n",
      "Training Loss: 1.256494 \tValidation Loss: 11.998652\n",
      "epoch:   139\n",
      "batch_idx:     0, train_loss: 7.940810\n",
      "batch_idx:  1000, train_loss: 1.029177\n",
      "batch_idx:  2000, train_loss: 1.039711\n",
      "batch_idx:  3000, train_loss: 1.024898\n",
      "batch_idx:  4000, train_loss: 1.073418\n",
      "batch_idx:  5000, train_loss: 1.110008\n",
      "batch_idx:  6000, train_loss: 1.123402\n",
      "final train_loss: 1.182041\n",
      "batch_idx:     0, valid_loss: 7.335462\n",
      "batch_idx:  1000, valid_loss: 8.330532\n",
      "batch_idx:  2000, valid_loss: 9.327941\n",
      "batch_idx:  3000, valid_loss: 9.679289\n",
      "batch_idx:  4000, valid_loss: 9.886221\n",
      "batch_idx:  5000, valid_loss: 10.024364\n",
      "batch_idx:  6000, valid_loss: 9.991993\n",
      "Training Loss: 1.182041 \tValidation Loss: 9.716011\n",
      "epoch:   140\n",
      "batch_idx:     0, train_loss: 4.666809\n",
      "batch_idx:  1000, train_loss: 1.529060\n",
      "batch_idx:  2000, train_loss: 1.508008\n",
      "batch_idx:  3000, train_loss: 1.346400\n",
      "batch_idx:  4000, train_loss: 1.286803\n",
      "batch_idx:  5000, train_loss: 1.281232\n",
      "batch_idx:  6000, train_loss: 1.359229\n",
      "final train_loss: 1.462820\n",
      "batch_idx:     0, valid_loss: 10.057212\n",
      "batch_idx:  1000, valid_loss: 10.740833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  2000, valid_loss: 10.683403\n",
      "batch_idx:  3000, valid_loss: 10.696327\n",
      "batch_idx:  4000, valid_loss: 10.677145\n",
      "batch_idx:  5000, valid_loss: 10.445643\n",
      "batch_idx:  6000, valid_loss: 10.100310\n",
      "Training Loss: 1.462820 \tValidation Loss: 9.704869\n",
      "epoch:   141\n",
      "batch_idx:     0, train_loss: 12.493339\n",
      "batch_idx:  1000, train_loss: 1.699550\n",
      "batch_idx:  2000, train_loss: 1.664368\n",
      "batch_idx:  3000, train_loss: 1.500050\n",
      "batch_idx:  4000, train_loss: 1.397507\n",
      "batch_idx:  5000, train_loss: 1.317345\n",
      "batch_idx:  6000, train_loss: 1.386112\n",
      "final train_loss: 1.404364\n",
      "batch_idx:     0, valid_loss: 11.896032\n",
      "batch_idx:  1000, valid_loss: 11.935003\n",
      "batch_idx:  2000, valid_loss: 11.486614\n",
      "batch_idx:  3000, valid_loss: 11.795334\n",
      "batch_idx:  4000, valid_loss: 12.146322\n",
      "batch_idx:  5000, valid_loss: 12.349952\n",
      "batch_idx:  6000, valid_loss: 12.266519\n",
      "Training Loss: 1.404364 \tValidation Loss: 11.833409\n",
      "epoch:   142\n",
      "batch_idx:     0, train_loss: 6.791263\n",
      "batch_idx:  1000, train_loss: 1.472195\n",
      "batch_idx:  2000, train_loss: 1.538099\n",
      "batch_idx:  3000, train_loss: 1.551000\n",
      "batch_idx:  4000, train_loss: 1.533864\n",
      "batch_idx:  5000, train_loss: 1.395977\n",
      "batch_idx:  6000, train_loss: 1.438000\n",
      "final train_loss: 1.508371\n",
      "batch_idx:     0, valid_loss: 7.852831\n",
      "batch_idx:  1000, valid_loss: 8.349879\n",
      "batch_idx:  2000, valid_loss: 8.388889\n",
      "batch_idx:  3000, valid_loss: 8.333330\n",
      "batch_idx:  4000, valid_loss: 8.222257\n",
      "batch_idx:  5000, valid_loss: 8.085783\n",
      "batch_idx:  6000, valid_loss: 7.792909\n",
      "Training Loss: 1.508371 \tValidation Loss: 7.437660\n",
      "epoch:   143\n",
      "batch_idx:     0, train_loss: 9.202065\n",
      "batch_idx:  1000, train_loss: 2.208297\n",
      "batch_idx:  2000, train_loss: 1.757315\n",
      "batch_idx:  3000, train_loss: 1.696979\n",
      "batch_idx:  4000, train_loss: 1.833866\n",
      "batch_idx:  5000, train_loss: 1.803846\n",
      "batch_idx:  6000, train_loss: 1.777875\n",
      "final train_loss: 1.864292\n",
      "batch_idx:     0, valid_loss: 9.384130\n",
      "batch_idx:  1000, valid_loss: 10.173393\n",
      "batch_idx:  2000, valid_loss: 10.195784\n",
      "batch_idx:  3000, valid_loss: 10.218425\n",
      "batch_idx:  4000, valid_loss: 10.230157\n",
      "batch_idx:  5000, valid_loss: 10.033756\n",
      "batch_idx:  6000, valid_loss: 9.705527\n",
      "Training Loss: 1.864292 \tValidation Loss: 9.321533\n",
      "epoch:   144\n",
      "batch_idx:     0, train_loss: 11.886110\n",
      "batch_idx:  1000, train_loss: 2.337429\n",
      "batch_idx:  2000, train_loss: 2.342281\n",
      "batch_idx:  3000, train_loss: 2.208722\n",
      "batch_idx:  4000, train_loss: 2.064134\n",
      "batch_idx:  5000, train_loss: 2.021191\n",
      "batch_idx:  6000, train_loss: 1.977686\n",
      "final train_loss: 1.998978\n",
      "batch_idx:     0, valid_loss: 9.590719\n",
      "batch_idx:  1000, valid_loss: 9.663263\n",
      "batch_idx:  2000, valid_loss: 9.736823\n",
      "batch_idx:  3000, valid_loss: 9.685102\n",
      "batch_idx:  4000, valid_loss: 9.772187\n",
      "batch_idx:  5000, valid_loss: 9.733129\n",
      "batch_idx:  6000, valid_loss: 9.442129\n",
      "Training Loss: 1.998978 \tValidation Loss: 9.084439\n",
      "epoch:   145\n",
      "batch_idx:     0, train_loss: 12.633917\n",
      "batch_idx:  1000, train_loss: 1.860916\n",
      "batch_idx:  2000, train_loss: 1.759431\n",
      "batch_idx:  3000, train_loss: 1.861327\n",
      "batch_idx:  4000, train_loss: 1.867053\n",
      "batch_idx:  5000, train_loss: 1.830449\n",
      "batch_idx:  6000, train_loss: 1.686952\n",
      "final train_loss: 1.713293\n",
      "batch_idx:     0, valid_loss: 10.462399\n",
      "batch_idx:  1000, valid_loss: 10.207389\n",
      "batch_idx:  2000, valid_loss: 10.165641\n",
      "batch_idx:  3000, valid_loss: 10.311080\n",
      "batch_idx:  4000, valid_loss: 10.445379\n",
      "batch_idx:  5000, valid_loss: 10.290769\n",
      "batch_idx:  6000, valid_loss: 9.991417\n",
      "Training Loss: 1.713293 \tValidation Loss: 9.572726\n",
      "epoch:   146\n",
      "batch_idx:     0, train_loss: 13.462219\n",
      "batch_idx:  1000, train_loss: 2.138025\n",
      "batch_idx:  2000, train_loss: 1.820518\n",
      "batch_idx:  3000, train_loss: 1.631385\n",
      "batch_idx:  4000, train_loss: 1.540106\n",
      "batch_idx:  5000, train_loss: 1.437618\n",
      "batch_idx:  6000, train_loss: 1.461806\n",
      "final train_loss: 1.568178\n",
      "batch_idx:     0, valid_loss: 8.639463\n",
      "batch_idx:  1000, valid_loss: 8.860028\n",
      "batch_idx:  2000, valid_loss: 9.034085\n",
      "batch_idx:  3000, valid_loss: 9.142756\n",
      "batch_idx:  4000, valid_loss: 9.261733\n",
      "batch_idx:  5000, valid_loss: 9.315504\n",
      "batch_idx:  6000, valid_loss: 9.263399\n",
      "Training Loss: 1.568178 \tValidation Loss: 9.068807\n",
      "epoch:   147\n",
      "batch_idx:     0, train_loss: 4.524628\n",
      "batch_idx:  1000, train_loss: 1.592788\n",
      "batch_idx:  2000, train_loss: 1.455310\n",
      "batch_idx:  3000, train_loss: 1.360980\n",
      "batch_idx:  4000, train_loss: 1.320376\n",
      "batch_idx:  5000, train_loss: 1.363751\n",
      "batch_idx:  6000, train_loss: 1.479922\n",
      "final train_loss: 1.632733\n",
      "batch_idx:     0, valid_loss: 9.390336\n",
      "batch_idx:  1000, valid_loss: 10.227739\n",
      "batch_idx:  2000, valid_loss: 10.360809\n",
      "batch_idx:  3000, valid_loss: 10.445528\n",
      "batch_idx:  4000, valid_loss: 10.502690\n",
      "batch_idx:  5000, valid_loss: 10.337417\n",
      "batch_idx:  6000, valid_loss: 9.970251\n",
      "Training Loss: 1.632733 \tValidation Loss: 9.557778\n",
      "epoch:   148\n",
      "batch_idx:     0, train_loss: 11.966406\n",
      "batch_idx:  1000, train_loss: 2.045945\n",
      "batch_idx:  2000, train_loss: 1.563890\n",
      "batch_idx:  3000, train_loss: 1.340827\n",
      "batch_idx:  4000, train_loss: 1.407536\n",
      "batch_idx:  5000, train_loss: 1.333303\n",
      "batch_idx:  6000, train_loss: 1.328712\n",
      "final train_loss: 1.352056\n",
      "batch_idx:     0, valid_loss: 10.055991\n",
      "batch_idx:  1000, valid_loss: 10.849479\n",
      "batch_idx:  2000, valid_loss: 10.957977\n",
      "batch_idx:  3000, valid_loss: 11.003513\n",
      "batch_idx:  4000, valid_loss: 11.096608\n",
      "batch_idx:  5000, valid_loss: 11.196879\n",
      "batch_idx:  6000, valid_loss: 10.877566\n",
      "Training Loss: 1.352056 \tValidation Loss: 10.401161\n",
      "epoch:   149\n",
      "batch_idx:     0, train_loss: 12.615068\n",
      "batch_idx:  1000, train_loss: 1.498656\n",
      "batch_idx:  2000, train_loss: 1.239485\n",
      "batch_idx:  3000, train_loss: 1.237497\n",
      "batch_idx:  4000, train_loss: 1.333947\n",
      "batch_idx:  5000, train_loss: 1.341486\n",
      "batch_idx:  6000, train_loss: 1.435206\n",
      "final train_loss: 1.530234\n",
      "batch_idx:     0, valid_loss: 6.786208\n",
      "batch_idx:  1000, valid_loss: 11.421389\n",
      "batch_idx:  2000, valid_loss: 11.793866\n",
      "batch_idx:  3000, valid_loss: 11.899836\n",
      "batch_idx:  4000, valid_loss: 12.023131\n",
      "batch_idx:  5000, valid_loss: 12.131199\n",
      "batch_idx:  6000, valid_loss: 11.844058\n",
      "Training Loss: 1.530234 \tValidation Loss: 11.380418\n",
      "epoch:   150\n",
      "batch_idx:     0, train_loss: 4.420866\n",
      "batch_idx:  1000, train_loss: 1.395389\n",
      "batch_idx:  2000, train_loss: 1.282294\n",
      "batch_idx:  3000, train_loss: 1.140648\n",
      "batch_idx:  4000, train_loss: 1.158238\n",
      "batch_idx:  5000, train_loss: 1.251716\n",
      "batch_idx:  6000, train_loss: 1.386494\n",
      "final train_loss: 1.485811\n",
      "batch_idx:     0, valid_loss: 9.771533\n",
      "batch_idx:  1000, valid_loss: 10.001527\n",
      "batch_idx:  2000, valid_loss: 10.178249\n",
      "batch_idx:  3000, valid_loss: 10.235260\n",
      "batch_idx:  4000, valid_loss: 10.222063\n",
      "batch_idx:  5000, valid_loss: 10.066419\n",
      "batch_idx:  6000, valid_loss: 9.756495\n",
      "Training Loss: 1.485811 \tValidation Loss: 9.379475\n",
      "epoch:   151\n",
      "batch_idx:     0, train_loss: 4.409447\n",
      "batch_idx:  1000, train_loss: 1.754014\n",
      "batch_idx:  2000, train_loss: 1.405742\n",
      "batch_idx:  3000, train_loss: 1.348806\n",
      "batch_idx:  4000, train_loss: 1.495812\n",
      "batch_idx:  5000, train_loss: 1.705829\n",
      "batch_idx:  6000, train_loss: 1.780661\n",
      "final train_loss: 1.830662\n",
      "batch_idx:     0, valid_loss: 10.235334\n",
      "batch_idx:  1000, valid_loss: 10.262292\n",
      "batch_idx:  2000, valid_loss: 10.426282\n",
      "batch_idx:  3000, valid_loss: 10.466673\n",
      "batch_idx:  4000, valid_loss: 10.535344\n",
      "batch_idx:  5000, valid_loss: 10.457874\n",
      "batch_idx:  6000, valid_loss: 10.167611\n",
      "Training Loss: 1.830662 \tValidation Loss: 9.780985\n",
      "epoch:   152\n",
      "batch_idx:     0, train_loss: 12.583188\n",
      "batch_idx:  1000, train_loss: 1.858551\n",
      "batch_idx:  2000, train_loss: 1.496921\n",
      "batch_idx:  3000, train_loss: 1.385515\n",
      "batch_idx:  4000, train_loss: 1.522544\n",
      "batch_idx:  5000, train_loss: 1.657544\n",
      "batch_idx:  6000, train_loss: 1.806046\n",
      "final train_loss: 1.929039\n",
      "batch_idx:     0, valid_loss: 8.254675\n",
      "batch_idx:  1000, valid_loss: 7.595880\n",
      "batch_idx:  2000, valid_loss: 9.931060\n",
      "batch_idx:  3000, valid_loss: 10.714494\n",
      "batch_idx:  4000, valid_loss: 11.122014\n",
      "batch_idx:  5000, valid_loss: 11.265278\n",
      "batch_idx:  6000, valid_loss: 11.166294\n",
      "Training Loss: 1.929039 \tValidation Loss: 10.928077\n",
      "epoch:   153\n",
      "batch_idx:     0, train_loss: 4.455015\n",
      "batch_idx:  1000, train_loss: 1.471162\n",
      "batch_idx:  2000, train_loss: 1.334233\n",
      "batch_idx:  3000, train_loss: 1.251283\n",
      "batch_idx:  4000, train_loss: 1.387491\n",
      "batch_idx:  5000, train_loss: 1.538975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  6000, train_loss: 1.526612\n",
      "final train_loss: 1.656904\n",
      "batch_idx:     0, valid_loss: 8.849884\n",
      "batch_idx:  1000, valid_loss: 10.075924\n",
      "batch_idx:  2000, valid_loss: 10.163121\n",
      "batch_idx:  3000, valid_loss: 10.132865\n",
      "batch_idx:  4000, valid_loss: 9.951776\n",
      "batch_idx:  5000, valid_loss: 9.745393\n",
      "batch_idx:  6000, valid_loss: 9.392341\n",
      "Training Loss: 1.656904 \tValidation Loss: 9.001919\n",
      "epoch:   154\n",
      "batch_idx:     0, train_loss: 9.810859\n",
      "batch_idx:  1000, train_loss: 1.748853\n",
      "batch_idx:  2000, train_loss: 1.423612\n",
      "batch_idx:  3000, train_loss: 1.350317\n",
      "batch_idx:  4000, train_loss: 1.483385\n",
      "batch_idx:  5000, train_loss: 1.648069\n",
      "batch_idx:  6000, train_loss: 1.814676\n",
      "final train_loss: 1.962336\n",
      "batch_idx:     0, valid_loss: 7.990786\n",
      "batch_idx:  1000, valid_loss: 7.672784\n",
      "batch_idx:  2000, valid_loss: 8.682210\n",
      "batch_idx:  3000, valid_loss: 9.322113\n",
      "batch_idx:  4000, valid_loss: 9.613111\n",
      "batch_idx:  5000, valid_loss: 9.656886\n",
      "batch_idx:  6000, valid_loss: 9.484222\n",
      "Training Loss: 1.962336 \tValidation Loss: 9.164113\n",
      "epoch:   155\n",
      "batch_idx:     0, train_loss: 8.762838\n",
      "batch_idx:  1000, train_loss: 1.585027\n",
      "batch_idx:  2000, train_loss: 1.630175\n",
      "batch_idx:  3000, train_loss: 1.838955\n",
      "batch_idx:  4000, train_loss: 1.850115\n",
      "batch_idx:  5000, train_loss: 1.983066\n",
      "batch_idx:  6000, train_loss: 2.167320\n",
      "final train_loss: 2.114054\n",
      "batch_idx:     0, valid_loss: 9.573026\n",
      "batch_idx:  1000, valid_loss: 11.404021\n",
      "batch_idx:  2000, valid_loss: 11.654830\n",
      "batch_idx:  3000, valid_loss: 11.656384\n",
      "batch_idx:  4000, valid_loss: 11.647170\n",
      "batch_idx:  5000, valid_loss: 11.535944\n",
      "batch_idx:  6000, valid_loss: 11.287155\n",
      "Training Loss: 2.114054 \tValidation Loss: 10.815354\n",
      "epoch:   156\n",
      "batch_idx:     0, train_loss: 10.774694\n",
      "batch_idx:  1000, train_loss: 1.475103\n",
      "batch_idx:  2000, train_loss: 1.395908\n",
      "batch_idx:  3000, train_loss: 1.460319\n",
      "batch_idx:  4000, train_loss: 1.664801\n",
      "batch_idx:  5000, train_loss: 1.777095\n",
      "batch_idx:  6000, train_loss: 1.774966\n",
      "final train_loss: 1.762499\n",
      "batch_idx:     0, valid_loss: 9.299054\n",
      "batch_idx:  1000, valid_loss: 9.183980\n",
      "batch_idx:  2000, valid_loss: 9.397538\n",
      "batch_idx:  3000, valid_loss: 10.345295\n",
      "batch_idx:  4000, valid_loss: 10.760863\n",
      "batch_idx:  5000, valid_loss: 10.884822\n",
      "batch_idx:  6000, valid_loss: 10.832854\n",
      "Training Loss: 1.762499 \tValidation Loss: 10.448591\n",
      "epoch:   157\n",
      "batch_idx:     0, train_loss: 10.404634\n",
      "batch_idx:  1000, train_loss: 1.177869\n",
      "batch_idx:  2000, train_loss: 1.282325\n",
      "batch_idx:  3000, train_loss: 1.333511\n",
      "batch_idx:  4000, train_loss: 1.459610\n",
      "batch_idx:  5000, train_loss: 1.537209\n",
      "batch_idx:  6000, train_loss: 1.499239\n",
      "final train_loss: 1.580646\n",
      "batch_idx:     0, valid_loss: 9.759295\n",
      "batch_idx:  1000, valid_loss: 9.993386\n",
      "batch_idx:  2000, valid_loss: 9.178976\n",
      "batch_idx:  3000, valid_loss: 10.162180\n",
      "batch_idx:  4000, valid_loss: 10.990876\n",
      "batch_idx:  5000, valid_loss: 11.442050\n",
      "batch_idx:  6000, valid_loss: 11.537722\n",
      "Training Loss: 1.580646 \tValidation Loss: 11.202134\n",
      "epoch:   158\n",
      "batch_idx:     0, train_loss: 12.966038\n",
      "batch_idx:  1000, train_loss: 1.326095\n",
      "batch_idx:  2000, train_loss: 1.127781\n",
      "batch_idx:  3000, train_loss: 1.086739\n",
      "batch_idx:  4000, train_loss: 0.901439\n",
      "batch_idx:  5000, train_loss: 0.802961\n",
      "batch_idx:  6000, train_loss: 0.740579\n",
      "final train_loss: 0.727097\n",
      "batch_idx:     0, valid_loss: 4.754632\n",
      "batch_idx:  1000, valid_loss: 12.553899\n",
      "batch_idx:  2000, valid_loss: 16.507929\n",
      "batch_idx:  3000, valid_loss: 18.514532\n",
      "batch_idx:  4000, valid_loss: 19.777811\n",
      "batch_idx:  5000, valid_loss: 20.430937\n",
      "batch_idx:  6000, valid_loss: 20.124058\n",
      "Training Loss: 0.727097 \tValidation Loss: 19.169750\n",
      "epoch:   159\n",
      "batch_idx:     0, train_loss: 5.668869\n",
      "batch_idx:  1000, train_loss: 0.327861\n",
      "batch_idx:  2000, train_loss: 0.348006\n",
      "batch_idx:  3000, train_loss: 0.414221\n",
      "batch_idx:  4000, train_loss: 0.387729\n",
      "batch_idx:  5000, train_loss: 0.383005\n",
      "batch_idx:  6000, train_loss: 0.390079\n",
      "final train_loss: 0.401025\n",
      "batch_idx:     0, valid_loss: 6.663143\n",
      "batch_idx:  1000, valid_loss: 15.938125\n",
      "batch_idx:  2000, valid_loss: 20.656626\n",
      "batch_idx:  3000, valid_loss: 22.710142\n",
      "batch_idx:  4000, valid_loss: 23.939894\n",
      "batch_idx:  5000, valid_loss: 24.542963\n",
      "batch_idx:  6000, valid_loss: 24.006807\n",
      "Training Loss: 0.401025 \tValidation Loss: 22.844896\n",
      "epoch:   160\n",
      "batch_idx:     0, train_loss: 7.026944\n",
      "batch_idx:  1000, train_loss: 0.360762\n",
      "batch_idx:  2000, train_loss: 0.362767\n",
      "batch_idx:  3000, train_loss: 0.397349\n",
      "batch_idx:  4000, train_loss: 0.610867\n",
      "batch_idx:  5000, train_loss: 0.751589\n",
      "batch_idx:  6000, train_loss: 0.879245\n",
      "final train_loss: 0.960016\n",
      "batch_idx:     0, valid_loss: 5.583865\n",
      "batch_idx:  1000, valid_loss: 7.775177\n",
      "batch_idx:  2000, valid_loss: 9.189560\n",
      "batch_idx:  3000, valid_loss: 9.965672\n",
      "batch_idx:  4000, valid_loss: 10.285874\n",
      "batch_idx:  5000, valid_loss: 10.398109\n",
      "batch_idx:  6000, valid_loss: 10.107637\n",
      "Training Loss: 0.960016 \tValidation Loss: 9.605057\n",
      "epoch:   161\n",
      "batch_idx:     0, train_loss: 6.020193\n",
      "batch_idx:  1000, train_loss: 1.260304\n",
      "batch_idx:  2000, train_loss: 1.097795\n",
      "batch_idx:  3000, train_loss: 0.833598\n",
      "batch_idx:  4000, train_loss: 0.709463\n",
      "batch_idx:  5000, train_loss: 0.645465\n",
      "batch_idx:  6000, train_loss: 0.602897\n",
      "final train_loss: 0.595099\n",
      "batch_idx:     0, valid_loss: 5.836471\n",
      "batch_idx:  1000, valid_loss: 14.563848\n",
      "batch_idx:  2000, valid_loss: 19.621956\n",
      "batch_idx:  3000, valid_loss: 21.773825\n",
      "batch_idx:  4000, valid_loss: 23.013519\n",
      "batch_idx:  5000, valid_loss: 23.617311\n",
      "batch_idx:  6000, valid_loss: 23.229771\n",
      "Training Loss: 0.595099 \tValidation Loss: 22.204473\n",
      "epoch:   162\n",
      "batch_idx:     0, train_loss: 4.185088\n",
      "batch_idx:  1000, train_loss: 0.362470\n",
      "batch_idx:  2000, train_loss: 0.512100\n",
      "batch_idx:  3000, train_loss: 0.459098\n",
      "batch_idx:  4000, train_loss: 0.458948\n",
      "batch_idx:  5000, train_loss: 0.473910\n",
      "batch_idx:  6000, train_loss: 0.488298\n",
      "final train_loss: 0.558341\n",
      "batch_idx:     0, valid_loss: 6.650379\n",
      "batch_idx:  1000, valid_loss: 7.389370\n",
      "batch_idx:  2000, valid_loss: 7.545128\n",
      "batch_idx:  3000, valid_loss: 7.889296\n",
      "batch_idx:  4000, valid_loss: 8.120009\n",
      "batch_idx:  5000, valid_loss: 8.199177\n",
      "batch_idx:  6000, valid_loss: 8.115812\n",
      "Training Loss: 0.558341 \tValidation Loss: 7.778332\n",
      "epoch:   163\n",
      "batch_idx:     0, train_loss: 7.718336\n",
      "batch_idx:  1000, train_loss: 0.677741\n",
      "batch_idx:  2000, train_loss: 0.638789\n",
      "batch_idx:  3000, train_loss: 0.648101\n",
      "batch_idx:  4000, train_loss: 0.706596\n",
      "batch_idx:  5000, train_loss: 0.822645\n",
      "batch_idx:  6000, train_loss: 0.924614\n",
      "final train_loss: 0.999310\n",
      "batch_idx:     0, valid_loss: 13.432026\n",
      "batch_idx:  1000, valid_loss: 12.909308\n",
      "batch_idx:  2000, valid_loss: 12.942595\n",
      "batch_idx:  3000, valid_loss: 12.997584\n",
      "batch_idx:  4000, valid_loss: 13.004812\n",
      "batch_idx:  5000, valid_loss: 12.965109\n",
      "batch_idx:  6000, valid_loss: 12.698144\n",
      "Training Loss: 0.999310 \tValidation Loss: 12.235256\n",
      "epoch:   164\n",
      "batch_idx:     0, train_loss: 16.678953\n",
      "batch_idx:  1000, train_loss: 1.299601\n",
      "batch_idx:  2000, train_loss: 0.985478\n",
      "batch_idx:  3000, train_loss: 0.935403\n",
      "batch_idx:  4000, train_loss: 1.013842\n",
      "batch_idx:  5000, train_loss: 1.048064\n",
      "batch_idx:  6000, train_loss: 1.117998\n",
      "final train_loss: 1.245983\n",
      "batch_idx:     0, valid_loss: 8.578636\n",
      "batch_idx:  1000, valid_loss: 8.915623\n",
      "batch_idx:  2000, valid_loss: 8.985510\n",
      "batch_idx:  3000, valid_loss: 9.030004\n",
      "batch_idx:  4000, valid_loss: 9.019309\n",
      "batch_idx:  5000, valid_loss: 8.867807\n",
      "batch_idx:  6000, valid_loss: 8.617313\n",
      "Training Loss: 1.245983 \tValidation Loss: 8.289826\n",
      "epoch:   165\n",
      "batch_idx:     0, train_loss: 9.497790\n",
      "batch_idx:  1000, train_loss: 1.679844\n",
      "batch_idx:  2000, train_loss: 1.807119\n",
      "batch_idx:  3000, train_loss: 1.779890\n",
      "batch_idx:  4000, train_loss: 1.799249\n",
      "batch_idx:  5000, train_loss: 1.882394\n",
      "batch_idx:  6000, train_loss: 1.823293\n",
      "final train_loss: 1.891883\n",
      "batch_idx:     0, valid_loss: 9.092703\n",
      "batch_idx:  1000, valid_loss: 9.409739\n",
      "batch_idx:  2000, valid_loss: 9.497987\n",
      "batch_idx:  3000, valid_loss: 9.521116\n",
      "batch_idx:  4000, valid_loss: 9.515964\n",
      "batch_idx:  5000, valid_loss: 9.369531\n",
      "batch_idx:  6000, valid_loss: 9.085486\n",
      "Training Loss: 1.891883 \tValidation Loss: 8.730219\n",
      "epoch:   166\n",
      "batch_idx:     0, train_loss: 11.666073\n",
      "batch_idx:  1000, train_loss: 1.565816\n",
      "batch_idx:  2000, train_loss: 1.306049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  3000, train_loss: 1.376648\n",
      "batch_idx:  4000, train_loss: 1.349700\n",
      "batch_idx:  5000, train_loss: 1.448379\n",
      "batch_idx:  6000, train_loss: 1.483496\n",
      "final train_loss: 1.552906\n",
      "batch_idx:     0, valid_loss: 9.848550\n",
      "batch_idx:  1000, valid_loss: 10.024994\n",
      "batch_idx:  2000, valid_loss: 10.157029\n",
      "batch_idx:  3000, valid_loss: 10.185193\n",
      "batch_idx:  4000, valid_loss: 10.199237\n",
      "batch_idx:  5000, valid_loss: 10.134998\n",
      "batch_idx:  6000, valid_loss: 9.848102\n",
      "Training Loss: 1.552906 \tValidation Loss: 9.481497\n",
      "epoch:   167\n",
      "batch_idx:     0, train_loss: 4.500287\n",
      "batch_idx:  1000, train_loss: 1.771743\n",
      "batch_idx:  2000, train_loss: 1.267135\n",
      "batch_idx:  3000, train_loss: 0.972106\n",
      "batch_idx:  4000, train_loss: 0.852747\n",
      "batch_idx:  5000, train_loss: 0.800684\n",
      "batch_idx:  6000, train_loss: 0.797113\n",
      "final train_loss: 0.816780\n",
      "batch_idx:     0, valid_loss: 9.875921\n",
      "batch_idx:  1000, valid_loss: 11.812125\n",
      "batch_idx:  2000, valid_loss: 14.562203\n",
      "batch_idx:  3000, valid_loss: 15.522734\n",
      "batch_idx:  4000, valid_loss: 16.014000\n",
      "batch_idx:  5000, valid_loss: 16.128651\n",
      "batch_idx:  6000, valid_loss: 15.884607\n",
      "Training Loss: 0.816780 \tValidation Loss: 15.309601\n",
      "epoch:   168\n",
      "batch_idx:     0, train_loss: 11.514191\n",
      "batch_idx:  1000, train_loss: 0.816839\n",
      "batch_idx:  2000, train_loss: 0.879041\n",
      "batch_idx:  3000, train_loss: 0.934708\n",
      "batch_idx:  4000, train_loss: 0.929374\n",
      "batch_idx:  5000, train_loss: 0.993071\n",
      "batch_idx:  6000, train_loss: 0.990232\n",
      "final train_loss: 1.051576\n",
      "batch_idx:     0, valid_loss: 7.008421\n",
      "batch_idx:  1000, valid_loss: 8.640615\n",
      "batch_idx:  2000, valid_loss: 9.739766\n",
      "batch_idx:  3000, valid_loss: 10.119356\n",
      "batch_idx:  4000, valid_loss: 10.315497\n",
      "batch_idx:  5000, valid_loss: 10.442558\n",
      "batch_idx:  6000, valid_loss: 10.443154\n",
      "Training Loss: 1.051576 \tValidation Loss: 10.129478\n",
      "epoch:   169\n",
      "batch_idx:     0, train_loss: 4.673186\n",
      "batch_idx:  1000, train_loss: 1.345546\n",
      "batch_idx:  2000, train_loss: 1.062052\n",
      "batch_idx:  3000, train_loss: 0.872602\n",
      "batch_idx:  4000, train_loss: 0.803626\n",
      "batch_idx:  5000, train_loss: 0.791774\n",
      "batch_idx:  6000, train_loss: 0.819927\n",
      "final train_loss: 0.876349\n",
      "batch_idx:     0, valid_loss: 12.906624\n",
      "batch_idx:  1000, valid_loss: 17.229265\n",
      "batch_idx:  2000, valid_loss: 17.362608\n",
      "batch_idx:  3000, valid_loss: 17.568182\n",
      "batch_idx:  4000, valid_loss: 17.626842\n",
      "batch_idx:  5000, valid_loss: 17.540966\n",
      "batch_idx:  6000, valid_loss: 17.252502\n",
      "Training Loss: 0.876349 \tValidation Loss: 16.628382\n",
      "epoch:   170\n",
      "batch_idx:     0, train_loss: 15.569169\n",
      "batch_idx:  1000, train_loss: 0.961109\n",
      "batch_idx:  2000, train_loss: 0.966363\n",
      "batch_idx:  3000, train_loss: 0.762278\n",
      "batch_idx:  4000, train_loss: 0.686827\n",
      "batch_idx:  5000, train_loss: 0.647822\n",
      "batch_idx:  6000, train_loss: 0.635540\n",
      "final train_loss: 0.661130\n",
      "batch_idx:     0, valid_loss: 11.422408\n",
      "batch_idx:  1000, valid_loss: 13.134125\n",
      "batch_idx:  2000, valid_loss: 15.198187\n",
      "batch_idx:  3000, valid_loss: 16.098978\n",
      "batch_idx:  4000, valid_loss: 16.645304\n",
      "batch_idx:  5000, valid_loss: 16.891413\n",
      "batch_idx:  6000, valid_loss: 16.743607\n",
      "Training Loss: 0.661130 \tValidation Loss: 16.142763\n",
      "epoch:   171\n",
      "batch_idx:     0, train_loss: 13.293522\n",
      "batch_idx:  1000, train_loss: 0.721432\n",
      "batch_idx:  2000, train_loss: 0.733833\n",
      "batch_idx:  3000, train_loss: 0.753994\n",
      "batch_idx:  4000, train_loss: 0.775505\n",
      "batch_idx:  5000, train_loss: 0.799496\n",
      "batch_idx:  6000, train_loss: 0.791033\n",
      "final train_loss: 0.854596\n",
      "batch_idx:     0, valid_loss: 6.577301\n",
      "batch_idx:  1000, valid_loss: 10.031977\n",
      "batch_idx:  2000, valid_loss: 10.473941\n",
      "batch_idx:  3000, valid_loss: 10.611603\n",
      "batch_idx:  4000, valid_loss: 10.695647\n",
      "batch_idx:  5000, valid_loss: 10.724474\n",
      "batch_idx:  6000, valid_loss: 10.613544\n",
      "Training Loss: 0.854596 \tValidation Loss: 10.196591\n",
      "epoch:   172\n",
      "batch_idx:     0, train_loss: 8.700086\n",
      "batch_idx:  1000, train_loss: 1.500516\n",
      "batch_idx:  2000, train_loss: 1.229584\n",
      "batch_idx:  3000, train_loss: 1.142490\n",
      "batch_idx:  4000, train_loss: 1.086542\n",
      "batch_idx:  5000, train_loss: 1.050244\n",
      "batch_idx:  6000, train_loss: 1.008826\n",
      "final train_loss: 1.032826\n",
      "batch_idx:     0, valid_loss: 8.398797\n",
      "batch_idx:  1000, valid_loss: 15.952357\n",
      "batch_idx:  2000, valid_loss: 16.258795\n",
      "batch_idx:  3000, valid_loss: 16.347553\n",
      "batch_idx:  4000, valid_loss: 16.353012\n",
      "batch_idx:  5000, valid_loss: 16.220562\n",
      "batch_idx:  6000, valid_loss: 16.012558\n",
      "Training Loss: 1.032826 \tValidation Loss: 15.501334\n",
      "epoch:   173\n",
      "batch_idx:     0, train_loss: 10.817776\n",
      "batch_idx:  1000, train_loss: 0.778983\n",
      "batch_idx:  2000, train_loss: 0.732376\n",
      "batch_idx:  3000, train_loss: 0.766380\n",
      "batch_idx:  4000, train_loss: 0.851903\n",
      "batch_idx:  5000, train_loss: 1.010914\n",
      "batch_idx:  6000, train_loss: 1.204925\n",
      "final train_loss: 1.365328\n",
      "batch_idx:     0, valid_loss: 9.324690\n",
      "batch_idx:  1000, valid_loss: 9.724750\n",
      "batch_idx:  2000, valid_loss: 9.830260\n",
      "batch_idx:  3000, valid_loss: 9.875486\n",
      "batch_idx:  4000, valid_loss: 9.813133\n",
      "batch_idx:  5000, valid_loss: 9.575091\n",
      "batch_idx:  6000, valid_loss: 9.256089\n",
      "Training Loss: 1.365328 \tValidation Loss: 8.878237\n",
      "epoch:   174\n",
      "batch_idx:     0, train_loss: 4.360220\n",
      "batch_idx:  1000, train_loss: 1.868425\n",
      "batch_idx:  2000, train_loss: 1.723392\n",
      "batch_idx:  3000, train_loss: 1.546823\n",
      "batch_idx:  4000, train_loss: 1.484220\n",
      "batch_idx:  5000, train_loss: 1.563599\n",
      "batch_idx:  6000, train_loss: 1.649097\n",
      "final train_loss: 1.749447\n",
      "batch_idx:     0, valid_loss: 10.020572\n",
      "batch_idx:  1000, valid_loss: 9.959733\n",
      "batch_idx:  2000, valid_loss: 10.143369\n",
      "batch_idx:  3000, valid_loss: 10.185800\n",
      "batch_idx:  4000, valid_loss: 10.103637\n",
      "batch_idx:  5000, valid_loss: 9.909980\n",
      "batch_idx:  6000, valid_loss: 9.582709\n",
      "Training Loss: 1.749447 \tValidation Loss: 9.198767\n",
      "epoch:   175\n",
      "batch_idx:     0, train_loss: 11.506872\n",
      "batch_idx:  1000, train_loss: 1.655850\n",
      "batch_idx:  2000, train_loss: 1.421042\n",
      "batch_idx:  3000, train_loss: 1.609402\n",
      "batch_idx:  4000, train_loss: 1.760023\n",
      "batch_idx:  5000, train_loss: 1.858857\n",
      "batch_idx:  6000, train_loss: 1.897204\n",
      "final train_loss: 1.909968\n",
      "batch_idx:     0, valid_loss: 10.361470\n",
      "batch_idx:  1000, valid_loss: 9.478539\n",
      "batch_idx:  2000, valid_loss: 9.704910\n",
      "batch_idx:  3000, valid_loss: 9.715845\n",
      "batch_idx:  4000, valid_loss: 9.764291\n",
      "batch_idx:  5000, valid_loss: 9.784066\n",
      "batch_idx:  6000, valid_loss: 9.666217\n",
      "Training Loss: 1.909968 \tValidation Loss: 9.319749\n",
      "epoch:   176\n",
      "batch_idx:     0, train_loss: 14.111572\n",
      "batch_idx:  1000, train_loss: 1.412158\n",
      "batch_idx:  2000, train_loss: 1.162497\n",
      "batch_idx:  3000, train_loss: 1.312857\n",
      "batch_idx:  4000, train_loss: 1.386285\n",
      "batch_idx:  5000, train_loss: 1.565922\n",
      "batch_idx:  6000, train_loss: 1.700325\n",
      "final train_loss: 1.749672\n",
      "batch_idx:     0, valid_loss: 8.231024\n",
      "batch_idx:  1000, valid_loss: 8.598398\n",
      "batch_idx:  2000, valid_loss: 8.912401\n",
      "batch_idx:  3000, valid_loss: 9.982793\n",
      "batch_idx:  4000, valid_loss: 10.500984\n",
      "batch_idx:  5000, valid_loss: 10.728362\n",
      "batch_idx:  6000, valid_loss: 10.709718\n",
      "Training Loss: 1.749672 \tValidation Loss: 10.388616\n",
      "epoch:   177\n",
      "batch_idx:     0, train_loss: 4.378537\n",
      "batch_idx:  1000, train_loss: 1.513928\n",
      "batch_idx:  2000, train_loss: 1.370460\n",
      "batch_idx:  3000, train_loss: 1.412657\n",
      "batch_idx:  4000, train_loss: 1.461379\n",
      "batch_idx:  5000, train_loss: 1.541271\n",
      "batch_idx:  6000, train_loss: 1.609395\n",
      "final train_loss: 1.667156\n",
      "batch_idx:     0, valid_loss: 7.990126\n",
      "batch_idx:  1000, valid_loss: 7.760684\n",
      "batch_idx:  2000, valid_loss: 7.999035\n",
      "batch_idx:  3000, valid_loss: 9.471077\n",
      "batch_idx:  4000, valid_loss: 10.169895\n",
      "batch_idx:  5000, valid_loss: 10.530324\n",
      "batch_idx:  6000, valid_loss: 10.627982\n",
      "Training Loss: 1.667156 \tValidation Loss: 10.444338\n",
      "epoch:   178\n",
      "batch_idx:     0, train_loss: 9.939977\n",
      "batch_idx:  1000, train_loss: 1.475020\n",
      "batch_idx:  2000, train_loss: 1.676205\n",
      "batch_idx:  3000, train_loss: 1.603651\n",
      "batch_idx:  4000, train_loss: 1.651976\n",
      "batch_idx:  5000, train_loss: 1.724237\n",
      "batch_idx:  6000, train_loss: 1.748399\n",
      "final train_loss: 1.847827\n",
      "batch_idx:     0, valid_loss: 9.738789\n",
      "batch_idx:  1000, valid_loss: 10.218141\n",
      "batch_idx:  2000, valid_loss: 10.491786\n",
      "batch_idx:  3000, valid_loss: 10.579435\n",
      "batch_idx:  4000, valid_loss: 10.503679\n",
      "batch_idx:  5000, valid_loss: 10.312714\n",
      "batch_idx:  6000, valid_loss: 9.964352\n",
      "Training Loss: 1.847827 \tValidation Loss: 9.547352\n",
      "epoch:   179\n",
      "batch_idx:     0, train_loss: 13.372391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  1000, train_loss: 2.039026\n",
      "batch_idx:  2000, train_loss: 1.548138\n",
      "batch_idx:  3000, train_loss: 1.553462\n",
      "batch_idx:  4000, train_loss: 1.539840\n",
      "batch_idx:  5000, train_loss: 1.636101\n",
      "batch_idx:  6000, train_loss: 1.679273\n",
      "final train_loss: 1.769007\n",
      "batch_idx:     0, valid_loss: 9.002757\n",
      "batch_idx:  1000, valid_loss: 9.378140\n",
      "batch_idx:  2000, valid_loss: 9.600018\n",
      "batch_idx:  3000, valid_loss: 9.656013\n",
      "batch_idx:  4000, valid_loss: 9.582648\n",
      "batch_idx:  5000, valid_loss: 9.392822\n",
      "batch_idx:  6000, valid_loss: 9.117354\n",
      "Training Loss: 1.769007 \tValidation Loss: 8.742447\n",
      "epoch:   180\n",
      "batch_idx:     0, train_loss: 10.367226\n",
      "batch_idx:  1000, train_loss: 2.250203\n",
      "batch_idx:  2000, train_loss: 2.097784\n",
      "batch_idx:  3000, train_loss: 1.957549\n",
      "batch_idx:  4000, train_loss: 1.892954\n",
      "batch_idx:  5000, train_loss: 1.959621\n",
      "batch_idx:  6000, train_loss: 2.054843\n",
      "final train_loss: 2.176700\n",
      "batch_idx:     0, valid_loss: 4.076129\n",
      "batch_idx:  1000, valid_loss: 9.746826\n",
      "batch_idx:  2000, valid_loss: 10.027975\n",
      "batch_idx:  3000, valid_loss: 10.332892\n",
      "batch_idx:  4000, valid_loss: 10.636952\n",
      "batch_idx:  5000, valid_loss: 10.703408\n",
      "batch_idx:  6000, valid_loss: 10.513225\n",
      "Training Loss: 2.176700 \tValidation Loss: 10.065472\n",
      "epoch:   181\n",
      "batch_idx:     0, train_loss: 2.574446\n",
      "batch_idx:  1000, train_loss: 0.907690\n",
      "batch_idx:  2000, train_loss: 1.058491\n",
      "batch_idx:  3000, train_loss: 1.095546\n",
      "batch_idx:  4000, train_loss: 1.241456\n",
      "batch_idx:  5000, train_loss: 1.386795\n",
      "batch_idx:  6000, train_loss: 1.584768\n",
      "final train_loss: 1.694787\n",
      "batch_idx:     0, valid_loss: 6.787714\n",
      "batch_idx:  1000, valid_loss: 9.740458\n",
      "batch_idx:  2000, valid_loss: 9.572678\n",
      "batch_idx:  3000, valid_loss: 9.855110\n",
      "batch_idx:  4000, valid_loss: 10.585097\n",
      "batch_idx:  5000, valid_loss: 10.969988\n",
      "batch_idx:  6000, valid_loss: 11.024491\n",
      "Training Loss: 1.694787 \tValidation Loss: 10.751531\n",
      "epoch:   182\n",
      "batch_idx:     0, train_loss: 8.687338\n",
      "batch_idx:  1000, train_loss: 2.173498\n",
      "batch_idx:  2000, train_loss: 2.083677\n",
      "batch_idx:  3000, train_loss: 1.804800\n",
      "batch_idx:  4000, train_loss: 1.604829\n",
      "batch_idx:  5000, train_loss: 1.683982\n",
      "batch_idx:  6000, train_loss: 1.861326\n",
      "final train_loss: 1.895109\n",
      "batch_idx:     0, valid_loss: 5.588312\n",
      "batch_idx:  1000, valid_loss: 12.636987\n",
      "batch_idx:  2000, valid_loss: 12.663868\n",
      "batch_idx:  3000, valid_loss: 12.401460\n",
      "batch_idx:  4000, valid_loss: 12.237180\n",
      "batch_idx:  5000, valid_loss: 11.983379\n",
      "batch_idx:  6000, valid_loss: 11.679512\n",
      "Training Loss: 1.895109 \tValidation Loss: 11.246990\n",
      "epoch:   183\n",
      "batch_idx:     0, train_loss: 6.662818\n",
      "batch_idx:  1000, train_loss: 1.958431\n",
      "batch_idx:  2000, train_loss: 1.918008\n",
      "batch_idx:  3000, train_loss: 1.861024\n",
      "batch_idx:  4000, train_loss: 2.056581\n",
      "batch_idx:  5000, train_loss: 2.160764\n",
      "batch_idx:  6000, train_loss: 2.105969\n",
      "final train_loss: 2.156112\n",
      "batch_idx:     0, valid_loss: 9.863253\n",
      "batch_idx:  1000, valid_loss: 10.238059\n",
      "batch_idx:  2000, valid_loss: 10.368008\n",
      "batch_idx:  3000, valid_loss: 10.440978\n",
      "batch_idx:  4000, valid_loss: 10.417830\n",
      "batch_idx:  5000, valid_loss: 10.217655\n",
      "batch_idx:  6000, valid_loss: 9.886407\n",
      "Training Loss: 2.156112 \tValidation Loss: 9.488090\n",
      "epoch:   184\n",
      "batch_idx:     0, train_loss: 4.825019\n",
      "batch_idx:  1000, train_loss: 2.208740\n",
      "batch_idx:  2000, train_loss: 2.331946\n",
      "batch_idx:  3000, train_loss: 2.271987\n",
      "batch_idx:  4000, train_loss: 2.063942\n",
      "batch_idx:  5000, train_loss: 1.869563\n",
      "batch_idx:  6000, train_loss: 1.948155\n",
      "final train_loss: 1.896208\n",
      "batch_idx:     0, valid_loss: 5.806379\n",
      "batch_idx:  1000, valid_loss: 11.873631\n",
      "batch_idx:  2000, valid_loss: 12.462215\n",
      "batch_idx:  3000, valid_loss: 12.407620\n",
      "batch_idx:  4000, valid_loss: 12.357368\n",
      "batch_idx:  5000, valid_loss: 12.199079\n",
      "batch_idx:  6000, valid_loss: 11.950215\n",
      "Training Loss: 1.896208 \tValidation Loss: 11.586022\n",
      "epoch:   185\n",
      "batch_idx:     0, train_loss: 7.229574\n",
      "batch_idx:  1000, train_loss: 1.637687\n",
      "batch_idx:  2000, train_loss: 1.847685\n",
      "batch_idx:  3000, train_loss: 1.928749\n",
      "batch_idx:  4000, train_loss: 1.951109\n",
      "batch_idx:  5000, train_loss: 1.944535\n",
      "batch_idx:  6000, train_loss: 2.022527\n",
      "final train_loss: 2.028858\n",
      "batch_idx:     0, valid_loss: 5.309955\n",
      "batch_idx:  1000, valid_loss: 6.523070\n",
      "batch_idx:  2000, valid_loss: 6.864436\n",
      "batch_idx:  3000, valid_loss: 6.864733\n",
      "batch_idx:  4000, valid_loss: 6.833979\n",
      "batch_idx:  5000, valid_loss: 6.733996\n",
      "batch_idx:  6000, valid_loss: 6.557774\n",
      "Training Loss: 2.028858 \tValidation Loss: 6.278293\n",
      "epoch:   186\n",
      "batch_idx:     0, train_loss: 6.796997\n",
      "batch_idx:  1000, train_loss: 1.973436\n",
      "batch_idx:  2000, train_loss: 2.323991\n",
      "batch_idx:  3000, train_loss: 2.335594\n",
      "batch_idx:  4000, train_loss: 2.301081\n",
      "batch_idx:  5000, train_loss: 2.157066\n",
      "batch_idx:  6000, train_loss: 2.275542\n",
      "final train_loss: 2.385588\n",
      "batch_idx:     0, valid_loss: 10.298590\n",
      "batch_idx:  1000, valid_loss: 11.959925\n",
      "batch_idx:  2000, valid_loss: 11.749434\n",
      "batch_idx:  3000, valid_loss: 11.528392\n",
      "batch_idx:  4000, valid_loss: 11.302011\n",
      "batch_idx:  5000, valid_loss: 11.128844\n",
      "batch_idx:  6000, valid_loss: 10.758658\n",
      "Training Loss: 2.385588 \tValidation Loss: 10.343788\n",
      "epoch:   187\n",
      "batch_idx:     0, train_loss: 13.926764\n",
      "batch_idx:  1000, train_loss: 2.617362\n",
      "batch_idx:  2000, train_loss: 2.339327\n",
      "batch_idx:  3000, train_loss: 2.247362\n",
      "batch_idx:  4000, train_loss: 2.082373\n",
      "batch_idx:  5000, train_loss: 1.855838\n",
      "batch_idx:  6000, train_loss: 1.852614\n",
      "final train_loss: 1.875378\n",
      "batch_idx:     0, valid_loss: 9.113174\n",
      "batch_idx:  1000, valid_loss: 9.211486\n",
      "batch_idx:  2000, valid_loss: 9.563916\n",
      "batch_idx:  3000, valid_loss: 9.646376\n",
      "batch_idx:  4000, valid_loss: 9.682231\n",
      "batch_idx:  5000, valid_loss: 9.576726\n",
      "batch_idx:  6000, valid_loss: 9.308509\n",
      "Training Loss: 1.875378 \tValidation Loss: 8.901233\n",
      "epoch:   188\n",
      "batch_idx:     0, train_loss: 10.625338\n",
      "batch_idx:  1000, train_loss: 2.181312\n",
      "batch_idx:  2000, train_loss: 2.042872\n",
      "batch_idx:  3000, train_loss: 2.002867\n",
      "batch_idx:  4000, train_loss: 1.887527\n",
      "batch_idx:  5000, train_loss: 1.879591\n",
      "batch_idx:  6000, train_loss: 1.891541\n",
      "final train_loss: 1.947929\n",
      "batch_idx:     0, valid_loss: 5.836667\n",
      "batch_idx:  1000, valid_loss: 5.830443\n",
      "batch_idx:  2000, valid_loss: 5.913737\n",
      "batch_idx:  3000, valid_loss: 5.881060\n",
      "batch_idx:  4000, valid_loss: 5.826811\n",
      "batch_idx:  5000, valid_loss: 5.720665\n",
      "batch_idx:  6000, valid_loss: 5.582865\n",
      "Training Loss: 1.947929 \tValidation Loss: 5.402438\n",
      "Training loss decreased (6.093254 --> 5.402438).  Saving model ...\n",
      "epoch:   189\n",
      "batch_idx:     0, train_loss: 6.234493\n",
      "batch_idx:  1000, train_loss: 1.872825\n",
      "batch_idx:  2000, train_loss: 2.022322\n",
      "batch_idx:  3000, train_loss: 1.821321\n",
      "batch_idx:  4000, train_loss: 1.809604\n",
      "batch_idx:  5000, train_loss: 1.839709\n",
      "batch_idx:  6000, train_loss: 1.917457\n",
      "final train_loss: 1.939528\n",
      "batch_idx:     0, valid_loss: 6.330524\n",
      "batch_idx:  1000, valid_loss: 10.809180\n",
      "batch_idx:  2000, valid_loss: 11.727602\n",
      "batch_idx:  3000, valid_loss: 11.873766\n",
      "batch_idx:  4000, valid_loss: 11.785576\n",
      "batch_idx:  5000, valid_loss: 11.551810\n",
      "batch_idx:  6000, valid_loss: 11.243133\n",
      "Training Loss: 1.939528 \tValidation Loss: 10.854328\n",
      "epoch:   190\n",
      "batch_idx:     0, train_loss: 7.062975\n",
      "batch_idx:  1000, train_loss: 1.876446\n",
      "batch_idx:  2000, train_loss: 1.820006\n",
      "batch_idx:  3000, train_loss: 1.809436\n",
      "batch_idx:  4000, train_loss: 1.811931\n",
      "batch_idx:  5000, train_loss: 1.685440\n",
      "batch_idx:  6000, train_loss: 1.768849\n",
      "final train_loss: 1.853460\n",
      "batch_idx:     0, valid_loss: 8.151831\n",
      "batch_idx:  1000, valid_loss: 9.867785\n",
      "batch_idx:  2000, valid_loss: 9.941602\n",
      "batch_idx:  3000, valid_loss: 9.968906\n",
      "batch_idx:  4000, valid_loss: 9.863132\n",
      "batch_idx:  5000, valid_loss: 9.629199\n",
      "batch_idx:  6000, valid_loss: 9.330058\n",
      "Training Loss: 1.853460 \tValidation Loss: 8.953555\n",
      "epoch:   191\n",
      "batch_idx:     0, train_loss: 10.708882\n",
      "batch_idx:  1000, train_loss: 1.705118\n",
      "batch_idx:  2000, train_loss: 1.461588\n",
      "batch_idx:  3000, train_loss: 1.332502\n",
      "batch_idx:  4000, train_loss: 1.150360\n",
      "batch_idx:  5000, train_loss: 1.167769\n",
      "batch_idx:  6000, train_loss: 1.355730\n",
      "final train_loss: 1.417510\n",
      "batch_idx:     0, valid_loss: 9.839265\n",
      "batch_idx:  1000, valid_loss: 9.713206\n",
      "batch_idx:  2000, valid_loss: 9.967404\n",
      "batch_idx:  3000, valid_loss: 9.798398\n",
      "batch_idx:  4000, valid_loss: 9.865061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  5000, valid_loss: 9.954178\n",
      "batch_idx:  6000, valid_loss: 9.867649\n",
      "Training Loss: 1.417510 \tValidation Loss: 9.537218\n",
      "epoch:   192\n",
      "batch_idx:     0, train_loss: 9.883172\n",
      "batch_idx:  1000, train_loss: 1.850283\n",
      "batch_idx:  2000, train_loss: 1.692915\n",
      "batch_idx:  3000, train_loss: 1.653571\n",
      "batch_idx:  4000, train_loss: 1.679492\n",
      "batch_idx:  5000, train_loss: 1.754437\n",
      "batch_idx:  6000, train_loss: 1.707654\n",
      "final train_loss: 1.941174\n",
      "batch_idx:     0, valid_loss: 7.791487\n",
      "batch_idx:  1000, valid_loss: 9.116601\n",
      "batch_idx:  2000, valid_loss: 10.706686\n",
      "batch_idx:  3000, valid_loss: 11.653307\n",
      "batch_idx:  4000, valid_loss: 12.053524\n",
      "batch_idx:  5000, valid_loss: 12.181334\n",
      "batch_idx:  6000, valid_loss: 12.043739\n",
      "Training Loss: 1.941174 \tValidation Loss: 11.710904\n",
      "epoch:   193\n",
      "batch_idx:     0, train_loss: 4.917046\n",
      "batch_idx:  1000, train_loss: 1.150742\n",
      "batch_idx:  2000, train_loss: 1.687880\n",
      "batch_idx:  3000, train_loss: 1.464780\n",
      "batch_idx:  4000, train_loss: 1.416054\n",
      "batch_idx:  5000, train_loss: 1.407555\n",
      "batch_idx:  6000, train_loss: 1.358695\n",
      "final train_loss: 1.271408\n",
      "batch_idx:     0, valid_loss: 5.983785\n",
      "batch_idx:  1000, valid_loss: 14.490387\n",
      "batch_idx:  2000, valid_loss: 18.584932\n",
      "batch_idx:  3000, valid_loss: 21.057697\n",
      "batch_idx:  4000, valid_loss: 22.532537\n",
      "batch_idx:  5000, valid_loss: 23.362682\n",
      "batch_idx:  6000, valid_loss: 22.906622\n",
      "Training Loss: 1.271408 \tValidation Loss: 21.840696\n",
      "epoch:   194\n",
      "batch_idx:     0, train_loss: 6.760708\n",
      "batch_idx:  1000, train_loss: 0.335354\n",
      "batch_idx:  2000, train_loss: 0.359207\n",
      "batch_idx:  3000, train_loss: 0.377030\n",
      "batch_idx:  4000, train_loss: 0.405033\n",
      "batch_idx:  5000, train_loss: 0.429831\n",
      "batch_idx:  6000, train_loss: 0.506226\n",
      "final train_loss: 0.626818\n",
      "batch_idx:     0, valid_loss: 6.134636\n",
      "batch_idx:  1000, valid_loss: 7.735856\n",
      "batch_idx:  2000, valid_loss: 8.507866\n",
      "batch_idx:  3000, valid_loss: 9.111393\n",
      "batch_idx:  4000, valid_loss: 9.553807\n",
      "batch_idx:  5000, valid_loss: 9.775608\n",
      "batch_idx:  6000, valid_loss: 9.537245\n",
      "Training Loss: 0.626818 \tValidation Loss: 9.072688\n",
      "epoch:   195\n",
      "batch_idx:     0, train_loss: 6.426388\n",
      "batch_idx:  1000, train_loss: 1.283267\n",
      "batch_idx:  2000, train_loss: 1.307391\n",
      "batch_idx:  3000, train_loss: 1.338539\n",
      "batch_idx:  4000, train_loss: 1.373366\n",
      "batch_idx:  5000, train_loss: 1.406552\n",
      "batch_idx:  6000, train_loss: 1.378820\n",
      "final train_loss: 1.303245\n",
      "batch_idx:     0, valid_loss: 9.581550\n",
      "batch_idx:  1000, valid_loss: 12.730494\n",
      "batch_idx:  2000, valid_loss: 15.907517\n",
      "batch_idx:  3000, valid_loss: 17.426704\n",
      "batch_idx:  4000, valid_loss: 18.215878\n",
      "batch_idx:  5000, valid_loss: 18.675978\n",
      "batch_idx:  6000, valid_loss: 18.213726\n",
      "Training Loss: 1.303245 \tValidation Loss: 17.422815\n",
      "epoch:   196\n",
      "batch_idx:     0, train_loss: 13.331112\n",
      "batch_idx:  1000, train_loss: 0.567943\n",
      "batch_idx:  2000, train_loss: 0.683585\n",
      "batch_idx:  3000, train_loss: 1.007807\n",
      "batch_idx:  4000, train_loss: 1.090798\n",
      "batch_idx:  5000, train_loss: 1.103197\n",
      "batch_idx:  6000, train_loss: 1.149071\n",
      "final train_loss: 1.188404\n",
      "batch_idx:     0, valid_loss: 10.782758\n",
      "batch_idx:  1000, valid_loss: 11.975619\n",
      "batch_idx:  2000, valid_loss: 12.551712\n",
      "batch_idx:  3000, valid_loss: 12.753876\n",
      "batch_idx:  4000, valid_loss: 12.935657\n",
      "batch_idx:  5000, valid_loss: 12.923268\n",
      "batch_idx:  6000, valid_loss: 12.798723\n",
      "Training Loss: 1.188404 \tValidation Loss: 12.283844\n",
      "epoch:   197\n",
      "batch_idx:     0, train_loss: 11.706839\n",
      "batch_idx:  1000, train_loss: 0.908822\n",
      "batch_idx:  2000, train_loss: 1.164274\n",
      "batch_idx:  3000, train_loss: 1.477604\n",
      "batch_idx:  4000, train_loss: 1.545889\n",
      "batch_idx:  5000, train_loss: 1.597527\n",
      "batch_idx:  6000, train_loss: 1.599508\n",
      "final train_loss: 1.667003\n",
      "batch_idx:     0, valid_loss: 9.812073\n",
      "batch_idx:  1000, valid_loss: 10.399932\n",
      "batch_idx:  2000, valid_loss: 10.371512\n",
      "batch_idx:  3000, valid_loss: 10.393634\n",
      "batch_idx:  4000, valid_loss: 10.348922\n",
      "batch_idx:  5000, valid_loss: 10.233381\n",
      "batch_idx:  6000, valid_loss: 9.879301\n",
      "Training Loss: 1.667003 \tValidation Loss: 9.472726\n",
      "epoch:   198\n",
      "batch_idx:     0, train_loss: 12.323313\n",
      "batch_idx:  1000, train_loss: 1.792644\n",
      "batch_idx:  2000, train_loss: 1.807759\n",
      "batch_idx:  3000, train_loss: 1.725901\n",
      "batch_idx:  4000, train_loss: 1.832747\n",
      "batch_idx:  5000, train_loss: 1.925829\n",
      "batch_idx:  6000, train_loss: 1.889679\n",
      "final train_loss: 1.922181\n",
      "batch_idx:     0, valid_loss: 8.293113\n",
      "batch_idx:  1000, valid_loss: 9.547432\n",
      "batch_idx:  2000, valid_loss: 9.593076\n",
      "batch_idx:  3000, valid_loss: 9.607057\n",
      "batch_idx:  4000, valid_loss: 9.311459\n",
      "batch_idx:  5000, valid_loss: 9.403576\n",
      "batch_idx:  6000, valid_loss: 9.272686\n",
      "Training Loss: 1.922181 \tValidation Loss: 8.931156\n",
      "epoch:   199\n",
      "batch_idx:     0, train_loss: 11.349139\n",
      "batch_idx:  1000, train_loss: 1.339808\n",
      "batch_idx:  2000, train_loss: 1.507978\n",
      "batch_idx:  3000, train_loss: 1.706876\n",
      "batch_idx:  4000, train_loss: 1.702458\n",
      "batch_idx:  5000, train_loss: 1.784083\n",
      "batch_idx:  6000, train_loss: 1.710807\n",
      "final train_loss: 1.763214\n",
      "batch_idx:     0, valid_loss: 11.319656\n",
      "batch_idx:  1000, valid_loss: 14.415160\n",
      "batch_idx:  2000, valid_loss: 15.650215\n",
      "batch_idx:  3000, valid_loss: 16.090937\n",
      "batch_idx:  4000, valid_loss: 16.240976\n",
      "batch_idx:  5000, valid_loss: 16.172514\n",
      "batch_idx:  6000, valid_loss: 15.700229\n",
      "Training Loss: 1.763214 \tValidation Loss: 15.151653\n",
      "epoch:   200\n",
      "batch_idx:     0, train_loss: 13.319007\n",
      "batch_idx:  1000, train_loss: 1.259311\n",
      "batch_idx:  2000, train_loss: 1.443693\n",
      "batch_idx:  3000, train_loss: 1.631318\n",
      "batch_idx:  4000, train_loss: 1.751341\n",
      "batch_idx:  5000, train_loss: 1.879365\n",
      "batch_idx:  6000, train_loss: 1.816576\n",
      "final train_loss: 1.769614\n",
      "batch_idx:     0, valid_loss: 9.043442\n",
      "batch_idx:  1000, valid_loss: 12.502857\n",
      "batch_idx:  2000, valid_loss: 12.918816\n",
      "batch_idx:  3000, valid_loss: 13.430175\n",
      "batch_idx:  4000, valid_loss: 13.729249\n",
      "batch_idx:  5000, valid_loss: 13.777407\n",
      "batch_idx:  6000, valid_loss: 13.551223\n",
      "Training Loss: 1.769614 \tValidation Loss: 13.006098\n",
      "epoch:   201\n",
      "batch_idx:     0, train_loss: 9.514219\n",
      "batch_idx:  1000, train_loss: 1.430750\n",
      "batch_idx:  2000, train_loss: 1.947335\n",
      "batch_idx:  3000, train_loss: 1.747562\n",
      "batch_idx:  4000, train_loss: 1.914975\n",
      "batch_idx:  5000, train_loss: 1.753290\n",
      "batch_idx:  6000, train_loss: 1.720737\n",
      "final train_loss: 1.822456\n",
      "batch_idx:     0, valid_loss: 9.644561\n",
      "batch_idx:  1000, valid_loss: 10.711306\n",
      "batch_idx:  2000, valid_loss: 10.703818\n",
      "batch_idx:  3000, valid_loss: 10.710654\n",
      "batch_idx:  4000, valid_loss: 10.609882\n",
      "batch_idx:  5000, valid_loss: 10.461778\n",
      "batch_idx:  6000, valid_loss: 10.113066\n",
      "Training Loss: 1.822456 \tValidation Loss: 9.703021\n",
      "epoch:   202\n",
      "batch_idx:     0, train_loss: 13.561741\n",
      "batch_idx:  1000, train_loss: 1.485573\n",
      "batch_idx:  2000, train_loss: 1.444655\n",
      "batch_idx:  3000, train_loss: 1.576730\n",
      "batch_idx:  4000, train_loss: 1.598810\n",
      "batch_idx:  5000, train_loss: 1.755762\n",
      "batch_idx:  6000, train_loss: 1.813512\n",
      "final train_loss: 1.758322\n",
      "batch_idx:     0, valid_loss: 10.330242\n",
      "batch_idx:  1000, valid_loss: 15.349561\n",
      "batch_idx:  2000, valid_loss: 16.610489\n",
      "batch_idx:  3000, valid_loss: 16.966232\n",
      "batch_idx:  4000, valid_loss: 17.227972\n",
      "batch_idx:  5000, valid_loss: 17.391975\n",
      "batch_idx:  6000, valid_loss: 16.968336\n",
      "Training Loss: 1.758322 \tValidation Loss: 16.152866\n",
      "epoch:   203\n",
      "batch_idx:     0, train_loss: 7.663280\n",
      "batch_idx:  1000, train_loss: 1.340105\n",
      "batch_idx:  2000, train_loss: 1.525169\n",
      "batch_idx:  3000, train_loss: 1.675815\n",
      "batch_idx:  4000, train_loss: 1.758461\n",
      "batch_idx:  5000, train_loss: 1.866439\n",
      "batch_idx:  6000, train_loss: 1.803874\n",
      "final train_loss: 1.815635\n",
      "batch_idx:     0, valid_loss: 6.789062\n",
      "batch_idx:  1000, valid_loss: 8.924627\n",
      "batch_idx:  2000, valid_loss: 10.055313\n",
      "batch_idx:  3000, valid_loss: 10.392421\n",
      "batch_idx:  4000, valid_loss: 10.585460\n",
      "batch_idx:  5000, valid_loss: 10.719663\n",
      "batch_idx:  6000, valid_loss: 10.544959\n",
      "Training Loss: 1.815635 \tValidation Loss: 10.192060\n",
      "epoch:   204\n",
      "batch_idx:     0, train_loss: 8.515290\n",
      "batch_idx:  1000, train_loss: 1.476051\n",
      "batch_idx:  2000, train_loss: 1.607093\n",
      "batch_idx:  3000, train_loss: 1.610216\n",
      "batch_idx:  4000, train_loss: 1.645283\n",
      "batch_idx:  5000, train_loss: 1.734770\n",
      "batch_idx:  6000, train_loss: 1.798473\n",
      "final train_loss: 1.875644\n",
      "batch_idx:     0, valid_loss: 9.244250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  1000, valid_loss: 9.613518\n",
      "batch_idx:  2000, valid_loss: 9.632249\n",
      "batch_idx:  3000, valid_loss: 9.634123\n",
      "batch_idx:  4000, valid_loss: 9.654112\n",
      "batch_idx:  5000, valid_loss: 9.568261\n",
      "batch_idx:  6000, valid_loss: 9.299567\n",
      "Training Loss: 1.875644 \tValidation Loss: 8.931631\n",
      "epoch:   205\n",
      "batch_idx:     0, train_loss: 11.549079\n",
      "batch_idx:  1000, train_loss: 1.661991\n",
      "batch_idx:  2000, train_loss: 1.635865\n",
      "batch_idx:  3000, train_loss: 1.709276\n",
      "batch_idx:  4000, train_loss: 1.802953\n",
      "batch_idx:  5000, train_loss: 1.915189\n",
      "batch_idx:  6000, train_loss: 1.901430\n",
      "final train_loss: 1.871958\n",
      "batch_idx:     0, valid_loss: 8.191433\n",
      "batch_idx:  1000, valid_loss: 9.288290\n",
      "batch_idx:  2000, valid_loss: 9.680874\n",
      "batch_idx:  3000, valid_loss: 9.793471\n",
      "batch_idx:  4000, valid_loss: 9.864356\n",
      "batch_idx:  5000, valid_loss: 9.830966\n",
      "batch_idx:  6000, valid_loss: 9.697964\n",
      "Training Loss: 1.871958 \tValidation Loss: 9.412416\n",
      "epoch:   206\n",
      "batch_idx:     0, train_loss: 9.520706\n",
      "batch_idx:  1000, train_loss: 1.643230\n",
      "batch_idx:  2000, train_loss: 1.893188\n",
      "batch_idx:  3000, train_loss: 1.964596\n",
      "batch_idx:  4000, train_loss: 2.063696\n",
      "batch_idx:  5000, train_loss: 2.151118\n",
      "batch_idx:  6000, train_loss: 2.038797\n",
      "final train_loss: 2.047595\n",
      "batch_idx:     0, valid_loss: 10.268403\n",
      "batch_idx:  1000, valid_loss: 10.832616\n",
      "batch_idx:  2000, valid_loss: 10.780633\n",
      "batch_idx:  3000, valid_loss: 11.314734\n",
      "batch_idx:  4000, valid_loss: 11.648830\n",
      "batch_idx:  5000, valid_loss: 11.701653\n",
      "batch_idx:  6000, valid_loss: 11.651198\n",
      "Training Loss: 2.047595 \tValidation Loss: 11.238781\n",
      "epoch:   207\n",
      "batch_idx:     0, train_loss: 12.569083\n",
      "batch_idx:  1000, train_loss: 1.189016\n",
      "batch_idx:  2000, train_loss: 1.309634\n",
      "batch_idx:  3000, train_loss: 1.540180\n",
      "batch_idx:  4000, train_loss: 1.732731\n",
      "batch_idx:  5000, train_loss: 1.762146\n",
      "batch_idx:  6000, train_loss: 1.783286\n",
      "final train_loss: 1.783071\n",
      "batch_idx:     0, valid_loss: 10.143459\n",
      "batch_idx:  1000, valid_loss: 10.547731\n",
      "batch_idx:  2000, valid_loss: 10.274225\n",
      "batch_idx:  3000, valid_loss: 9.984606\n",
      "batch_idx:  4000, valid_loss: 9.757561\n",
      "batch_idx:  5000, valid_loss: 9.547636\n",
      "batch_idx:  6000, valid_loss: 9.165155\n",
      "Training Loss: 1.783071 \tValidation Loss: 8.717194\n",
      "epoch:   208\n",
      "batch_idx:     0, train_loss: 11.777956\n",
      "batch_idx:  1000, train_loss: 1.250820\n",
      "batch_idx:  2000, train_loss: 1.554458\n",
      "batch_idx:  3000, train_loss: 1.871921\n",
      "batch_idx:  4000, train_loss: 1.945123\n",
      "batch_idx:  5000, train_loss: 1.856714\n",
      "batch_idx:  6000, train_loss: 1.874852\n",
      "final train_loss: 1.821368\n",
      "batch_idx:     0, valid_loss: 8.133689\n",
      "batch_idx:  1000, valid_loss: 13.890554\n",
      "batch_idx:  2000, valid_loss: 14.576283\n",
      "batch_idx:  3000, valid_loss: 14.569173\n",
      "batch_idx:  4000, valid_loss: 14.525257\n",
      "batch_idx:  5000, valid_loss: 14.446576\n",
      "batch_idx:  6000, valid_loss: 13.979739\n",
      "Training Loss: 1.821368 \tValidation Loss: 13.271234\n",
      "epoch:   209\n",
      "batch_idx:     0, train_loss: 10.988607\n",
      "batch_idx:  1000, train_loss: 0.940786\n",
      "batch_idx:  2000, train_loss: 1.334819\n",
      "batch_idx:  3000, train_loss: 1.414234\n",
      "batch_idx:  4000, train_loss: 1.485977\n",
      "batch_idx:  5000, train_loss: 1.616931\n",
      "batch_idx:  6000, train_loss: 1.767553\n",
      "final train_loss: 1.880792\n",
      "batch_idx:     0, valid_loss: 8.885325\n",
      "batch_idx:  1000, valid_loss: 10.449985\n",
      "batch_idx:  2000, valid_loss: 10.462731\n",
      "batch_idx:  3000, valid_loss: 10.250022\n",
      "batch_idx:  4000, valid_loss: 10.030066\n",
      "batch_idx:  5000, valid_loss: 9.806345\n",
      "batch_idx:  6000, valid_loss: 9.476217\n",
      "Training Loss: 1.880792 \tValidation Loss: 9.086761\n",
      "epoch:   210\n",
      "batch_idx:     0, train_loss: 12.500544\n",
      "batch_idx:  1000, train_loss: 2.406554\n",
      "batch_idx:  2000, train_loss: 2.016130\n",
      "batch_idx:  3000, train_loss: 1.819982\n",
      "batch_idx:  4000, train_loss: 1.696332\n",
      "batch_idx:  5000, train_loss: 1.707880\n",
      "batch_idx:  6000, train_loss: 1.801055\n",
      "final train_loss: 1.906647\n",
      "batch_idx:     0, valid_loss: 10.295577\n",
      "batch_idx:  1000, valid_loss: 10.308391\n",
      "batch_idx:  2000, valid_loss: 10.310689\n",
      "batch_idx:  3000, valid_loss: 10.291679\n",
      "batch_idx:  4000, valid_loss: 10.129085\n",
      "batch_idx:  5000, valid_loss: 9.888773\n",
      "batch_idx:  6000, valid_loss: 9.547250\n",
      "Training Loss: 1.906647 \tValidation Loss: 9.161012\n",
      "epoch:   211\n",
      "batch_idx:     0, train_loss: 11.241373\n",
      "batch_idx:  1000, train_loss: 1.978333\n",
      "batch_idx:  2000, train_loss: 1.910591\n",
      "batch_idx:  3000, train_loss: 1.869072\n",
      "batch_idx:  4000, train_loss: 1.847915\n",
      "batch_idx:  5000, train_loss: 1.781550\n",
      "batch_idx:  6000, train_loss: 1.812762\n",
      "final train_loss: 1.853577\n",
      "batch_idx:     0, valid_loss: 9.207585\n",
      "batch_idx:  1000, valid_loss: 9.163891\n",
      "batch_idx:  2000, valid_loss: 9.126885\n",
      "batch_idx:  3000, valid_loss: 9.086543\n",
      "batch_idx:  4000, valid_loss: 8.936780\n",
      "batch_idx:  5000, valid_loss: 8.754047\n",
      "batch_idx:  6000, valid_loss: 8.532335\n",
      "Training Loss: 1.853577 \tValidation Loss: 8.144302\n",
      "epoch:   212\n",
      "batch_idx:     0, train_loss: 11.528294\n",
      "batch_idx:  1000, train_loss: 1.467160\n",
      "batch_idx:  2000, train_loss: 1.300466\n",
      "batch_idx:  3000, train_loss: 1.334039\n",
      "batch_idx:  4000, train_loss: 1.345923\n",
      "batch_idx:  5000, train_loss: 1.429186\n",
      "batch_idx:  6000, train_loss: 1.602696\n",
      "final train_loss: 1.644886\n",
      "batch_idx:     0, valid_loss: 5.521209\n",
      "batch_idx:  1000, valid_loss: 11.096810\n",
      "batch_idx:  2000, valid_loss: 11.489744\n",
      "batch_idx:  3000, valid_loss: 11.529408\n",
      "batch_idx:  4000, valid_loss: 11.582012\n",
      "batch_idx:  5000, valid_loss: 11.568128\n",
      "batch_idx:  6000, valid_loss: 11.385526\n",
      "Training Loss: 1.644886 \tValidation Loss: 10.957242\n",
      "epoch:   213\n",
      "batch_idx:     0, train_loss: 6.950529\n",
      "batch_idx:  1000, train_loss: 1.524169\n",
      "batch_idx:  2000, train_loss: 1.561621\n",
      "batch_idx:  3000, train_loss: 1.570337\n",
      "batch_idx:  4000, train_loss: 1.578931\n",
      "batch_idx:  5000, train_loss: 1.662199\n",
      "batch_idx:  6000, train_loss: 1.781448\n",
      "final train_loss: 1.881616\n",
      "batch_idx:     0, valid_loss: 10.292814\n",
      "batch_idx:  1000, valid_loss: 10.811178\n",
      "batch_idx:  2000, valid_loss: 13.679702\n",
      "batch_idx:  3000, valid_loss: 14.975585\n",
      "batch_idx:  4000, valid_loss: 15.478887\n",
      "batch_idx:  5000, valid_loss: 15.618469\n",
      "batch_idx:  6000, valid_loss: 15.414734\n",
      "Training Loss: 1.881616 \tValidation Loss: 15.042313\n",
      "epoch:   214\n",
      "batch_idx:     0, train_loss: 12.968931\n",
      "batch_idx:  1000, train_loss: 1.173950\n",
      "batch_idx:  2000, train_loss: 1.899665\n",
      "batch_idx:  3000, train_loss: 1.645990\n",
      "batch_idx:  4000, train_loss: 1.711455\n",
      "batch_idx:  5000, train_loss: 1.833386\n",
      "batch_idx:  6000, train_loss: 1.916187\n",
      "final train_loss: 1.985775\n",
      "batch_idx:     0, valid_loss: 8.864385\n",
      "batch_idx:  1000, valid_loss: 8.697317\n",
      "batch_idx:  2000, valid_loss: 8.679671\n",
      "batch_idx:  3000, valid_loss: 8.688700\n",
      "batch_idx:  4000, valid_loss: 8.730386\n",
      "batch_idx:  5000, valid_loss: 8.587165\n",
      "batch_idx:  6000, valid_loss: 8.401820\n",
      "Training Loss: 1.985775 \tValidation Loss: 8.024840\n",
      "epoch:   215\n",
      "batch_idx:     0, train_loss: 10.207809\n",
      "batch_idx:  1000, train_loss: 1.655155\n",
      "batch_idx:  2000, train_loss: 1.498063\n",
      "batch_idx:  3000, train_loss: 1.640796\n",
      "batch_idx:  4000, train_loss: 1.727652\n",
      "batch_idx:  5000, train_loss: 1.826250\n",
      "batch_idx:  6000, train_loss: 1.899833\n",
      "final train_loss: 1.862263\n",
      "batch_idx:     0, valid_loss: 7.370539\n",
      "batch_idx:  1000, valid_loss: 8.877101\n",
      "batch_idx:  2000, valid_loss: 10.225381\n",
      "batch_idx:  3000, valid_loss: 10.636946\n",
      "batch_idx:  4000, valid_loss: 10.842943\n",
      "batch_idx:  5000, valid_loss: 10.882471\n",
      "batch_idx:  6000, valid_loss: 10.717030\n",
      "Training Loss: 1.862263 \tValidation Loss: 10.304718\n",
      "epoch:   216\n",
      "batch_idx:     0, train_loss: 11.665655\n",
      "batch_idx:  1000, train_loss: 1.311494\n",
      "batch_idx:  2000, train_loss: 1.485749\n",
      "batch_idx:  3000, train_loss: 1.386284\n",
      "batch_idx:  4000, train_loss: 1.509127\n",
      "batch_idx:  5000, train_loss: 1.713652\n",
      "batch_idx:  6000, train_loss: 1.637792\n",
      "final train_loss: 1.644249\n",
      "batch_idx:     0, valid_loss: 6.512136\n",
      "batch_idx:  1000, valid_loss: 9.099184\n",
      "batch_idx:  2000, valid_loss: 10.295604\n",
      "batch_idx:  3000, valid_loss: 10.690322\n",
      "batch_idx:  4000, valid_loss: 10.910982\n",
      "batch_idx:  5000, valid_loss: 10.847301\n",
      "batch_idx:  6000, valid_loss: 10.722878\n",
      "Training Loss: 1.644249 \tValidation Loss: 10.351480\n",
      "epoch:   217\n",
      "batch_idx:     0, train_loss: 7.598544\n",
      "batch_idx:  1000, train_loss: 1.901614\n",
      "batch_idx:  2000, train_loss: 1.906547\n",
      "batch_idx:  3000, train_loss: 1.914618\n",
      "batch_idx:  4000, train_loss: 2.009295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  5000, train_loss: 1.950099\n",
      "batch_idx:  6000, train_loss: 1.966708\n",
      "final train_loss: 1.882852\n",
      "batch_idx:     0, valid_loss: 6.519439\n",
      "batch_idx:  1000, valid_loss: 10.040334\n",
      "batch_idx:  2000, valid_loss: 11.280352\n",
      "batch_idx:  3000, valid_loss: 11.548587\n",
      "batch_idx:  4000, valid_loss: 11.457113\n",
      "batch_idx:  5000, valid_loss: 11.349034\n",
      "batch_idx:  6000, valid_loss: 11.113245\n",
      "Training Loss: 1.882852 \tValidation Loss: 10.728235\n",
      "epoch:   218\n",
      "batch_idx:     0, train_loss: 8.212432\n",
      "batch_idx:  1000, train_loss: 1.553379\n",
      "batch_idx:  2000, train_loss: 1.971442\n",
      "batch_idx:  3000, train_loss: 1.690847\n",
      "batch_idx:  4000, train_loss: 1.550133\n",
      "batch_idx:  5000, train_loss: 1.661540\n",
      "batch_idx:  6000, train_loss: 1.662106\n",
      "final train_loss: 1.536399\n",
      "batch_idx:     0, valid_loss: 5.835615\n",
      "batch_idx:  1000, valid_loss: 14.512053\n",
      "batch_idx:  2000, valid_loss: 18.511726\n",
      "batch_idx:  3000, valid_loss: 20.721735\n",
      "batch_idx:  4000, valid_loss: 21.915548\n",
      "batch_idx:  5000, valid_loss: 22.739433\n",
      "batch_idx:  6000, valid_loss: 22.539909\n",
      "Training Loss: 1.536399 \tValidation Loss: 21.520020\n",
      "epoch:   219\n",
      "batch_idx:     0, train_loss: 5.846186\n",
      "batch_idx:  1000, train_loss: 0.308764\n",
      "batch_idx:  2000, train_loss: 0.327225\n",
      "batch_idx:  3000, train_loss: 0.353781\n",
      "batch_idx:  4000, train_loss: 0.390627\n",
      "batch_idx:  5000, train_loss: 0.414566\n",
      "batch_idx:  6000, train_loss: 0.456999\n",
      "final train_loss: 0.498633\n",
      "batch_idx:     0, valid_loss: 8.849380\n",
      "batch_idx:  1000, valid_loss: 15.700065\n",
      "batch_idx:  2000, valid_loss: 17.050318\n",
      "batch_idx:  3000, valid_loss: 17.666445\n",
      "batch_idx:  4000, valid_loss: 17.874891\n",
      "batch_idx:  5000, valid_loss: 17.822111\n",
      "batch_idx:  6000, valid_loss: 17.371000\n",
      "Training Loss: 0.498633 \tValidation Loss: 16.580004\n",
      "epoch:   220\n",
      "batch_idx:     0, train_loss: 8.965438\n",
      "batch_idx:  1000, train_loss: 0.687635\n",
      "batch_idx:  2000, train_loss: 0.717282\n",
      "batch_idx:  3000, train_loss: 0.780295\n",
      "batch_idx:  4000, train_loss: 0.829125\n",
      "batch_idx:  5000, train_loss: 0.879528\n",
      "batch_idx:  6000, train_loss: 0.960109\n",
      "final train_loss: 1.013035\n",
      "batch_idx:     0, valid_loss: 11.945541\n",
      "batch_idx:  1000, valid_loss: 11.703037\n",
      "batch_idx:  2000, valid_loss: 13.497019\n",
      "batch_idx:  3000, valid_loss: 14.464034\n",
      "batch_idx:  4000, valid_loss: 14.878812\n",
      "batch_idx:  5000, valid_loss: 15.004456\n",
      "batch_idx:  6000, valid_loss: 14.913403\n",
      "Training Loss: 1.013035 \tValidation Loss: 14.359824\n",
      "epoch:   221\n",
      "batch_idx:     0, train_loss: 13.640043\n",
      "batch_idx:  1000, train_loss: 0.645814\n",
      "batch_idx:  2000, train_loss: 0.650874\n",
      "batch_idx:  3000, train_loss: 0.651172\n",
      "batch_idx:  4000, train_loss: 0.712374\n",
      "batch_idx:  5000, train_loss: 0.753263\n",
      "batch_idx:  6000, train_loss: 0.859665\n",
      "final train_loss: 0.944686\n",
      "batch_idx:     0, valid_loss: 6.873106\n",
      "batch_idx:  1000, valid_loss: 7.770648\n",
      "batch_idx:  2000, valid_loss: 9.409124\n",
      "batch_idx:  3000, valid_loss: 10.342319\n",
      "batch_idx:  4000, valid_loss: 10.796883\n",
      "batch_idx:  5000, valid_loss: 10.960336\n",
      "batch_idx:  6000, valid_loss: 10.933104\n",
      "Training Loss: 0.944686 \tValidation Loss: 10.554112\n",
      "epoch:   222\n",
      "batch_idx:     0, train_loss: 8.041328\n",
      "batch_idx:  1000, train_loss: 1.476996\n",
      "batch_idx:  2000, train_loss: 1.423603\n",
      "batch_idx:  3000, train_loss: 1.526651\n",
      "batch_idx:  4000, train_loss: 1.629918\n",
      "batch_idx:  5000, train_loss: 1.500806\n",
      "batch_idx:  6000, train_loss: 1.410431\n",
      "final train_loss: 1.351904\n",
      "batch_idx:     0, valid_loss: 8.631542\n",
      "batch_idx:  1000, valid_loss: 14.979404\n",
      "batch_idx:  2000, valid_loss: 15.959240\n",
      "batch_idx:  3000, valid_loss: 16.228148\n",
      "batch_idx:  4000, valid_loss: 16.392845\n",
      "batch_idx:  5000, valid_loss: 16.331894\n",
      "batch_idx:  6000, valid_loss: 15.839183\n",
      "Training Loss: 1.351904 \tValidation Loss: 15.176206\n",
      "epoch:   223\n",
      "batch_idx:     0, train_loss: 5.273800\n",
      "batch_idx:  1000, train_loss: 0.795801\n",
      "batch_idx:  2000, train_loss: 0.884412\n",
      "batch_idx:  3000, train_loss: 1.166122\n",
      "batch_idx:  4000, train_loss: 1.370660\n",
      "batch_idx:  5000, train_loss: 1.493879\n",
      "batch_idx:  6000, train_loss: 1.619485\n",
      "final train_loss: 1.577846\n",
      "batch_idx:     0, valid_loss: 7.114614\n",
      "batch_idx:  1000, valid_loss: 10.548742\n",
      "batch_idx:  2000, valid_loss: 10.652315\n",
      "batch_idx:  3000, valid_loss: 10.635141\n",
      "batch_idx:  4000, valid_loss: 10.617170\n",
      "batch_idx:  5000, valid_loss: 10.519094\n",
      "batch_idx:  6000, valid_loss: 10.255663\n",
      "Training Loss: 1.577846 \tValidation Loss: 9.907070\n",
      "epoch:   224\n",
      "batch_idx:     0, train_loss: 9.155978\n",
      "batch_idx:  1000, train_loss: 1.662704\n",
      "batch_idx:  2000, train_loss: 1.410437\n",
      "batch_idx:  3000, train_loss: 1.448874\n",
      "batch_idx:  4000, train_loss: 1.307582\n",
      "batch_idx:  5000, train_loss: 1.253468\n",
      "batch_idx:  6000, train_loss: 1.344258\n",
      "final train_loss: 1.300438\n",
      "batch_idx:     0, valid_loss: 12.829132\n",
      "batch_idx:  1000, valid_loss: 18.891640\n",
      "batch_idx:  2000, valid_loss: 20.120298\n",
      "batch_idx:  3000, valid_loss: 20.293217\n",
      "batch_idx:  4000, valid_loss: 20.202761\n",
      "batch_idx:  5000, valid_loss: 20.037703\n",
      "batch_idx:  6000, valid_loss: 19.534744\n",
      "Training Loss: 1.300438 \tValidation Loss: 18.830008\n",
      "epoch:   225\n",
      "batch_idx:     0, train_loss: 16.333073\n",
      "batch_idx:  1000, train_loss: 1.216821\n",
      "batch_idx:  2000, train_loss: 1.017810\n",
      "batch_idx:  3000, train_loss: 1.113505\n",
      "batch_idx:  4000, train_loss: 1.171624\n",
      "batch_idx:  5000, train_loss: 1.162036\n",
      "batch_idx:  6000, train_loss: 1.202151\n",
      "final train_loss: 1.178364\n",
      "batch_idx:     0, valid_loss: 10.208168\n",
      "batch_idx:  1000, valid_loss: 15.945542\n",
      "batch_idx:  2000, valid_loss: 16.829716\n",
      "batch_idx:  3000, valid_loss: 16.787462\n",
      "batch_idx:  4000, valid_loss: 16.882414\n",
      "batch_idx:  5000, valid_loss: 16.936802\n",
      "batch_idx:  6000, valid_loss: 16.769676\n",
      "Training Loss: 1.178364 \tValidation Loss: 16.078360\n",
      "epoch:   226\n",
      "batch_idx:     0, train_loss: 13.364850\n",
      "batch_idx:  1000, train_loss: 0.759460\n",
      "batch_idx:  2000, train_loss: 0.869978\n",
      "batch_idx:  3000, train_loss: 1.022004\n",
      "batch_idx:  4000, train_loss: 1.083432\n",
      "batch_idx:  5000, train_loss: 1.157514\n",
      "batch_idx:  6000, train_loss: 1.161262\n",
      "final train_loss: 1.158813\n",
      "batch_idx:     0, valid_loss: 8.374531\n",
      "batch_idx:  1000, valid_loss: 10.949028\n",
      "batch_idx:  2000, valid_loss: 11.764109\n",
      "batch_idx:  3000, valid_loss: 11.931421\n",
      "batch_idx:  4000, valid_loss: 11.956331\n",
      "batch_idx:  5000, valid_loss: 11.730449\n",
      "batch_idx:  6000, valid_loss: 11.434900\n",
      "Training Loss: 1.158813 \tValidation Loss: 11.038671\n",
      "epoch:   227\n",
      "batch_idx:     0, train_loss: 10.053792\n",
      "batch_idx:  1000, train_loss: 1.110717\n",
      "batch_idx:  2000, train_loss: 1.321663\n",
      "batch_idx:  3000, train_loss: 1.729542\n",
      "batch_idx:  4000, train_loss: 1.738180\n",
      "batch_idx:  5000, train_loss: 1.806148\n",
      "batch_idx:  6000, train_loss: 1.829107\n",
      "final train_loss: 1.828100\n",
      "batch_idx:     0, valid_loss: 8.885962\n",
      "batch_idx:  1000, valid_loss: 9.518481\n",
      "batch_idx:  2000, valid_loss: 9.459208\n",
      "batch_idx:  3000, valid_loss: 9.353754\n",
      "batch_idx:  4000, valid_loss: 9.371616\n",
      "batch_idx:  5000, valid_loss: 9.399665\n",
      "batch_idx:  6000, valid_loss: 9.212480\n",
      "Training Loss: 1.828100 \tValidation Loss: 8.847426\n",
      "epoch:   228\n",
      "batch_idx:     0, train_loss: 4.890448\n",
      "batch_idx:  1000, train_loss: 2.165147\n",
      "batch_idx:  2000, train_loss: 2.092722\n",
      "batch_idx:  3000, train_loss: 2.001730\n",
      "batch_idx:  4000, train_loss: 1.913607\n",
      "batch_idx:  5000, train_loss: 1.943395\n",
      "batch_idx:  6000, train_loss: 1.796872\n",
      "final train_loss: 1.700728\n",
      "batch_idx:     0, valid_loss: 7.470318\n",
      "batch_idx:  1000, valid_loss: 12.197937\n",
      "batch_idx:  2000, valid_loss: 13.164583\n",
      "batch_idx:  3000, valid_loss: 13.345520\n",
      "batch_idx:  4000, valid_loss: 13.516191\n",
      "batch_idx:  5000, valid_loss: 13.589541\n",
      "batch_idx:  6000, valid_loss: 13.194797\n",
      "Training Loss: 1.700728 \tValidation Loss: 12.764509\n",
      "epoch:   229\n",
      "batch_idx:     0, train_loss: 5.563326\n",
      "batch_idx:  1000, train_loss: 1.498252\n",
      "batch_idx:  2000, train_loss: 1.594614\n",
      "batch_idx:  3000, train_loss: 1.608899\n",
      "batch_idx:  4000, train_loss: 1.696474\n",
      "batch_idx:  5000, train_loss: 1.789665\n",
      "batch_idx:  6000, train_loss: 1.800922\n",
      "final train_loss: 1.816086\n",
      "batch_idx:     0, valid_loss: 7.947955\n",
      "batch_idx:  1000, valid_loss: 10.542940\n",
      "batch_idx:  2000, valid_loss: 10.634536\n",
      "batch_idx:  3000, valid_loss: 10.557588\n",
      "batch_idx:  4000, valid_loss: 10.574172\n",
      "batch_idx:  5000, valid_loss: 10.600315\n",
      "batch_idx:  6000, valid_loss: 10.460569\n",
      "Training Loss: 1.816086 \tValidation Loss: 10.062056\n",
      "epoch:   230\n",
      "batch_idx:     0, train_loss: 9.192292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  1000, train_loss: 1.725334\n",
      "batch_idx:  2000, train_loss: 1.738476\n",
      "batch_idx:  3000, train_loss: 1.761073\n",
      "batch_idx:  4000, train_loss: 1.772514\n",
      "batch_idx:  5000, train_loss: 1.778124\n",
      "batch_idx:  6000, train_loss: 1.848169\n",
      "final train_loss: 1.909740\n",
      "batch_idx:     0, valid_loss: 10.078556\n",
      "batch_idx:  1000, valid_loss: 10.085802\n",
      "batch_idx:  2000, valid_loss: 10.121454\n",
      "batch_idx:  3000, valid_loss: 10.110206\n",
      "batch_idx:  4000, valid_loss: 9.947692\n",
      "batch_idx:  5000, valid_loss: 9.729257\n",
      "batch_idx:  6000, valid_loss: 9.408532\n",
      "Training Loss: 1.909740 \tValidation Loss: 9.041711\n",
      "epoch:   231\n",
      "batch_idx:     0, train_loss: 9.761908\n",
      "batch_idx:  1000, train_loss: 2.137649\n",
      "batch_idx:  2000, train_loss: 2.065853\n",
      "batch_idx:  3000, train_loss: 1.927213\n",
      "batch_idx:  4000, train_loss: 2.117857\n",
      "batch_idx:  5000, train_loss: 2.120301\n",
      "batch_idx:  6000, train_loss: 2.097842\n",
      "final train_loss: 2.048335\n",
      "batch_idx:     0, valid_loss: 12.719818\n",
      "batch_idx:  1000, valid_loss: 16.175678\n",
      "batch_idx:  2000, valid_loss: 17.514523\n",
      "batch_idx:  3000, valid_loss: 17.806501\n",
      "batch_idx:  4000, valid_loss: 17.733213\n",
      "batch_idx:  5000, valid_loss: 17.521412\n",
      "batch_idx:  6000, valid_loss: 16.957249\n",
      "Training Loss: 2.048335 \tValidation Loss: 16.223555\n",
      "epoch:   232\n",
      "batch_idx:     0, train_loss: 16.263718\n",
      "batch_idx:  1000, train_loss: 1.733393\n",
      "batch_idx:  2000, train_loss: 2.006256\n",
      "batch_idx:  3000, train_loss: 2.258711\n",
      "batch_idx:  4000, train_loss: 2.410548\n",
      "batch_idx:  5000, train_loss: 2.259792\n",
      "batch_idx:  6000, train_loss: 2.205028\n",
      "final train_loss: 2.200109\n",
      "batch_idx:     0, valid_loss: 10.355979\n",
      "batch_idx:  1000, valid_loss: 10.173970\n",
      "batch_idx:  2000, valid_loss: 10.248704\n",
      "batch_idx:  3000, valid_loss: 10.167620\n",
      "batch_idx:  4000, valid_loss: 10.174166\n",
      "batch_idx:  5000, valid_loss: 10.102859\n",
      "batch_idx:  6000, valid_loss: 9.859356\n",
      "Training Loss: 2.200109 \tValidation Loss: 9.515476\n",
      "epoch:   233\n",
      "batch_idx:     0, train_loss: 13.819473\n",
      "batch_idx:  1000, train_loss: 1.755215\n",
      "batch_idx:  2000, train_loss: 1.940205\n",
      "batch_idx:  3000, train_loss: 2.072717\n",
      "batch_idx:  4000, train_loss: 2.077540\n",
      "batch_idx:  5000, train_loss: 2.007557\n",
      "batch_idx:  6000, train_loss: 2.011598\n",
      "final train_loss: 2.045601\n",
      "batch_idx:     0, valid_loss: 9.223528\n",
      "batch_idx:  1000, valid_loss: 10.215280\n",
      "batch_idx:  2000, valid_loss: 10.317644\n",
      "batch_idx:  3000, valid_loss: 10.295857\n",
      "batch_idx:  4000, valid_loss: 10.313907\n",
      "batch_idx:  5000, valid_loss: 10.269628\n",
      "batch_idx:  6000, valid_loss: 9.977177\n",
      "Training Loss: 2.045601 \tValidation Loss: 9.610303\n",
      "epoch:   234\n",
      "batch_idx:     0, train_loss: 11.754788\n",
      "batch_idx:  1000, train_loss: 1.972907\n",
      "batch_idx:  2000, train_loss: 2.104600\n",
      "batch_idx:  3000, train_loss: 1.890546\n",
      "batch_idx:  4000, train_loss: 1.878996\n",
      "batch_idx:  5000, train_loss: 1.949614\n",
      "batch_idx:  6000, train_loss: 1.872040\n",
      "final train_loss: 1.876517\n",
      "batch_idx:     0, valid_loss: 8.409823\n",
      "batch_idx:  1000, valid_loss: 9.261286\n",
      "batch_idx:  2000, valid_loss: 9.289530\n",
      "batch_idx:  3000, valid_loss: 9.260964\n",
      "batch_idx:  4000, valid_loss: 9.260414\n",
      "batch_idx:  5000, valid_loss: 9.272373\n",
      "batch_idx:  6000, valid_loss: 9.250403\n",
      "Training Loss: 1.876517 \tValidation Loss: 8.988338\n",
      "epoch:   235\n",
      "batch_idx:     0, train_loss: 11.561962\n",
      "batch_idx:  1000, train_loss: 1.749762\n",
      "batch_idx:  2000, train_loss: 1.626480\n",
      "batch_idx:  3000, train_loss: 1.961777\n",
      "batch_idx:  4000, train_loss: 2.017623\n",
      "batch_idx:  5000, train_loss: 1.827097\n",
      "batch_idx:  6000, train_loss: 1.745977\n",
      "final train_loss: 1.771550\n",
      "batch_idx:     0, valid_loss: 7.905033\n",
      "batch_idx:  1000, valid_loss: 8.005811\n",
      "batch_idx:  2000, valid_loss: 7.819116\n",
      "batch_idx:  3000, valid_loss: 7.622955\n",
      "batch_idx:  4000, valid_loss: 7.351487\n",
      "batch_idx:  5000, valid_loss: 7.273763\n",
      "batch_idx:  6000, valid_loss: 7.095942\n",
      "Training Loss: 1.771550 \tValidation Loss: 6.766341\n",
      "epoch:   236\n",
      "batch_idx:     0, train_loss: 8.987097\n",
      "batch_idx:  1000, train_loss: 1.698662\n",
      "batch_idx:  2000, train_loss: 1.793275\n",
      "batch_idx:  3000, train_loss: 1.872830\n",
      "batch_idx:  4000, train_loss: 1.947746\n",
      "batch_idx:  5000, train_loss: 1.823880\n",
      "batch_idx:  6000, train_loss: 1.899216\n",
      "final train_loss: 1.931823\n",
      "batch_idx:     0, valid_loss: 7.938039\n",
      "batch_idx:  1000, valid_loss: 8.735190\n",
      "batch_idx:  2000, valid_loss: 10.067983\n",
      "batch_idx:  3000, valid_loss: 10.367863\n",
      "batch_idx:  4000, valid_loss: 10.522020\n",
      "batch_idx:  5000, valid_loss: 10.577055\n",
      "batch_idx:  6000, valid_loss: 10.470274\n",
      "Training Loss: 1.931823 \tValidation Loss: 10.148081\n",
      "epoch:   237\n",
      "batch_idx:     0, train_loss: 10.483664\n",
      "batch_idx:  1000, train_loss: 1.565609\n",
      "batch_idx:  2000, train_loss: 1.710730\n",
      "batch_idx:  3000, train_loss: 1.734239\n",
      "batch_idx:  4000, train_loss: 1.759244\n",
      "batch_idx:  5000, train_loss: 1.635595\n",
      "batch_idx:  6000, train_loss: 1.679810\n",
      "final train_loss: 1.845031\n",
      "batch_idx:     0, valid_loss: 9.720003\n",
      "batch_idx:  1000, valid_loss: 11.103797\n",
      "batch_idx:  2000, valid_loss: 11.003942\n",
      "batch_idx:  3000, valid_loss: 10.793694\n",
      "batch_idx:  4000, valid_loss: 10.526355\n",
      "batch_idx:  5000, valid_loss: 10.145032\n",
      "batch_idx:  6000, valid_loss: 9.823134\n",
      "Training Loss: 1.845031 \tValidation Loss: 9.401178\n",
      "epoch:   238\n",
      "batch_idx:     0, train_loss: 13.130255\n",
      "batch_idx:  1000, train_loss: 1.367416\n",
      "batch_idx:  2000, train_loss: 1.427196\n",
      "batch_idx:  3000, train_loss: 1.471901\n",
      "batch_idx:  4000, train_loss: 1.670887\n",
      "batch_idx:  5000, train_loss: 1.685030\n",
      "batch_idx:  6000, train_loss: 1.731514\n",
      "final train_loss: 1.742961\n",
      "batch_idx:     0, valid_loss: 7.503029\n",
      "batch_idx:  1000, valid_loss: 11.579680\n",
      "batch_idx:  2000, valid_loss: 12.392476\n",
      "batch_idx:  3000, valid_loss: 12.475013\n",
      "batch_idx:  4000, valid_loss: 12.368885\n",
      "batch_idx:  5000, valid_loss: 12.067194\n",
      "batch_idx:  6000, valid_loss: 11.752710\n",
      "Training Loss: 1.742961 \tValidation Loss: 11.228169\n",
      "epoch:   239\n",
      "batch_idx:     0, train_loss: 10.035295\n",
      "batch_idx:  1000, train_loss: 2.145741\n",
      "batch_idx:  2000, train_loss: 1.841051\n",
      "batch_idx:  3000, train_loss: 1.908363\n",
      "batch_idx:  4000, train_loss: 1.935077\n",
      "batch_idx:  5000, train_loss: 1.992176\n",
      "batch_idx:  6000, train_loss: 2.042115\n",
      "final train_loss: 2.011224\n",
      "batch_idx:     0, valid_loss: 8.112277\n",
      "batch_idx:  1000, valid_loss: 10.973906\n",
      "batch_idx:  2000, valid_loss: 11.980698\n",
      "batch_idx:  3000, valid_loss: 12.128249\n",
      "batch_idx:  4000, valid_loss: 12.073723\n",
      "batch_idx:  5000, valid_loss: 11.867591\n",
      "batch_idx:  6000, valid_loss: 11.538012\n",
      "Training Loss: 2.011224 \tValidation Loss: 11.000798\n",
      "epoch:   240\n",
      "batch_idx:     0, train_loss: 10.714301\n",
      "batch_idx:  1000, train_loss: 1.423845\n",
      "batch_idx:  2000, train_loss: 1.292751\n",
      "batch_idx:  3000, train_loss: 1.268063\n",
      "batch_idx:  4000, train_loss: 1.378924\n",
      "batch_idx:  5000, train_loss: 1.543253\n",
      "batch_idx:  6000, train_loss: 1.567901\n",
      "final train_loss: 1.587784\n",
      "batch_idx:     0, valid_loss: 10.245239\n",
      "batch_idx:  1000, valid_loss: 10.819017\n",
      "batch_idx:  2000, valid_loss: 10.918563\n",
      "batch_idx:  3000, valid_loss: 10.907881\n",
      "batch_idx:  4000, valid_loss: 10.973255\n",
      "batch_idx:  5000, valid_loss: 10.740209\n",
      "batch_idx:  6000, valid_loss: 10.412868\n",
      "Training Loss: 1.587784 \tValidation Loss: 9.981351\n",
      "epoch:   241\n",
      "batch_idx:     0, train_loss: 11.228482\n",
      "batch_idx:  1000, train_loss: 1.681952\n",
      "batch_idx:  2000, train_loss: 1.437270\n",
      "batch_idx:  3000, train_loss: 1.518731\n",
      "batch_idx:  4000, train_loss: 1.648363\n",
      "batch_idx:  5000, train_loss: 1.686448\n",
      "batch_idx:  6000, train_loss: 1.814688\n",
      "final train_loss: 1.857999\n",
      "batch_idx:     0, valid_loss: 9.684135\n",
      "batch_idx:  1000, valid_loss: 10.271770\n",
      "batch_idx:  2000, valid_loss: 10.563320\n",
      "batch_idx:  3000, valid_loss: 10.583419\n",
      "batch_idx:  4000, valid_loss: 10.545428\n",
      "batch_idx:  5000, valid_loss: 10.398221\n",
      "batch_idx:  6000, valid_loss: 10.090061\n",
      "Training Loss: 1.857999 \tValidation Loss: 9.721852\n",
      "epoch:   242\n",
      "batch_idx:     0, train_loss: 4.755202\n",
      "batch_idx:  1000, train_loss: 1.612765\n",
      "batch_idx:  2000, train_loss: 1.545716\n",
      "batch_idx:  3000, train_loss: 1.775912\n",
      "batch_idx:  4000, train_loss: 1.971412\n",
      "batch_idx:  5000, train_loss: 2.135233\n",
      "batch_idx:  6000, train_loss: 2.185150\n",
      "final train_loss: 2.307760\n",
      "batch_idx:     0, valid_loss: 6.956706\n",
      "batch_idx:  1000, valid_loss: 7.945219\n",
      "batch_idx:  2000, valid_loss: 7.825780\n",
      "batch_idx:  3000, valid_loss: 7.641876\n",
      "batch_idx:  4000, valid_loss: 7.487864\n",
      "batch_idx:  5000, valid_loss: 7.330277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  6000, valid_loss: 7.067672\n",
      "Training Loss: 2.307760 \tValidation Loss: 6.742459\n",
      "epoch:   243\n",
      "batch_idx:     0, train_loss: 7.917422\n",
      "batch_idx:  1000, train_loss: 1.759457\n",
      "batch_idx:  2000, train_loss: 1.582314\n",
      "batch_idx:  3000, train_loss: 1.805919\n",
      "batch_idx:  4000, train_loss: 1.961713\n",
      "batch_idx:  5000, train_loss: 2.119333\n",
      "batch_idx:  6000, train_loss: 2.157446\n",
      "final train_loss: 2.141855\n",
      "batch_idx:     0, valid_loss: 7.300408\n",
      "batch_idx:  1000, valid_loss: 9.475616\n",
      "batch_idx:  2000, valid_loss: 10.033765\n",
      "batch_idx:  3000, valid_loss: 10.149453\n",
      "batch_idx:  4000, valid_loss: 10.205051\n",
      "batch_idx:  5000, valid_loss: 10.289742\n",
      "batch_idx:  6000, valid_loss: 10.234321\n",
      "Training Loss: 2.141855 \tValidation Loss: 9.853326\n",
      "epoch:   244\n",
      "batch_idx:     0, train_loss: 8.556833\n",
      "batch_idx:  1000, train_loss: 1.755307\n",
      "batch_idx:  2000, train_loss: 1.713088\n",
      "batch_idx:  3000, train_loss: 2.076357\n",
      "batch_idx:  4000, train_loss: 2.204919\n",
      "batch_idx:  5000, train_loss: 2.105804\n",
      "batch_idx:  6000, train_loss: 2.145504\n",
      "final train_loss: 2.109586\n",
      "batch_idx:     0, valid_loss: 7.151793\n",
      "batch_idx:  1000, valid_loss: 10.051457\n",
      "batch_idx:  2000, valid_loss: 10.901522\n",
      "batch_idx:  3000, valid_loss: 11.094297\n",
      "batch_idx:  4000, valid_loss: 10.873277\n",
      "batch_idx:  5000, valid_loss: 11.015401\n",
      "batch_idx:  6000, valid_loss: 10.899933\n",
      "Training Loss: 2.109586 \tValidation Loss: 10.465453\n",
      "epoch:   245\n",
      "batch_idx:     0, train_loss: 4.775440\n",
      "batch_idx:  1000, train_loss: 1.998823\n",
      "batch_idx:  2000, train_loss: 2.227590\n",
      "batch_idx:  3000, train_loss: 2.409860\n",
      "batch_idx:  4000, train_loss: 2.378885\n",
      "batch_idx:  5000, train_loss: 2.306996\n",
      "batch_idx:  6000, train_loss: 2.234207\n",
      "final train_loss: 2.162220\n",
      "batch_idx:     0, valid_loss: 4.904098\n",
      "batch_idx:  1000, valid_loss: 14.493565\n",
      "batch_idx:  2000, valid_loss: 16.052572\n",
      "batch_idx:  3000, valid_loss: 16.366024\n",
      "batch_idx:  4000, valid_loss: 16.515545\n",
      "batch_idx:  5000, valid_loss: 16.469200\n",
      "batch_idx:  6000, valid_loss: 15.988090\n",
      "Training Loss: 2.162220 \tValidation Loss: 15.381841\n",
      "epoch:   246\n",
      "batch_idx:     0, train_loss: 6.551110\n",
      "batch_idx:  1000, train_loss: 0.426892\n",
      "batch_idx:  2000, train_loss: 0.462147\n",
      "batch_idx:  3000, train_loss: 0.552094\n",
      "batch_idx:  4000, train_loss: 0.878814\n",
      "batch_idx:  5000, train_loss: 1.091071\n",
      "batch_idx:  6000, train_loss: 1.245600\n",
      "final train_loss: 1.308328\n",
      "batch_idx:     0, valid_loss: 6.935957\n",
      "batch_idx:  1000, valid_loss: 11.956260\n",
      "batch_idx:  2000, valid_loss: 12.165970\n",
      "batch_idx:  3000, valid_loss: 12.078190\n",
      "batch_idx:  4000, valid_loss: 11.988818\n",
      "batch_idx:  5000, valid_loss: 11.880956\n",
      "batch_idx:  6000, valid_loss: 11.573090\n",
      "Training Loss: 1.308328 \tValidation Loss: 11.021346\n",
      "epoch:   247\n",
      "batch_idx:     0, train_loss: 8.905048\n",
      "batch_idx:  1000, train_loss: 1.724369\n",
      "batch_idx:  2000, train_loss: 1.510259\n",
      "batch_idx:  3000, train_loss: 1.714890\n",
      "batch_idx:  4000, train_loss: 1.850412\n",
      "batch_idx:  5000, train_loss: 1.896319\n",
      "batch_idx:  6000, train_loss: 1.961723\n",
      "final train_loss: 2.042114\n",
      "batch_idx:     0, valid_loss: 8.664492\n",
      "batch_idx:  1000, valid_loss: 10.807592\n",
      "batch_idx:  2000, valid_loss: 11.045605\n",
      "batch_idx:  3000, valid_loss: 11.091168\n",
      "batch_idx:  4000, valid_loss: 10.904792\n",
      "batch_idx:  5000, valid_loss: 10.648196\n",
      "batch_idx:  6000, valid_loss: 10.247636\n",
      "Training Loss: 2.042114 \tValidation Loss: 9.821342\n",
      "epoch:   248\n",
      "batch_idx:     0, train_loss: 10.429349\n",
      "batch_idx:  1000, train_loss: 2.092060\n",
      "batch_idx:  2000, train_loss: 2.353032\n",
      "batch_idx:  3000, train_loss: 2.144161\n",
      "batch_idx:  4000, train_loss: 2.185579\n",
      "batch_idx:  5000, train_loss: 2.089332\n",
      "batch_idx:  6000, train_loss: 2.067495\n",
      "final train_loss: 2.064548\n",
      "batch_idx:     0, valid_loss: 8.537796\n",
      "batch_idx:  1000, valid_loss: 11.189506\n",
      "batch_idx:  2000, valid_loss: 11.998154\n",
      "batch_idx:  3000, valid_loss: 12.162381\n",
      "batch_idx:  4000, valid_loss: 12.128288\n",
      "batch_idx:  5000, valid_loss: 11.777138\n",
      "batch_idx:  6000, valid_loss: 11.243610\n",
      "Training Loss: 2.064548 \tValidation Loss: 10.793692\n",
      "epoch:   249\n",
      "batch_idx:     0, train_loss: 11.669960\n",
      "batch_idx:  1000, train_loss: 2.296674\n",
      "batch_idx:  2000, train_loss: 1.743708\n",
      "batch_idx:  3000, train_loss: 1.877023\n",
      "batch_idx:  4000, train_loss: 1.981152\n",
      "batch_idx:  5000, train_loss: 1.910177\n",
      "batch_idx:  6000, train_loss: 1.954144\n",
      "final train_loss: 2.029523\n",
      "batch_idx:     0, valid_loss: 10.180386\n",
      "batch_idx:  1000, valid_loss: 11.590962\n",
      "batch_idx:  2000, valid_loss: 11.607271\n",
      "batch_idx:  3000, valid_loss: 11.357665\n",
      "batch_idx:  4000, valid_loss: 11.175584\n",
      "batch_idx:  5000, valid_loss: 10.904215\n",
      "batch_idx:  6000, valid_loss: 10.498793\n",
      "Training Loss: 2.029523 \tValidation Loss: 10.028210\n",
      "epoch:   250\n",
      "batch_idx:     0, train_loss: 11.927244\n",
      "batch_idx:  1000, train_loss: 2.510134\n",
      "batch_idx:  2000, train_loss: 2.020515\n",
      "batch_idx:  3000, train_loss: 2.004911\n",
      "batch_idx:  4000, train_loss: 1.929650\n",
      "batch_idx:  5000, train_loss: 2.006588\n",
      "batch_idx:  6000, train_loss: 2.120387\n",
      "final train_loss: 2.233267\n",
      "batch_idx:     0, valid_loss: 9.757802\n",
      "batch_idx:  1000, valid_loss: 10.296527\n",
      "batch_idx:  2000, valid_loss: 10.053952\n",
      "batch_idx:  3000, valid_loss: 9.896502\n",
      "batch_idx:  4000, valid_loss: 9.738214\n",
      "batch_idx:  5000, valid_loss: 9.504502\n",
      "batch_idx:  6000, valid_loss: 9.177685\n",
      "Training Loss: 2.233267 \tValidation Loss: 8.790055\n",
      "epoch:   251\n",
      "batch_idx:     0, train_loss: 11.647168\n",
      "batch_idx:  1000, train_loss: 2.448839\n",
      "batch_idx:  2000, train_loss: 2.029423\n",
      "batch_idx:  3000, train_loss: 1.989283\n",
      "batch_idx:  4000, train_loss: 2.064229\n",
      "batch_idx:  5000, train_loss: 2.152438\n",
      "batch_idx:  6000, train_loss: 2.294163\n",
      "final train_loss: 2.257874\n",
      "batch_idx:     0, valid_loss: 12.819647\n",
      "batch_idx:  1000, valid_loss: 13.227741\n",
      "batch_idx:  2000, valid_loss: 16.574053\n",
      "batch_idx:  3000, valid_loss: 17.644415\n",
      "batch_idx:  4000, valid_loss: 18.095692\n",
      "batch_idx:  5000, valid_loss: 18.098846\n",
      "batch_idx:  6000, valid_loss: 17.795158\n",
      "Training Loss: 2.257874 \tValidation Loss: 17.085558\n",
      "epoch:   252\n",
      "batch_idx:     0, train_loss: 8.370934\n",
      "batch_idx:  1000, train_loss: 1.226772\n",
      "batch_idx:  2000, train_loss: 1.557127\n",
      "batch_idx:  3000, train_loss: 1.858906\n",
      "batch_idx:  4000, train_loss: 1.988506\n",
      "batch_idx:  5000, train_loss: 1.920450\n",
      "batch_idx:  6000, train_loss: 1.939736\n",
      "final train_loss: 2.063426\n",
      "batch_idx:     0, valid_loss: 12.088028\n",
      "batch_idx:  1000, valid_loss: 11.800591\n",
      "batch_idx:  2000, valid_loss: 11.419334\n",
      "batch_idx:  3000, valid_loss: 11.192875\n",
      "batch_idx:  4000, valid_loss: 11.043420\n",
      "batch_idx:  5000, valid_loss: 10.811056\n",
      "batch_idx:  6000, valid_loss: 10.393501\n",
      "Training Loss: 2.063426 \tValidation Loss: 9.928122\n",
      "epoch:   253\n",
      "batch_idx:     0, train_loss: 13.904105\n",
      "batch_idx:  1000, train_loss: 2.286150\n",
      "batch_idx:  2000, train_loss: 1.714752\n",
      "batch_idx:  3000, train_loss: 1.742574\n",
      "batch_idx:  4000, train_loss: 1.883796\n",
      "batch_idx:  5000, train_loss: 1.822160\n",
      "batch_idx:  6000, train_loss: 1.828503\n",
      "final train_loss: 1.866084\n",
      "batch_idx:     0, valid_loss: 9.991690\n",
      "batch_idx:  1000, valid_loss: 10.593811\n",
      "batch_idx:  2000, valid_loss: 10.738602\n",
      "batch_idx:  3000, valid_loss: 10.707883\n",
      "batch_idx:  4000, valid_loss: 10.729187\n",
      "batch_idx:  5000, valid_loss: 10.572836\n",
      "batch_idx:  6000, valid_loss: 10.265652\n",
      "Training Loss: 1.866084 \tValidation Loss: 9.874890\n",
      "epoch:   254\n",
      "batch_idx:     0, train_loss: 13.112570\n",
      "batch_idx:  1000, train_loss: 2.007220\n",
      "batch_idx:  2000, train_loss: 2.122444\n",
      "batch_idx:  3000, train_loss: 2.178152\n",
      "batch_idx:  4000, train_loss: 1.930998\n",
      "batch_idx:  5000, train_loss: 2.022928\n",
      "batch_idx:  6000, train_loss: 2.284438\n",
      "final train_loss: 2.232693\n",
      "batch_idx:     0, valid_loss: 7.551905\n",
      "batch_idx:  1000, valid_loss: 11.553727\n",
      "batch_idx:  2000, valid_loss: 12.340286\n",
      "batch_idx:  3000, valid_loss: 12.451514\n",
      "batch_idx:  4000, valid_loss: 12.509976\n",
      "batch_idx:  5000, valid_loss: 12.487514\n",
      "batch_idx:  6000, valid_loss: 12.177894\n",
      "Training Loss: 2.232693 \tValidation Loss: 11.593398\n",
      "epoch:   255\n",
      "batch_idx:     0, train_loss: 4.895859\n",
      "batch_idx:  1000, train_loss: 2.034581\n",
      "batch_idx:  2000, train_loss: 2.345656\n",
      "batch_idx:  3000, train_loss: 2.288946\n",
      "batch_idx:  4000, train_loss: 2.537111\n",
      "batch_idx:  5000, train_loss: 2.675516\n",
      "batch_idx:  6000, train_loss: 2.544103\n",
      "final train_loss: 2.588430\n",
      "batch_idx:     0, valid_loss: 10.298283\n",
      "batch_idx:  1000, valid_loss: 10.150414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  2000, valid_loss: 9.895967\n",
      "batch_idx:  3000, valid_loss: 9.665642\n",
      "batch_idx:  4000, valid_loss: 9.479105\n",
      "batch_idx:  5000, valid_loss: 9.209086\n",
      "batch_idx:  6000, valid_loss: 8.878664\n",
      "Training Loss: 2.588430 \tValidation Loss: 8.471962\n",
      "epoch:   256\n",
      "batch_idx:     0, train_loss: 13.376504\n",
      "batch_idx:  1000, train_loss: 2.912257\n",
      "batch_idx:  2000, train_loss: 2.830859\n",
      "batch_idx:  3000, train_loss: 2.575466\n",
      "batch_idx:  4000, train_loss: 2.546587\n",
      "batch_idx:  5000, train_loss: 2.614544\n",
      "batch_idx:  6000, train_loss: 2.730544\n",
      "final train_loss: 2.791318\n",
      "batch_idx:     0, valid_loss: 10.400884\n",
      "batch_idx:  1000, valid_loss: 10.410488\n",
      "batch_idx:  2000, valid_loss: 10.203609\n",
      "batch_idx:  3000, valid_loss: 10.036499\n",
      "batch_idx:  4000, valid_loss: 9.859917\n",
      "batch_idx:  5000, valid_loss: 9.595076\n",
      "batch_idx:  6000, valid_loss: 9.239548\n",
      "Training Loss: 2.791318 \tValidation Loss: 8.828851\n",
      "epoch:   257\n",
      "batch_idx:     0, train_loss: 10.501107\n",
      "batch_idx:  1000, train_loss: 1.940660\n",
      "batch_idx:  2000, train_loss: 2.138193\n",
      "batch_idx:  3000, train_loss: 2.209536\n",
      "batch_idx:  4000, train_loss: 2.092287\n",
      "batch_idx:  5000, train_loss: 2.222488\n",
      "batch_idx:  6000, train_loss: 2.629057\n",
      "final train_loss: 2.886605\n",
      "batch_idx:     0, valid_loss: 4.872586\n",
      "batch_idx:  1000, valid_loss: 4.870216\n",
      "batch_idx:  2000, valid_loss: 4.971330\n",
      "batch_idx:  3000, valid_loss: 4.940421\n",
      "batch_idx:  4000, valid_loss: 4.929211\n",
      "batch_idx:  5000, valid_loss: 4.911546\n",
      "batch_idx:  6000, valid_loss: 4.881384\n",
      "Training Loss: 2.886605 \tValidation Loss: 4.891576\n",
      "Training loss decreased (5.402438 --> 4.891576).  Saving model ...\n",
      "epoch:   258\n",
      "batch_idx:     0, train_loss: 4.872586\n",
      "batch_idx:  1000, train_loss: 2.637530\n",
      "batch_idx:  2000, train_loss: 2.662680\n",
      "batch_idx:  3000, train_loss: 2.384414\n",
      "batch_idx:  4000, train_loss: 2.175771\n",
      "batch_idx:  5000, train_loss: 2.142131\n",
      "batch_idx:  6000, train_loss: 2.168231\n",
      "final train_loss: 2.293649\n",
      "batch_idx:     0, valid_loss: 9.873260\n",
      "batch_idx:  1000, valid_loss: 9.935876\n",
      "batch_idx:  2000, valid_loss: 9.712095\n",
      "batch_idx:  3000, valid_loss: 9.535482\n",
      "batch_idx:  4000, valid_loss: 9.369389\n",
      "batch_idx:  5000, valid_loss: 9.126722\n",
      "batch_idx:  6000, valid_loss: 8.787272\n",
      "Training Loss: 2.293649 \tValidation Loss: 8.389590\n",
      "epoch:   259\n",
      "batch_idx:     0, train_loss: 4.527155\n",
      "batch_idx:  1000, train_loss: 2.997926\n",
      "batch_idx:  2000, train_loss: 2.410074\n",
      "batch_idx:  3000, train_loss: 2.139208\n",
      "batch_idx:  4000, train_loss: 2.217063\n",
      "batch_idx:  5000, train_loss: 2.323856\n",
      "batch_idx:  6000, train_loss: 2.355691\n",
      "final train_loss: 2.442022\n",
      "batch_idx:     0, valid_loss: 11.042521\n",
      "batch_idx:  1000, valid_loss: 11.178608\n",
      "batch_idx:  2000, valid_loss: 11.252427\n",
      "batch_idx:  3000, valid_loss: 11.188162\n",
      "batch_idx:  4000, valid_loss: 10.879399\n",
      "batch_idx:  5000, valid_loss: 10.580347\n",
      "batch_idx:  6000, valid_loss: 10.221601\n",
      "Training Loss: 2.442022 \tValidation Loss: 9.782368\n",
      "epoch:   260\n",
      "batch_idx:     0, train_loss: 13.503642\n",
      "batch_idx:  1000, train_loss: 1.902425\n",
      "batch_idx:  2000, train_loss: 1.834394\n",
      "batch_idx:  3000, train_loss: 1.880369\n",
      "batch_idx:  4000, train_loss: 1.989093\n",
      "batch_idx:  5000, train_loss: 2.097776\n",
      "batch_idx:  6000, train_loss: 2.307573\n",
      "final train_loss: 2.266704\n",
      "batch_idx:     0, valid_loss: 6.922999\n",
      "batch_idx:  1000, valid_loss: 10.929236\n",
      "batch_idx:  2000, valid_loss: 11.915748\n",
      "batch_idx:  3000, valid_loss: 12.120411\n",
      "batch_idx:  4000, valid_loss: 11.980703\n",
      "batch_idx:  5000, valid_loss: 11.786057\n",
      "batch_idx:  6000, valid_loss: 11.536338\n",
      "Training Loss: 2.266704 \tValidation Loss: 11.013145\n",
      "epoch:   261\n",
      "batch_idx:     0, train_loss: 9.603649\n",
      "batch_idx:  1000, train_loss: 1.985997\n",
      "batch_idx:  2000, train_loss: 2.118943\n",
      "batch_idx:  3000, train_loss: 2.203420\n",
      "batch_idx:  4000, train_loss: 2.253240\n",
      "batch_idx:  5000, train_loss: 2.460675\n",
      "batch_idx:  6000, train_loss: 2.639731\n",
      "final train_loss: 2.753504\n",
      "batch_idx:     0, valid_loss: 7.966432\n",
      "batch_idx:  1000, valid_loss: 8.188206\n",
      "batch_idx:  2000, valid_loss: 7.998239\n",
      "batch_idx:  3000, valid_loss: 7.852026\n",
      "batch_idx:  4000, valid_loss: 7.697515\n",
      "batch_idx:  5000, valid_loss: 7.506697\n",
      "batch_idx:  6000, valid_loss: 7.232021\n",
      "Training Loss: 2.753504 \tValidation Loss: 6.895857\n",
      "epoch:   262\n",
      "batch_idx:     0, train_loss: 9.879915\n",
      "batch_idx:  1000, train_loss: 2.703366\n",
      "batch_idx:  2000, train_loss: 2.516681\n",
      "batch_idx:  3000, train_loss: 2.503557\n",
      "batch_idx:  4000, train_loss: 2.578990\n",
      "batch_idx:  5000, train_loss: 2.682666\n",
      "batch_idx:  6000, train_loss: 2.857716\n",
      "final train_loss: 2.709662\n",
      "batch_idx:     0, valid_loss: 6.668590\n",
      "batch_idx:  1000, valid_loss: 9.361526\n",
      "batch_idx:  2000, valid_loss: 10.987856\n",
      "batch_idx:  3000, valid_loss: 12.081818\n",
      "batch_idx:  4000, valid_loss: 12.619984\n",
      "batch_idx:  5000, valid_loss: 12.915136\n",
      "batch_idx:  6000, valid_loss: 12.613245\n",
      "Training Loss: 2.709662 \tValidation Loss: 11.908586\n",
      "epoch:   263\n",
      "batch_idx:     0, train_loss: 6.952056\n",
      "batch_idx:  1000, train_loss: 1.174124\n",
      "batch_idx:  2000, train_loss: 0.823879\n",
      "batch_idx:  3000, train_loss: 1.016597\n",
      "batch_idx:  4000, train_loss: 1.327077\n",
      "batch_idx:  5000, train_loss: 1.480631\n",
      "batch_idx:  6000, train_loss: 1.527454\n",
      "final train_loss: 1.561195\n",
      "batch_idx:     0, valid_loss: 9.147163\n",
      "batch_idx:  1000, valid_loss: 8.930120\n",
      "batch_idx:  2000, valid_loss: 11.582551\n",
      "batch_idx:  3000, valid_loss: 12.379194\n",
      "batch_idx:  4000, valid_loss: 12.808214\n",
      "batch_idx:  5000, valid_loss: 13.011478\n",
      "batch_idx:  6000, valid_loss: 12.839148\n",
      "Training Loss: 1.561195 \tValidation Loss: 12.312726\n",
      "epoch:   264\n",
      "batch_idx:     0, train_loss: 11.784941\n",
      "batch_idx:  1000, train_loss: 0.899430\n",
      "batch_idx:  2000, train_loss: 0.782229\n",
      "batch_idx:  3000, train_loss: 0.814580\n",
      "batch_idx:  4000, train_loss: 0.990703\n",
      "batch_idx:  5000, train_loss: 1.169617\n",
      "batch_idx:  6000, train_loss: 1.294555\n",
      "final train_loss: 1.346285\n",
      "batch_idx:     0, valid_loss: 10.329844\n",
      "batch_idx:  1000, valid_loss: 9.794137\n",
      "batch_idx:  2000, valid_loss: 11.434483\n",
      "batch_idx:  3000, valid_loss: 12.927553\n",
      "batch_idx:  4000, valid_loss: 13.662676\n",
      "batch_idx:  5000, valid_loss: 14.063720\n",
      "batch_idx:  6000, valid_loss: 14.116933\n",
      "Training Loss: 1.346285 \tValidation Loss: 13.594207\n",
      "epoch:   265\n",
      "batch_idx:     0, train_loss: 9.722167\n",
      "batch_idx:  1000, train_loss: 0.875229\n",
      "batch_idx:  2000, train_loss: 0.827447\n",
      "batch_idx:  3000, train_loss: 1.070833\n",
      "batch_idx:  4000, train_loss: 1.354965\n",
      "batch_idx:  5000, train_loss: 1.555632\n",
      "batch_idx:  6000, train_loss: 1.556968\n",
      "final train_loss: 1.667273\n",
      "batch_idx:     0, valid_loss: 8.281865\n",
      "batch_idx:  1000, valid_loss: 9.340399\n",
      "batch_idx:  2000, valid_loss: 8.994473\n",
      "batch_idx:  3000, valid_loss: 8.792257\n",
      "batch_idx:  4000, valid_loss: 8.559509\n",
      "batch_idx:  5000, valid_loss: 8.253450\n",
      "batch_idx:  6000, valid_loss: 7.936170\n",
      "Training Loss: 1.667273 \tValidation Loss: 7.546908\n",
      "epoch:   266\n",
      "batch_idx:     0, train_loss: 7.320617\n",
      "batch_idx:  1000, train_loss: 1.905709\n",
      "batch_idx:  2000, train_loss: 2.049308\n",
      "batch_idx:  3000, train_loss: 2.009431\n",
      "batch_idx:  4000, train_loss: 1.957910\n",
      "batch_idx:  5000, train_loss: 1.956031\n",
      "batch_idx:  6000, train_loss: 2.026048\n",
      "final train_loss: 2.101589\n",
      "batch_idx:     0, valid_loss: 9.395156\n",
      "batch_idx:  1000, valid_loss: 11.119969\n",
      "batch_idx:  2000, valid_loss: 10.848407\n",
      "batch_idx:  3000, valid_loss: 10.888812\n",
      "batch_idx:  4000, valid_loss: 10.877073\n",
      "batch_idx:  5000, valid_loss: 10.655810\n",
      "batch_idx:  6000, valid_loss: 10.337789\n",
      "Training Loss: 2.101589 \tValidation Loss: 9.909443\n",
      "epoch:   267\n",
      "batch_idx:     0, train_loss: 12.045093\n",
      "batch_idx:  1000, train_loss: 2.358644\n",
      "batch_idx:  2000, train_loss: 1.977380\n",
      "batch_idx:  3000, train_loss: 1.903080\n",
      "batch_idx:  4000, train_loss: 1.726898\n",
      "batch_idx:  5000, train_loss: 1.675169\n",
      "batch_idx:  6000, train_loss: 1.751694\n",
      "final train_loss: 1.827403\n",
      "batch_idx:     0, valid_loss: 10.845728\n",
      "batch_idx:  1000, valid_loss: 10.581731\n",
      "batch_idx:  2000, valid_loss: 10.700030\n",
      "batch_idx:  3000, valid_loss: 10.678438\n",
      "batch_idx:  4000, valid_loss: 10.601371\n",
      "batch_idx:  5000, valid_loss: 10.393294\n",
      "batch_idx:  6000, valid_loss: 10.078791\n",
      "Training Loss: 1.827403 \tValidation Loss: 9.683246\n",
      "epoch:   268\n",
      "batch_idx:     0, train_loss: 13.723840\n",
      "batch_idx:  1000, train_loss: 1.923302\n",
      "batch_idx:  2000, train_loss: 1.578722\n",
      "batch_idx:  3000, train_loss: 1.389488\n",
      "batch_idx:  4000, train_loss: 1.403317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  5000, train_loss: 1.458672\n",
      "batch_idx:  6000, train_loss: 1.566349\n",
      "final train_loss: 1.681793\n",
      "batch_idx:     0, valid_loss: 9.619820\n",
      "batch_idx:  1000, valid_loss: 9.777710\n",
      "batch_idx:  2000, valid_loss: 9.976576\n",
      "batch_idx:  3000, valid_loss: 10.043115\n",
      "batch_idx:  4000, valid_loss: 10.054517\n",
      "batch_idx:  5000, valid_loss: 9.935047\n",
      "batch_idx:  6000, valid_loss: 9.639090\n",
      "Training Loss: 1.681793 \tValidation Loss: 9.243808\n",
      "epoch:   269\n",
      "batch_idx:     0, train_loss: 10.207262\n",
      "batch_idx:  1000, train_loss: 2.173703\n",
      "batch_idx:  2000, train_loss: 2.036208\n",
      "batch_idx:  3000, train_loss: 1.859953\n",
      "batch_idx:  4000, train_loss: 1.931521\n",
      "batch_idx:  5000, train_loss: 2.090958\n",
      "batch_idx:  6000, train_loss: 2.210688\n",
      "final train_loss: 2.401052\n",
      "batch_idx:     0, valid_loss: 4.716652\n",
      "batch_idx:  1000, valid_loss: 4.834722\n",
      "batch_idx:  2000, valid_loss: 5.020211\n",
      "batch_idx:  3000, valid_loss: 5.033766\n",
      "batch_idx:  4000, valid_loss: 5.013759\n",
      "batch_idx:  5000, valid_loss: 4.956158\n",
      "batch_idx:  6000, valid_loss: 4.917056\n",
      "Training Loss: 2.401052 \tValidation Loss: 4.910408\n",
      "epoch:   270\n",
      "batch_idx:     0, train_loss: 4.716652\n",
      "batch_idx:  1000, train_loss: 4.300060\n",
      "batch_idx:  2000, train_loss: 3.062667\n",
      "batch_idx:  3000, train_loss: 2.730217\n",
      "batch_idx:  4000, train_loss: 2.755050\n",
      "batch_idx:  5000, train_loss: 2.884695\n",
      "batch_idx:  6000, train_loss: 3.028735\n",
      "final train_loss: 3.066925\n",
      "batch_idx:     0, valid_loss: 4.528059\n",
      "batch_idx:  1000, valid_loss: 4.653952\n",
      "batch_idx:  2000, valid_loss: 4.974672\n",
      "batch_idx:  3000, valid_loss: 5.037188\n",
      "batch_idx:  4000, valid_loss: 5.030124\n",
      "batch_idx:  5000, valid_loss: 4.974436\n",
      "batch_idx:  6000, valid_loss: 4.930058\n",
      "Training Loss: 3.066925 \tValidation Loss: 4.924838\n",
      "epoch:   271\n",
      "batch_idx:     0, train_loss: 4.528059\n",
      "batch_idx:  1000, train_loss: 4.423907\n",
      "batch_idx:  2000, train_loss: 4.766313\n",
      "batch_idx:  3000, train_loss: 4.184914\n",
      "batch_idx:  4000, train_loss: 3.743753\n",
      "batch_idx:  5000, train_loss: 3.496880\n",
      "batch_idx:  6000, train_loss: 3.218092\n",
      "final train_loss: 3.134130\n",
      "batch_idx:     0, valid_loss: 4.480667\n",
      "batch_idx:  1000, valid_loss: 4.520191\n",
      "batch_idx:  2000, valid_loss: 4.751644\n",
      "batch_idx:  3000, valid_loss: 4.864900\n",
      "batch_idx:  4000, valid_loss: 4.919190\n",
      "batch_idx:  5000, valid_loss: 4.901947\n",
      "batch_idx:  6000, valid_loss: 4.897391\n",
      "Training Loss: 3.134130 \tValidation Loss: 4.901133\n",
      "epoch:   272\n",
      "batch_idx:     0, train_loss: 4.415162\n",
      "batch_idx:  1000, train_loss: 2.546914\n",
      "batch_idx:  2000, train_loss: 2.498439\n",
      "batch_idx:  3000, train_loss: 2.287542\n",
      "batch_idx:  4000, train_loss: 2.226358\n",
      "batch_idx:  5000, train_loss: 2.240897\n",
      "batch_idx:  6000, train_loss: 2.355862\n",
      "final train_loss: 2.347531\n",
      "batch_idx:     0, valid_loss: 6.602001\n",
      "batch_idx:  1000, valid_loss: 11.222893\n",
      "batch_idx:  2000, valid_loss: 11.743534\n",
      "batch_idx:  3000, valid_loss: 11.958514\n",
      "batch_idx:  4000, valid_loss: 11.970665\n",
      "batch_idx:  5000, valid_loss: 11.810476\n",
      "batch_idx:  6000, valid_loss: 11.570442\n",
      "Training Loss: 2.347531 \tValidation Loss: 11.124824\n",
      "epoch:   273\n",
      "batch_idx:     0, train_loss: 6.764902\n",
      "batch_idx:  1000, train_loss: 1.407258\n",
      "batch_idx:  2000, train_loss: 1.276525\n",
      "batch_idx:  3000, train_loss: 1.037259\n",
      "batch_idx:  4000, train_loss: 1.084383\n",
      "batch_idx:  5000, train_loss: 1.340101\n",
      "batch_idx:  6000, train_loss: 1.651532\n",
      "final train_loss: 1.719051\n",
      "batch_idx:     0, valid_loss: 8.690289\n",
      "batch_idx:  1000, valid_loss: 8.430003\n",
      "batch_idx:  2000, valid_loss: 10.601253\n",
      "batch_idx:  3000, valid_loss: 11.624814\n",
      "batch_idx:  4000, valid_loss: 12.080574\n",
      "batch_idx:  5000, valid_loss: 12.284300\n",
      "batch_idx:  6000, valid_loss: 11.930954\n",
      "Training Loss: 1.719051 \tValidation Loss: 11.326583\n",
      "epoch:   274\n",
      "batch_idx:     0, train_loss: 9.779473\n",
      "batch_idx:  1000, train_loss: 1.834677\n",
      "batch_idx:  2000, train_loss: 1.453024\n",
      "batch_idx:  3000, train_loss: 1.106544\n",
      "batch_idx:  4000, train_loss: 0.915994\n",
      "batch_idx:  5000, train_loss: 0.806489\n",
      "batch_idx:  6000, train_loss: 0.740239\n",
      "final train_loss: 0.717033\n",
      "batch_idx:     0, valid_loss: 6.467207\n",
      "batch_idx:  1000, valid_loss: 15.698989\n",
      "batch_idx:  2000, valid_loss: 22.194246\n",
      "batch_idx:  3000, valid_loss: 25.710035\n",
      "batch_idx:  4000, valid_loss: 27.329586\n",
      "batch_idx:  5000, valid_loss: 27.960299\n",
      "batch_idx:  6000, valid_loss: 27.420744\n",
      "Training Loss: 0.717033 \tValidation Loss: 25.933762\n",
      "epoch:   275\n",
      "batch_idx:     0, train_loss: 5.083941\n",
      "batch_idx:  1000, train_loss: 0.354084\n",
      "batch_idx:  2000, train_loss: 0.384405\n",
      "batch_idx:  3000, train_loss: 0.390825\n",
      "batch_idx:  4000, train_loss: 0.376858\n",
      "batch_idx:  5000, train_loss: 0.371600\n",
      "batch_idx:  6000, train_loss: 0.379352\n",
      "final train_loss: 0.394308\n",
      "batch_idx:     0, valid_loss: 4.682596\n",
      "batch_idx:  1000, valid_loss: 15.628307\n",
      "batch_idx:  2000, valid_loss: 21.895048\n",
      "batch_idx:  3000, valid_loss: 25.969389\n",
      "batch_idx:  4000, valid_loss: 27.999060\n",
      "batch_idx:  5000, valid_loss: 28.854517\n",
      "batch_idx:  6000, valid_loss: 28.320538\n",
      "Training Loss: 0.394308 \tValidation Loss: 26.753826\n",
      "epoch:   276\n",
      "batch_idx:     0, train_loss: 4.463522\n",
      "batch_idx:  1000, train_loss: 0.343749\n",
      "batch_idx:  2000, train_loss: 0.353741\n",
      "batch_idx:  3000, train_loss: 0.589516\n",
      "batch_idx:  4000, train_loss: 0.716388\n",
      "batch_idx:  5000, train_loss: 0.651329\n",
      "batch_idx:  6000, train_loss: 0.608653\n",
      "final train_loss: 0.599614\n",
      "batch_idx:     0, valid_loss: 5.754984\n",
      "batch_idx:  1000, valid_loss: 14.537793\n",
      "batch_idx:  2000, valid_loss: 21.297594\n",
      "batch_idx:  3000, valid_loss: 24.730862\n",
      "batch_idx:  4000, valid_loss: 26.665470\n",
      "batch_idx:  5000, valid_loss: 27.576136\n",
      "batch_idx:  6000, valid_loss: 27.012741\n",
      "Training Loss: 0.599614 \tValidation Loss: 25.514065\n",
      "epoch:   277\n",
      "batch_idx:     0, train_loss: 5.278581\n",
      "batch_idx:  1000, train_loss: 0.333650\n",
      "batch_idx:  2000, train_loss: 0.347130\n",
      "batch_idx:  3000, train_loss: 0.353741\n",
      "batch_idx:  4000, train_loss: 0.411314\n",
      "batch_idx:  5000, train_loss: 0.449926\n",
      "batch_idx:  6000, train_loss: 0.460022\n",
      "final train_loss: 0.478406\n",
      "batch_idx:     0, valid_loss: 6.238365\n",
      "batch_idx:  1000, valid_loss: 15.203959\n",
      "batch_idx:  2000, valid_loss: 18.995735\n",
      "batch_idx:  3000, valid_loss: 20.732544\n",
      "batch_idx:  4000, valid_loss: 21.491259\n",
      "batch_idx:  5000, valid_loss: 21.568651\n",
      "batch_idx:  6000, valid_loss: 20.877930\n",
      "Training Loss: 0.478406 \tValidation Loss: 19.725185\n",
      "epoch:   278\n",
      "batch_idx:     0, train_loss: 6.438601\n",
      "batch_idx:  1000, train_loss: 0.478475\n",
      "batch_idx:  2000, train_loss: 0.508772\n",
      "batch_idx:  3000, train_loss: 0.527041\n",
      "batch_idx:  4000, train_loss: 0.597771\n",
      "batch_idx:  5000, train_loss: 0.659530\n",
      "batch_idx:  6000, train_loss: 0.795146\n",
      "final train_loss: 0.869934\n",
      "batch_idx:     0, valid_loss: 12.313360\n",
      "batch_idx:  1000, valid_loss: 16.281116\n",
      "batch_idx:  2000, valid_loss: 16.452391\n",
      "batch_idx:  3000, valid_loss: 16.527321\n",
      "batch_idx:  4000, valid_loss: 16.636581\n",
      "batch_idx:  5000, valid_loss: 16.616365\n",
      "batch_idx:  6000, valid_loss: 16.277405\n",
      "Training Loss: 0.869934 \tValidation Loss: 15.780337\n",
      "epoch:   279\n",
      "batch_idx:     0, train_loss: 6.556671\n",
      "batch_idx:  1000, train_loss: 1.500556\n",
      "batch_idx:  2000, train_loss: 1.594777\n",
      "batch_idx:  3000, train_loss: 1.437039\n",
      "batch_idx:  4000, train_loss: 1.423785\n",
      "batch_idx:  5000, train_loss: 1.389610\n",
      "batch_idx:  6000, train_loss: 1.382975\n",
      "final train_loss: 1.407767\n",
      "batch_idx:     0, valid_loss: 8.654991\n",
      "batch_idx:  1000, valid_loss: 9.167369\n",
      "batch_idx:  2000, valid_loss: 9.471731\n",
      "batch_idx:  3000, valid_loss: 9.544308\n",
      "batch_idx:  4000, valid_loss: 9.601021\n",
      "batch_idx:  5000, valid_loss: 9.623692\n",
      "batch_idx:  6000, valid_loss: 9.514032\n",
      "Training Loss: 1.407767 \tValidation Loss: 9.175543\n",
      "epoch:   280\n",
      "batch_idx:     0, train_loss: 11.830259\n",
      "batch_idx:  1000, train_loss: 1.640020\n",
      "batch_idx:  2000, train_loss: 1.839228\n",
      "batch_idx:  3000, train_loss: 1.760740\n",
      "batch_idx:  4000, train_loss: 1.732131\n",
      "batch_idx:  5000, train_loss: 1.738603\n",
      "batch_idx:  6000, train_loss: 1.627693\n",
      "final train_loss: 1.663259\n",
      "batch_idx:     0, valid_loss: 9.006247\n",
      "batch_idx:  1000, valid_loss: 9.097690\n",
      "batch_idx:  2000, valid_loss: 9.282855\n",
      "batch_idx:  3000, valid_loss: 9.360279\n",
      "batch_idx:  4000, valid_loss: 9.434731\n",
      "batch_idx:  5000, valid_loss: 9.466928\n",
      "batch_idx:  6000, valid_loss: 9.316611\n",
      "Training Loss: 1.663259 \tValidation Loss: 9.042060\n",
      "epoch:   281\n",
      "batch_idx:     0, train_loss: 11.035872\n",
      "batch_idx:  1000, train_loss: 1.659300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  2000, train_loss: 1.424445\n",
      "batch_idx:  3000, train_loss: 1.334747\n",
      "batch_idx:  4000, train_loss: 1.298312\n",
      "batch_idx:  5000, train_loss: 1.244344\n",
      "batch_idx:  6000, train_loss: 1.134979\n",
      "final train_loss: 1.098363\n",
      "batch_idx:     0, valid_loss: 10.443865\n",
      "batch_idx:  1000, valid_loss: 13.534789\n",
      "batch_idx:  2000, valid_loss: 15.051726\n",
      "batch_idx:  3000, valid_loss: 15.420016\n",
      "batch_idx:  4000, valid_loss: 15.542964\n",
      "batch_idx:  5000, valid_loss: 15.463400\n",
      "batch_idx:  6000, valid_loss: 15.054364\n",
      "Training Loss: 1.098363 \tValidation Loss: 14.474545\n",
      "epoch:   282\n",
      "batch_idx:     0, train_loss: 13.674026\n",
      "batch_idx:  1000, train_loss: 0.855370\n",
      "batch_idx:  2000, train_loss: 1.119089\n",
      "batch_idx:  3000, train_loss: 1.010851\n",
      "batch_idx:  4000, train_loss: 1.159295\n",
      "batch_idx:  5000, train_loss: 1.312964\n",
      "batch_idx:  6000, train_loss: 1.453711\n",
      "final train_loss: 1.445724\n",
      "batch_idx:     0, valid_loss: 14.463669\n",
      "batch_idx:  1000, valid_loss: 16.243698\n",
      "batch_idx:  2000, valid_loss: 17.795210\n",
      "batch_idx:  3000, valid_loss: 18.152168\n",
      "batch_idx:  4000, valid_loss: 18.162331\n",
      "batch_idx:  5000, valid_loss: 17.931051\n",
      "batch_idx:  6000, valid_loss: 17.156771\n",
      "Training Loss: 1.445724 \tValidation Loss: 16.285248\n",
      "epoch:   283\n",
      "batch_idx:     0, train_loss: 18.883471\n",
      "batch_idx:  1000, train_loss: 1.545142\n",
      "batch_idx:  2000, train_loss: 1.579671\n",
      "batch_idx:  3000, train_loss: 1.612843\n",
      "batch_idx:  4000, train_loss: 1.766154\n",
      "batch_idx:  5000, train_loss: 1.858453\n",
      "batch_idx:  6000, train_loss: 1.821484\n",
      "final train_loss: 1.792990\n",
      "batch_idx:     0, valid_loss: 8.723841\n",
      "batch_idx:  1000, valid_loss: 13.474857\n",
      "batch_idx:  2000, valid_loss: 13.901549\n",
      "batch_idx:  3000, valid_loss: 13.898652\n",
      "batch_idx:  4000, valid_loss: 13.767429\n",
      "batch_idx:  5000, valid_loss: 13.484274\n",
      "batch_idx:  6000, valid_loss: 12.858529\n",
      "Training Loss: 1.792990 \tValidation Loss: 12.196338\n",
      "epoch:   284\n",
      "batch_idx:     0, train_loss: 10.880845\n",
      "batch_idx:  1000, train_loss: 0.799884\n",
      "batch_idx:  2000, train_loss: 0.741893\n",
      "batch_idx:  3000, train_loss: 0.994469\n",
      "batch_idx:  4000, train_loss: 1.062463\n",
      "batch_idx:  5000, train_loss: 1.205891\n",
      "batch_idx:  6000, train_loss: 1.293020\n",
      "final train_loss: 1.365610\n",
      "batch_idx:     0, valid_loss: 6.990767\n",
      "batch_idx:  1000, valid_loss: 8.957058\n",
      "batch_idx:  2000, valid_loss: 9.864419\n",
      "batch_idx:  3000, valid_loss: 10.171576\n",
      "batch_idx:  4000, valid_loss: 10.362393\n",
      "batch_idx:  5000, valid_loss: 10.419808\n",
      "batch_idx:  6000, valid_loss: 10.239003\n",
      "Training Loss: 1.365610 \tValidation Loss: 9.880388\n",
      "epoch:   285\n",
      "batch_idx:     0, train_loss: 9.795572\n",
      "batch_idx:  1000, train_loss: 1.636153\n",
      "batch_idx:  2000, train_loss: 1.759171\n",
      "batch_idx:  3000, train_loss: 1.662113\n",
      "batch_idx:  4000, train_loss: 1.736770\n",
      "batch_idx:  5000, train_loss: 1.637192\n",
      "batch_idx:  6000, train_loss: 1.455652\n",
      "final train_loss: 1.418364\n",
      "batch_idx:     0, valid_loss: 7.586768\n",
      "batch_idx:  1000, valid_loss: 8.626667\n",
      "batch_idx:  2000, valid_loss: 9.481042\n",
      "batch_idx:  3000, valid_loss: 9.833803\n",
      "batch_idx:  4000, valid_loss: 9.863762\n",
      "batch_idx:  5000, valid_loss: 9.734118\n",
      "batch_idx:  6000, valid_loss: 9.586344\n",
      "Training Loss: 1.418364 \tValidation Loss: 9.173640\n",
      "epoch:   286\n",
      "batch_idx:     0, train_loss: 8.277403\n",
      "batch_idx:  1000, train_loss: 0.704443\n",
      "batch_idx:  2000, train_loss: 0.921877\n",
      "batch_idx:  3000, train_loss: 1.163569\n",
      "batch_idx:  4000, train_loss: 1.242772\n",
      "batch_idx:  5000, train_loss: 1.251408\n",
      "batch_idx:  6000, train_loss: 1.157943\n",
      "final train_loss: 1.125051\n",
      "batch_idx:     0, valid_loss: 9.368193\n",
      "batch_idx:  1000, valid_loss: 17.904694\n",
      "batch_idx:  2000, valid_loss: 19.386461\n",
      "batch_idx:  3000, valid_loss: 19.775814\n",
      "batch_idx:  4000, valid_loss: 19.883804\n",
      "batch_idx:  5000, valid_loss: 19.792274\n",
      "batch_idx:  6000, valid_loss: 19.163719\n",
      "Training Loss: 1.125051 \tValidation Loss: 18.258820\n",
      "epoch:   287\n",
      "batch_idx:     0, train_loss: 6.013712\n",
      "batch_idx:  1000, train_loss: 0.901314\n",
      "batch_idx:  2000, train_loss: 1.275158\n",
      "batch_idx:  3000, train_loss: 1.352613\n",
      "batch_idx:  4000, train_loss: 1.418597\n",
      "batch_idx:  5000, train_loss: 1.370169\n",
      "batch_idx:  6000, train_loss: 1.317462\n",
      "final train_loss: 1.415031\n",
      "batch_idx:     0, valid_loss: 8.054455\n",
      "batch_idx:  1000, valid_loss: 8.713600\n",
      "batch_idx:  2000, valid_loss: 10.018325\n",
      "batch_idx:  3000, valid_loss: 10.437469\n",
      "batch_idx:  4000, valid_loss: 10.666550\n",
      "batch_idx:  5000, valid_loss: 10.848987\n",
      "batch_idx:  6000, valid_loss: 10.833676\n",
      "Training Loss: 1.415031 \tValidation Loss: 10.502718\n",
      "epoch:   288\n",
      "batch_idx:     0, train_loss: 10.194807\n",
      "batch_idx:  1000, train_loss: 1.468958\n",
      "batch_idx:  2000, train_loss: 1.747663\n",
      "batch_idx:  3000, train_loss: 1.804975\n",
      "batch_idx:  4000, train_loss: 1.651733\n",
      "batch_idx:  5000, train_loss: 1.575777\n",
      "batch_idx:  6000, train_loss: 1.587250\n",
      "final train_loss: 1.662619\n",
      "batch_idx:     0, valid_loss: 7.759611\n",
      "batch_idx:  1000, valid_loss: 7.055037\n",
      "batch_idx:  2000, valid_loss: 8.918078\n",
      "batch_idx:  3000, valid_loss: 9.606315\n",
      "batch_idx:  4000, valid_loss: 9.951628\n",
      "batch_idx:  5000, valid_loss: 10.121215\n",
      "batch_idx:  6000, valid_loss: 10.088171\n",
      "Training Loss: 1.662619 \tValidation Loss: 9.834395\n",
      "epoch:   289\n",
      "batch_idx:     0, train_loss: 4.632731\n",
      "batch_idx:  1000, train_loss: 1.458016\n",
      "batch_idx:  2000, train_loss: 1.969337\n",
      "batch_idx:  3000, train_loss: 1.838906\n",
      "batch_idx:  4000, train_loss: 1.787699\n",
      "batch_idx:  5000, train_loss: 1.779356\n",
      "batch_idx:  6000, train_loss: 1.778379\n",
      "final train_loss: 1.773576\n",
      "batch_idx:     0, valid_loss: 5.881353\n",
      "batch_idx:  1000, valid_loss: 10.797899\n",
      "batch_idx:  2000, valid_loss: 11.209257\n",
      "batch_idx:  3000, valid_loss: 11.418634\n",
      "batch_idx:  4000, valid_loss: 11.312007\n",
      "batch_idx:  5000, valid_loss: 11.183087\n",
      "batch_idx:  6000, valid_loss: 10.973714\n",
      "Training Loss: 1.773576 \tValidation Loss: 10.575488\n",
      "epoch:   290\n",
      "batch_idx:     0, train_loss: 4.682839\n",
      "batch_idx:  1000, train_loss: 1.513402\n",
      "batch_idx:  2000, train_loss: 1.469795\n",
      "batch_idx:  3000, train_loss: 1.670771\n",
      "batch_idx:  4000, train_loss: 1.801624\n",
      "batch_idx:  5000, train_loss: 1.733833\n",
      "batch_idx:  6000, train_loss: 1.794607\n",
      "final train_loss: 1.908107\n",
      "batch_idx:     0, valid_loss: 8.554106\n",
      "batch_idx:  1000, valid_loss: 9.461447\n",
      "batch_idx:  2000, valid_loss: 9.554113\n",
      "batch_idx:  3000, valid_loss: 9.592380\n",
      "batch_idx:  4000, valid_loss: 9.475333\n",
      "batch_idx:  5000, valid_loss: 9.323102\n",
      "batch_idx:  6000, valid_loss: 9.027195\n",
      "Training Loss: 1.908107 \tValidation Loss: 8.649175\n",
      "epoch:   291\n",
      "batch_idx:     0, train_loss: 4.692270\n",
      "batch_idx:  1000, train_loss: 2.258591\n",
      "batch_idx:  2000, train_loss: 2.456402\n",
      "batch_idx:  3000, train_loss: 2.278703\n",
      "batch_idx:  4000, train_loss: 2.386888\n",
      "batch_idx:  5000, train_loss: 2.500275\n",
      "batch_idx:  6000, train_loss: 2.473598\n",
      "final train_loss: 2.521585\n",
      "batch_idx:     0, valid_loss: 9.760567\n",
      "batch_idx:  1000, valid_loss: 10.075750\n",
      "batch_idx:  2000, valid_loss: 10.127684\n",
      "batch_idx:  3000, valid_loss: 10.179976\n",
      "batch_idx:  4000, valid_loss: 9.977925\n",
      "batch_idx:  5000, valid_loss: 9.750929\n",
      "batch_idx:  6000, valid_loss: 9.398219\n",
      "Training Loss: 2.521585 \tValidation Loss: 9.006824\n",
      "epoch:   292\n",
      "batch_idx:     0, train_loss: 12.812575\n",
      "batch_idx:  1000, train_loss: 2.397933\n",
      "batch_idx:  2000, train_loss: 2.429962\n",
      "batch_idx:  3000, train_loss: 2.366330\n",
      "batch_idx:  4000, train_loss: 2.311198\n",
      "batch_idx:  5000, train_loss: 2.158446\n",
      "batch_idx:  6000, train_loss: 2.055841\n",
      "final train_loss: 2.258090\n",
      "batch_idx:     0, valid_loss: 4.610843\n",
      "batch_idx:  1000, valid_loss: 4.417642\n",
      "batch_idx:  2000, valid_loss: 4.844561\n",
      "batch_idx:  3000, valid_loss: 4.970088\n",
      "batch_idx:  4000, valid_loss: 5.008722\n",
      "batch_idx:  5000, valid_loss: 5.042172\n",
      "batch_idx:  6000, valid_loss: 5.069438\n",
      "Training Loss: 2.258090 \tValidation Loss: 5.029284\n",
      "epoch:   293\n",
      "batch_idx:     0, train_loss: 4.447289\n",
      "batch_idx:  1000, train_loss: 2.441523\n",
      "batch_idx:  2000, train_loss: 2.939854\n",
      "batch_idx:  3000, train_loss: 2.544406\n",
      "batch_idx:  4000, train_loss: 2.680549\n",
      "batch_idx:  5000, train_loss: 3.115166\n",
      "batch_idx:  6000, train_loss: 3.456289\n",
      "final train_loss: 3.613804\n",
      "batch_idx:     0, valid_loss: 4.933227\n",
      "batch_idx:  1000, valid_loss: 4.873358\n",
      "batch_idx:  2000, valid_loss: 4.832696\n",
      "batch_idx:  3000, valid_loss: 4.902784\n",
      "batch_idx:  4000, valid_loss: 4.898332\n",
      "batch_idx:  5000, valid_loss: 4.871187\n",
      "batch_idx:  6000, valid_loss: 4.892634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.613804 \tValidation Loss: 4.886505\n",
      "Training loss decreased (4.891576 --> 4.886505).  Saving model ...\n",
      "epoch:   294\n",
      "batch_idx:     0, train_loss: 4.933227\n",
      "batch_idx:  1000, train_loss: 4.622923\n",
      "batch_idx:  2000, train_loss: 4.639710\n",
      "batch_idx:  3000, train_loss: 4.739048\n",
      "batch_idx:  4000, train_loss: 4.773853\n",
      "batch_idx:  5000, train_loss: 4.789058\n",
      "batch_idx:  6000, train_loss: 4.842316\n",
      "final train_loss: 4.862693\n",
      "batch_idx:     0, valid_loss: 4.926648\n",
      "batch_idx:  1000, valid_loss: 4.899362\n",
      "batch_idx:  2000, valid_loss: 4.875026\n",
      "batch_idx:  3000, valid_loss: 4.899973\n",
      "batch_idx:  4000, valid_loss: 4.889737\n",
      "batch_idx:  5000, valid_loss: 4.864377\n",
      "batch_idx:  6000, valid_loss: 4.878922\n",
      "Training Loss: 4.862693 \tValidation Loss: 4.877790\n",
      "Training loss decreased (4.886505 --> 4.877790).  Saving model ...\n",
      "epoch:   295\n",
      "batch_idx:     0, train_loss: 4.926648\n",
      "batch_idx:  1000, train_loss: 4.646879\n",
      "batch_idx:  2000, train_loss: 3.213539\n",
      "batch_idx:  3000, train_loss: 2.782431\n",
      "batch_idx:  4000, train_loss: 2.545675\n",
      "batch_idx:  5000, train_loss: 2.488390\n",
      "batch_idx:  6000, train_loss: 2.374280\n",
      "final train_loss: 2.524102\n",
      "batch_idx:     0, valid_loss: 5.949829\n",
      "batch_idx:  1000, valid_loss: 6.092537\n",
      "batch_idx:  2000, valid_loss: 6.073083\n",
      "batch_idx:  3000, valid_loss: 6.075527\n",
      "batch_idx:  4000, valid_loss: 6.028435\n",
      "batch_idx:  5000, valid_loss: 5.878804\n",
      "batch_idx:  6000, valid_loss: 5.716319\n",
      "Training Loss: 2.524102 \tValidation Loss: 5.509991\n",
      "epoch:   296\n",
      "batch_idx:     0, train_loss: 5.911073\n",
      "batch_idx:  1000, train_loss: 2.563563\n",
      "batch_idx:  2000, train_loss: 2.554506\n",
      "batch_idx:  3000, train_loss: 2.479392\n",
      "batch_idx:  4000, train_loss: 2.352824\n",
      "batch_idx:  5000, train_loss: 2.320547\n",
      "batch_idx:  6000, train_loss: 2.370451\n",
      "final train_loss: 2.451595\n",
      "batch_idx:     0, valid_loss: 8.893932\n",
      "batch_idx:  1000, valid_loss: 9.784016\n",
      "batch_idx:  2000, valid_loss: 9.647882\n",
      "batch_idx:  3000, valid_loss: 9.381089\n",
      "batch_idx:  4000, valid_loss: 9.169345\n",
      "batch_idx:  5000, valid_loss: 8.952173\n",
      "batch_idx:  6000, valid_loss: 8.649838\n",
      "Training Loss: 2.451595 \tValidation Loss: 8.285312\n",
      "epoch:   297\n",
      "batch_idx:     0, train_loss: 4.533819\n",
      "batch_idx:  1000, train_loss: 1.937485\n",
      "batch_idx:  2000, train_loss: 1.521582\n",
      "batch_idx:  3000, train_loss: 1.817863\n",
      "batch_idx:  4000, train_loss: 1.860749\n",
      "batch_idx:  5000, train_loss: 1.821985\n",
      "batch_idx:  6000, train_loss: 1.982276\n",
      "final train_loss: 2.070018\n",
      "batch_idx:     0, valid_loss: 7.371893\n",
      "batch_idx:  1000, valid_loss: 7.317143\n",
      "batch_idx:  2000, valid_loss: 9.941433\n",
      "batch_idx:  3000, valid_loss: 10.697020\n",
      "batch_idx:  4000, valid_loss: 11.146988\n",
      "batch_idx:  5000, valid_loss: 11.434789\n",
      "batch_idx:  6000, valid_loss: 11.490253\n",
      "Training Loss: 2.070018 \tValidation Loss: 11.247177\n",
      "epoch:   298\n",
      "batch_idx:     0, train_loss: 7.905408\n",
      "batch_idx:  1000, train_loss: 1.562904\n",
      "batch_idx:  2000, train_loss: 1.827705\n",
      "batch_idx:  3000, train_loss: 1.860072\n",
      "batch_idx:  4000, train_loss: 1.721773\n",
      "batch_idx:  5000, train_loss: 1.760768\n",
      "batch_idx:  6000, train_loss: 1.910133\n",
      "final train_loss: 1.970803\n",
      "batch_idx:     0, valid_loss: 5.902172\n",
      "batch_idx:  1000, valid_loss: 5.922306\n",
      "batch_idx:  2000, valid_loss: 5.929290\n",
      "batch_idx:  3000, valid_loss: 5.768123\n",
      "batch_idx:  4000, valid_loss: 5.640571\n",
      "batch_idx:  5000, valid_loss: 5.576229\n",
      "batch_idx:  6000, valid_loss: 5.593187\n",
      "Training Loss: 1.970803 \tValidation Loss: 5.417494\n",
      "epoch:   299\n",
      "batch_idx:     0, train_loss: 6.269919\n",
      "batch_idx:  1000, train_loss: 1.855159\n",
      "batch_idx:  2000, train_loss: 1.747720\n",
      "batch_idx:  3000, train_loss: 2.196955\n",
      "batch_idx:  4000, train_loss: 1.997690\n",
      "batch_idx:  5000, train_loss: 2.041014\n",
      "batch_idx:  6000, train_loss: 2.084110\n",
      "final train_loss: 2.091973\n",
      "batch_idx:     0, valid_loss: 8.564005\n",
      "batch_idx:  1000, valid_loss: 18.784746\n",
      "batch_idx:  2000, valid_loss: 18.618368\n",
      "batch_idx:  3000, valid_loss: 18.191368\n",
      "batch_idx:  4000, valid_loss: 17.815193\n",
      "batch_idx:  5000, valid_loss: 17.282812\n",
      "batch_idx:  6000, valid_loss: 16.913555\n",
      "Training Loss: 2.091973 \tValidation Loss: 16.161453\n",
      "epoch:   300\n",
      "batch_idx:     0, train_loss: 4.432510\n",
      "batch_idx:  1000, train_loss: 4.018741\n",
      "batch_idx:  2000, train_loss: 4.424837\n",
      "batch_idx:  3000, train_loss: 4.512495\n",
      "batch_idx:  4000, train_loss: 4.638407\n",
      "batch_idx:  5000, train_loss: 4.700503\n",
      "batch_idx:  6000, train_loss: 4.761851\n",
      "final train_loss: 4.792418\n",
      "batch_idx:     0, valid_loss: 4.947694\n",
      "batch_idx:  1000, valid_loss: 4.801920\n",
      "batch_idx:  2000, valid_loss: 4.876514\n",
      "batch_idx:  3000, valid_loss: 4.841437\n",
      "batch_idx:  4000, valid_loss: 4.869782\n",
      "batch_idx:  5000, valid_loss: 4.864339\n",
      "batch_idx:  6000, valid_loss: 4.872777\n",
      "Training Loss: 4.792418 \tValidation Loss: 4.874193\n",
      "Training loss decreased (4.877790 --> 4.874193).  Saving model ...\n",
      "epoch:   301\n",
      "batch_idx:     0, train_loss: 4.947694\n",
      "batch_idx:  1000, train_loss: 4.557558\n",
      "batch_idx:  2000, train_loss: 4.677761\n",
      "batch_idx:  3000, train_loss: 4.688917\n",
      "batch_idx:  4000, train_loss: 4.752358\n",
      "batch_idx:  5000, train_loss: 4.784281\n",
      "batch_idx:  6000, train_loss: 4.828451\n",
      "final train_loss: 4.854305\n",
      "batch_idx:     0, valid_loss: 4.936625\n",
      "batch_idx:  1000, valid_loss: 4.863005\n",
      "batch_idx:  2000, valid_loss: 4.901765\n",
      "batch_idx:  3000, valid_loss: 4.865985\n",
      "batch_idx:  4000, valid_loss: 4.875639\n",
      "batch_idx:  5000, valid_loss: 4.863325\n",
      "batch_idx:  6000, valid_loss: 4.868984\n",
      "Training Loss: 4.854305 \tValidation Loss: 4.872738\n",
      "Training loss decreased (4.874193 --> 4.872738).  Saving model ...\n",
      "epoch:   302\n",
      "batch_idx:     0, train_loss: 4.936625\n",
      "batch_idx:  1000, train_loss: 4.613436\n",
      "batch_idx:  2000, train_loss: 4.698833\n",
      "batch_idx:  3000, train_loss: 4.708422\n",
      "batch_idx:  4000, train_loss: 4.756617\n",
      "batch_idx:  5000, train_loss: 4.783756\n",
      "batch_idx:  6000, train_loss: 4.826324\n",
      "final train_loss: 4.854026\n",
      "batch_idx:     0, valid_loss: 4.932591\n",
      "batch_idx:  1000, valid_loss: 4.895686\n",
      "batch_idx:  2000, valid_loss: 4.915899\n",
      "batch_idx:  3000, valid_loss: 4.880162\n",
      "batch_idx:  4000, valid_loss: 4.879066\n",
      "batch_idx:  5000, valid_loss: 4.862953\n",
      "batch_idx:  6000, valid_loss: 4.867123\n",
      "Training Loss: 4.854026 \tValidation Loss: 4.872631\n",
      "Training loss decreased (4.872738 --> 4.872631).  Saving model ...\n",
      "epoch:   303\n",
      "batch_idx:     0, train_loss: 4.932591\n",
      "batch_idx:  1000, train_loss: 4.643366\n",
      "batch_idx:  2000, train_loss: 4.710827\n",
      "batch_idx:  3000, train_loss: 4.719677\n",
      "batch_idx:  4000, train_loss: 4.692425\n",
      "batch_idx:  5000, train_loss: 4.092042\n",
      "batch_idx:  6000, train_loss: 3.756251\n",
      "final train_loss: 3.751823\n",
      "batch_idx:     0, valid_loss: 6.983755\n",
      "batch_idx:  1000, valid_loss: 7.573930\n",
      "batch_idx:  2000, valid_loss: 7.370193\n",
      "batch_idx:  3000, valid_loss: 7.237169\n",
      "batch_idx:  4000, valid_loss: 7.114826\n",
      "batch_idx:  5000, valid_loss: 6.885133\n",
      "batch_idx:  6000, valid_loss: 6.635571\n",
      "Training Loss: 3.751823 \tValidation Loss: 6.347192\n",
      "epoch:   304\n",
      "batch_idx:     0, train_loss: 7.988048\n",
      "batch_idx:  1000, train_loss: 1.831182\n",
      "batch_idx:  2000, train_loss: 2.002177\n",
      "batch_idx:  3000, train_loss: 2.137927\n",
      "batch_idx:  4000, train_loss: 2.097358\n",
      "batch_idx:  5000, train_loss: 2.160750\n",
      "batch_idx:  6000, train_loss: 2.241342\n",
      "final train_loss: 2.240057\n",
      "batch_idx:     0, valid_loss: 7.277214\n",
      "batch_idx:  1000, valid_loss: 10.708259\n",
      "batch_idx:  2000, valid_loss: 11.442811\n",
      "batch_idx:  3000, valid_loss: 11.611560\n",
      "batch_idx:  4000, valid_loss: 11.637065\n",
      "batch_idx:  5000, valid_loss: 11.498814\n",
      "batch_idx:  6000, valid_loss: 11.213153\n",
      "Training Loss: 2.240057 \tValidation Loss: 10.742575\n",
      "epoch:   305\n",
      "batch_idx:     0, train_loss: 4.693327\n",
      "batch_idx:  1000, train_loss: 1.864951\n",
      "batch_idx:  2000, train_loss: 2.097883\n",
      "batch_idx:  3000, train_loss: 2.266110\n",
      "batch_idx:  4000, train_loss: 2.108934\n",
      "batch_idx:  5000, train_loss: 2.127724\n",
      "batch_idx:  6000, train_loss: 2.251052\n",
      "final train_loss: 2.378903\n",
      "batch_idx:     0, valid_loss: 9.040707\n",
      "batch_idx:  1000, valid_loss: 9.807535\n",
      "batch_idx:  2000, valid_loss: 9.507650\n",
      "batch_idx:  3000, valid_loss: 9.275007\n",
      "batch_idx:  4000, valid_loss: 9.086207\n",
      "batch_idx:  5000, valid_loss: 8.939445\n",
      "batch_idx:  6000, valid_loss: 8.627712\n",
      "Training Loss: 2.378903 \tValidation Loss: 8.252644\n",
      "epoch:   306\n",
      "batch_idx:     0, train_loss: 11.864501\n",
      "batch_idx:  1000, train_loss: 2.959141\n",
      "batch_idx:  2000, train_loss: 2.945707\n",
      "batch_idx:  3000, train_loss: 2.549477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  4000, train_loss: 2.417352\n",
      "batch_idx:  5000, train_loss: 2.536902\n",
      "batch_idx:  6000, train_loss: 2.643252\n",
      "final train_loss: 2.598779\n",
      "batch_idx:     0, valid_loss: 9.121602\n",
      "batch_idx:  1000, valid_loss: 9.887319\n",
      "batch_idx:  2000, valid_loss: 10.919734\n",
      "batch_idx:  3000, valid_loss: 11.123069\n",
      "batch_idx:  4000, valid_loss: 11.103667\n",
      "batch_idx:  5000, valid_loss: 11.102367\n",
      "batch_idx:  6000, valid_loss: 10.851614\n",
      "Training Loss: 2.598779 \tValidation Loss: 10.387830\n",
      "epoch:   307\n",
      "batch_idx:     0, train_loss: 10.867414\n",
      "batch_idx:  1000, train_loss: 1.984552\n",
      "batch_idx:  2000, train_loss: 2.288273\n",
      "batch_idx:  3000, train_loss: 2.114582\n",
      "batch_idx:  4000, train_loss: 1.929320\n",
      "batch_idx:  5000, train_loss: 1.990735\n",
      "batch_idx:  6000, train_loss: 2.060395\n",
      "final train_loss: 2.139591\n",
      "batch_idx:     0, valid_loss: 10.720120\n",
      "batch_idx:  1000, valid_loss: 10.865504\n",
      "batch_idx:  2000, valid_loss: 10.816996\n",
      "batch_idx:  3000, valid_loss: 10.666142\n",
      "batch_idx:  4000, valid_loss: 10.419007\n",
      "batch_idx:  5000, valid_loss: 10.156743\n",
      "batch_idx:  6000, valid_loss: 9.816297\n",
      "Training Loss: 2.139591 \tValidation Loss: 9.417862\n",
      "epoch:   308\n",
      "batch_idx:     0, train_loss: 4.796109\n",
      "batch_idx:  1000, train_loss: 2.273003\n",
      "batch_idx:  2000, train_loss: 2.437924\n",
      "batch_idx:  3000, train_loss: 2.149612\n",
      "batch_idx:  4000, train_loss: 2.069548\n",
      "batch_idx:  5000, train_loss: 2.103582\n",
      "batch_idx:  6000, train_loss: 2.199506\n",
      "final train_loss: 2.299949\n",
      "batch_idx:     0, valid_loss: 8.839134\n",
      "batch_idx:  1000, valid_loss: 9.075518\n",
      "batch_idx:  2000, valid_loss: 8.901337\n",
      "batch_idx:  3000, valid_loss: 8.759718\n",
      "batch_idx:  4000, valid_loss: 8.561139\n",
      "batch_idx:  5000, valid_loss: 8.353120\n",
      "batch_idx:  6000, valid_loss: 8.064849\n",
      "Training Loss: 2.299949 \tValidation Loss: 7.720101\n",
      "epoch:   309\n",
      "batch_idx:     0, train_loss: 9.823545\n",
      "batch_idx:  1000, train_loss: 2.832419\n",
      "batch_idx:  2000, train_loss: 2.151681\n",
      "batch_idx:  3000, train_loss: 1.776338\n",
      "batch_idx:  4000, train_loss: 1.757095\n",
      "batch_idx:  5000, train_loss: 1.821327\n",
      "batch_idx:  6000, train_loss: 1.964462\n",
      "final train_loss: 2.053938\n",
      "batch_idx:     0, valid_loss: 8.189172\n",
      "batch_idx:  1000, valid_loss: 8.042158\n",
      "batch_idx:  2000, valid_loss: 9.656587\n",
      "batch_idx:  3000, valid_loss: 10.686726\n",
      "batch_idx:  4000, valid_loss: 10.991818\n",
      "batch_idx:  5000, valid_loss: 11.102957\n",
      "batch_idx:  6000, valid_loss: 11.041661\n",
      "Training Loss: 2.053938 \tValidation Loss: 10.784475\n",
      "epoch:   310\n",
      "batch_idx:     0, train_loss: 9.679688\n",
      "batch_idx:  1000, train_loss: 1.496320\n",
      "batch_idx:  2000, train_loss: 1.385626\n",
      "batch_idx:  3000, train_loss: 1.270149\n",
      "batch_idx:  4000, train_loss: 1.346041\n",
      "batch_idx:  5000, train_loss: 1.472591\n",
      "batch_idx:  6000, train_loss: 1.846378\n",
      "final train_loss: 1.853535\n",
      "batch_idx:     0, valid_loss: 8.875530\n",
      "batch_idx:  1000, valid_loss: 9.858999\n",
      "batch_idx:  2000, valid_loss: 9.771876\n",
      "batch_idx:  3000, valid_loss: 9.547810\n",
      "batch_idx:  4000, valid_loss: 9.341104\n",
      "batch_idx:  5000, valid_loss: 9.124200\n",
      "batch_idx:  6000, valid_loss: 8.780429\n",
      "Training Loss: 1.853535 \tValidation Loss: 8.375937\n",
      "epoch:   311\n",
      "batch_idx:     0, train_loss: 11.441232\n",
      "batch_idx:  1000, train_loss: 2.782810\n",
      "batch_idx:  2000, train_loss: 2.187305\n",
      "batch_idx:  3000, train_loss: 2.098106\n",
      "batch_idx:  4000, train_loss: 2.237146\n",
      "batch_idx:  5000, train_loss: 2.366829\n",
      "batch_idx:  6000, train_loss: 2.475625\n",
      "final train_loss: 2.426126\n",
      "batch_idx:     0, valid_loss: 8.699625\n",
      "batch_idx:  1000, valid_loss: 7.756008\n",
      "batch_idx:  2000, valid_loss: 10.194949\n",
      "batch_idx:  3000, valid_loss: 10.780929\n",
      "batch_idx:  4000, valid_loss: 10.941086\n",
      "batch_idx:  5000, valid_loss: 10.969927\n",
      "batch_idx:  6000, valid_loss: 10.833632\n",
      "Training Loss: 2.426126 \tValidation Loss: 10.454394\n",
      "epoch:   312\n",
      "batch_idx:     0, train_loss: 10.223130\n",
      "batch_idx:  1000, train_loss: 1.653226\n",
      "batch_idx:  2000, train_loss: 1.394903\n",
      "batch_idx:  3000, train_loss: 1.357348\n",
      "batch_idx:  4000, train_loss: 1.440402\n",
      "batch_idx:  5000, train_loss: 1.632939\n",
      "batch_idx:  6000, train_loss: 1.782057\n",
      "final train_loss: 1.850472\n",
      "batch_idx:     0, valid_loss: 5.924774\n",
      "batch_idx:  1000, valid_loss: 7.899589\n",
      "batch_idx:  2000, valid_loss: 7.773867\n",
      "batch_idx:  3000, valid_loss: 7.928417\n",
      "batch_idx:  4000, valid_loss: 7.903595\n",
      "batch_idx:  5000, valid_loss: 7.797618\n",
      "batch_idx:  6000, valid_loss: 7.625973\n",
      "Training Loss: 1.850472 \tValidation Loss: 7.419184\n",
      "epoch:   313\n",
      "batch_idx:     0, train_loss: 5.105728\n",
      "batch_idx:  1000, train_loss: 0.554233\n",
      "batch_idx:  2000, train_loss: 0.746769\n",
      "batch_idx:  3000, train_loss: 1.113015\n",
      "batch_idx:  4000, train_loss: 1.421499\n",
      "batch_idx:  5000, train_loss: 1.814316\n",
      "batch_idx:  6000, train_loss: 1.924724\n",
      "final train_loss: 1.944890\n",
      "batch_idx:     0, valid_loss: 9.913718\n",
      "batch_idx:  1000, valid_loss: 9.665349\n",
      "batch_idx:  2000, valid_loss: 10.028230\n",
      "batch_idx:  3000, valid_loss: 10.153615\n",
      "batch_idx:  4000, valid_loss: 10.235742\n",
      "batch_idx:  5000, valid_loss: 10.251443\n",
      "batch_idx:  6000, valid_loss: 10.021444\n",
      "Training Loss: 1.944890 \tValidation Loss: 9.657293\n",
      "epoch:   314\n",
      "batch_idx:     0, train_loss: 11.512377\n",
      "batch_idx:  1000, train_loss: 1.736081\n",
      "batch_idx:  2000, train_loss: 1.937580\n",
      "batch_idx:  3000, train_loss: 2.034981\n",
      "batch_idx:  4000, train_loss: 2.174145\n",
      "batch_idx:  5000, train_loss: 2.126931\n",
      "batch_idx:  6000, train_loss: 2.086796\n",
      "final train_loss: 2.102269\n",
      "batch_idx:     0, valid_loss: 7.045348\n",
      "batch_idx:  1000, valid_loss: 11.103499\n",
      "batch_idx:  2000, valid_loss: 12.955255\n",
      "batch_idx:  3000, valid_loss: 13.256104\n",
      "batch_idx:  4000, valid_loss: 13.189125\n",
      "batch_idx:  5000, valid_loss: 12.774614\n",
      "batch_idx:  6000, valid_loss: 12.322480\n",
      "Training Loss: 2.102269 \tValidation Loss: 11.860407\n",
      "epoch:   315\n",
      "batch_idx:     0, train_loss: 8.883511\n",
      "batch_idx:  1000, train_loss: 2.267477\n",
      "batch_idx:  2000, train_loss: 2.416470\n",
      "batch_idx:  3000, train_loss: 2.239961\n",
      "batch_idx:  4000, train_loss: 2.348770\n",
      "batch_idx:  5000, train_loss: 2.444402\n",
      "batch_idx:  6000, train_loss: 2.327707\n",
      "final train_loss: 2.293778\n",
      "batch_idx:     0, valid_loss: 8.461924\n",
      "batch_idx:  1000, valid_loss: 10.808611\n",
      "batch_idx:  2000, valid_loss: 11.221160\n",
      "batch_idx:  3000, valid_loss: 11.380824\n",
      "batch_idx:  4000, valid_loss: 11.428208\n",
      "batch_idx:  5000, valid_loss: 11.385715\n",
      "batch_idx:  6000, valid_loss: 10.928496\n",
      "Training Loss: 2.293778 \tValidation Loss: 10.464901\n",
      "epoch:   316\n",
      "batch_idx:     0, train_loss: 11.407917\n",
      "batch_idx:  1000, train_loss: 2.213575\n",
      "batch_idx:  2000, train_loss: 2.454827\n",
      "batch_idx:  3000, train_loss: 2.570181\n",
      "batch_idx:  4000, train_loss: 2.478402\n",
      "batch_idx:  5000, train_loss: 2.333436\n",
      "batch_idx:  6000, train_loss: 2.721806\n",
      "final train_loss: 2.954941\n",
      "batch_idx:     0, valid_loss: 4.876448\n",
      "batch_idx:  1000, valid_loss: 11.406557\n",
      "batch_idx:  2000, valid_loss: 11.586489\n",
      "batch_idx:  3000, valid_loss: 11.322144\n",
      "batch_idx:  4000, valid_loss: 11.024056\n",
      "batch_idx:  5000, valid_loss: 10.202958\n",
      "batch_idx:  6000, valid_loss: 9.995904\n",
      "Training Loss: 2.954941 \tValidation Loss: 9.778296\n",
      "epoch:   317\n",
      "batch_idx:     0, train_loss: 4.594864\n",
      "batch_idx:  1000, train_loss: 2.470934\n",
      "batch_idx:  2000, train_loss: 2.503097\n",
      "batch_idx:  3000, train_loss: 2.401445\n",
      "batch_idx:  4000, train_loss: 2.321312\n",
      "batch_idx:  5000, train_loss: 2.169781\n",
      "batch_idx:  6000, train_loss: 2.457713\n",
      "final train_loss: 2.710299\n",
      "batch_idx:     0, valid_loss: 5.244088\n",
      "batch_idx:  1000, valid_loss: 4.947667\n",
      "batch_idx:  2000, valid_loss: 4.964052\n",
      "batch_idx:  3000, valid_loss: 4.988031\n",
      "batch_idx:  4000, valid_loss: 5.000327\n",
      "batch_idx:  5000, valid_loss: 4.993346\n",
      "batch_idx:  6000, valid_loss: 4.927692\n",
      "Training Loss: 2.710299 \tValidation Loss: 4.911962\n",
      "epoch:   318\n",
      "batch_idx:     0, train_loss: 5.244088\n",
      "batch_idx:  1000, train_loss: 4.690729\n",
      "batch_idx:  2000, train_loss: 4.751100\n",
      "batch_idx:  3000, train_loss: 4.805272\n",
      "batch_idx:  4000, train_loss: 4.849185\n",
      "batch_idx:  5000, train_loss: 4.874659\n",
      "batch_idx:  6000, train_loss: 4.865755\n",
      "final train_loss: 4.880046\n",
      "batch_idx:     0, valid_loss: 5.101900\n",
      "batch_idx:  1000, valid_loss: 4.941124\n",
      "batch_idx:  2000, valid_loss: 4.947362\n",
      "batch_idx:  3000, valid_loss: 4.946341\n",
      "batch_idx:  4000, valid_loss: 4.947254\n",
      "batch_idx:  5000, valid_loss: 4.934999\n",
      "batch_idx:  6000, valid_loss: 4.896419\n",
      "Training Loss: 4.880046 \tValidation Loss: 4.890039\n",
      "epoch:   319\n",
      "batch_idx:     0, train_loss: 5.101900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  1000, train_loss: 4.685211\n",
      "batch_idx:  2000, train_loss: 4.737536\n",
      "batch_idx:  3000, train_loss: 4.772685\n",
      "batch_idx:  4000, train_loss: 4.810229\n",
      "batch_idx:  5000, train_loss: 4.834522\n",
      "batch_idx:  6000, train_loss: 4.845210\n",
      "final train_loss: 4.866201\n",
      "batch_idx:     0, valid_loss: 5.024341\n",
      "batch_idx:  1000, valid_loss: 4.940141\n",
      "batch_idx:  2000, valid_loss: 4.941825\n",
      "batch_idx:  3000, valid_loss: 4.926176\n",
      "batch_idx:  4000, valid_loss: 4.920151\n",
      "batch_idx:  5000, valid_loss: 4.904591\n",
      "batch_idx:  6000, valid_loss: 4.881771\n",
      "Training Loss: 4.866201 \tValidation Loss: 4.881269\n",
      "epoch:   320\n",
      "batch_idx:     0, train_loss: 5.024341\n",
      "batch_idx:  1000, train_loss: 4.684465\n",
      "batch_idx:  2000, train_loss: 4.732946\n",
      "batch_idx:  3000, train_loss: 4.756684\n",
      "batch_idx:  4000, train_loss: 4.790054\n",
      "batch_idx:  5000, train_loss: 4.813300\n",
      "batch_idx:  6000, train_loss: 4.835429\n",
      "final train_loss: 4.860491\n",
      "batch_idx:     0, valid_loss: 4.981931\n",
      "batch_idx:  1000, valid_loss: 4.939708\n",
      "batch_idx:  2000, valid_loss: 4.939392\n",
      "batch_idx:  3000, valid_loss: 4.915318\n",
      "batch_idx:  4000, valid_loss: 4.905143\n",
      "batch_idx:  5000, valid_loss: 4.887549\n",
      "batch_idx:  6000, valid_loss: 4.874190\n",
      "Training Loss: 4.860491 \tValidation Loss: 4.877398\n",
      "epoch:   321\n",
      "batch_idx:     0, train_loss: 4.981931\n",
      "batch_idx:  1000, train_loss: 4.684113\n",
      "batch_idx:  2000, train_loss: 4.730955\n",
      "batch_idx:  3000, train_loss: 4.748010\n",
      "batch_idx:  4000, train_loss: 4.778813\n",
      "batch_idx:  5000, train_loss: 4.801289\n",
      "batch_idx:  6000, train_loss: 4.830306\n",
      "final train_loss: 4.857915\n",
      "batch_idx:     0, valid_loss: 4.958680\n",
      "batch_idx:  1000, valid_loss: 4.939116\n",
      "batch_idx:  2000, valid_loss: 4.937925\n",
      "batch_idx:  3000, valid_loss: 4.909015\n",
      "batch_idx:  4000, valid_loss: 4.896368\n",
      "batch_idx:  5000, valid_loss: 4.877531\n",
      "batch_idx:  6000, valid_loss: 4.869919\n",
      "Training Loss: 4.857915 \tValidation Loss: 4.875475\n",
      "epoch:   322\n",
      "batch_idx:     0, train_loss: 4.958680\n",
      "batch_idx:  1000, train_loss: 4.683675\n",
      "batch_idx:  2000, train_loss: 4.729701\n",
      "batch_idx:  3000, train_loss: 4.742959\n",
      "batch_idx:  4000, train_loss: 4.772239\n",
      "batch_idx:  5000, train_loss: 4.794209\n",
      "batch_idx:  6000, train_loss: 4.827459\n",
      "final train_loss: 4.856680\n",
      "batch_idx:     0, valid_loss: 4.945860\n",
      "batch_idx:  1000, valid_loss: 4.938368\n",
      "batch_idx:  2000, valid_loss: 4.936816\n",
      "batch_idx:  3000, valid_loss: 4.905207\n",
      "batch_idx:  4000, valid_loss: 4.891107\n",
      "batch_idx:  5000, valid_loss: 4.871461\n",
      "batch_idx:  6000, valid_loss: 4.867430\n",
      "Training Loss: 4.856680 \tValidation Loss: 4.874503\n",
      "epoch:   323\n",
      "batch_idx:     0, train_loss: 4.945860\n",
      "batch_idx:  1000, train_loss: 4.682971\n",
      "batch_idx:  2000, train_loss: 4.728745\n",
      "batch_idx:  3000, train_loss: 4.739824\n",
      "batch_idx:  4000, train_loss: 4.768161\n",
      "batch_idx:  5000, train_loss: 4.789832\n",
      "batch_idx:  6000, train_loss: 4.825714\n",
      "final train_loss: 4.855978\n",
      "batch_idx:     0, valid_loss: 4.938694\n",
      "batch_idx:  1000, valid_loss: 4.937615\n",
      "batch_idx:  2000, valid_loss: 4.935940\n",
      "batch_idx:  3000, valid_loss: 4.902725\n",
      "batch_idx:  4000, valid_loss: 4.887760\n",
      "batch_idx:  5000, valid_loss: 4.867664\n",
      "batch_idx:  6000, valid_loss: 4.865855\n",
      "Training Loss: 4.855978 \tValidation Loss: 4.873912\n",
      "epoch:   324\n",
      "batch_idx:     0, train_loss: 4.938694\n",
      "batch_idx:  1000, train_loss: 4.682131\n",
      "batch_idx:  2000, train_loss: 4.727827\n",
      "batch_idx:  3000, train_loss: 4.737730\n",
      "batch_idx:  4000, train_loss: 4.765564\n",
      "batch_idx:  5000, train_loss: 4.787045\n",
      "batch_idx:  6000, train_loss: 4.824593\n",
      "final train_loss: 4.855542\n",
      "batch_idx:     0, valid_loss: 4.934674\n",
      "batch_idx:  1000, valid_loss: 4.936963\n",
      "batch_idx:  2000, valid_loss: 4.935250\n",
      "batch_idx:  3000, valid_loss: 4.901144\n",
      "batch_idx:  4000, valid_loss: 4.885667\n",
      "batch_idx:  5000, valid_loss: 4.865290\n",
      "batch_idx:  6000, valid_loss: 4.864915\n",
      "Training Loss: 4.855542 \tValidation Loss: 4.873607\n",
      "epoch:   325\n",
      "batch_idx:     0, train_loss: 4.934674\n",
      "batch_idx:  1000, train_loss: 4.681551\n",
      "batch_idx:  2000, train_loss: 4.727264\n",
      "batch_idx:  3000, train_loss: 4.736330\n",
      "batch_idx:  4000, train_loss: 4.502431\n",
      "batch_idx:  5000, train_loss: 4.120674\n",
      "batch_idx:  6000, train_loss: 3.843859\n",
      "final train_loss: 3.735064\n",
      "batch_idx:     0, valid_loss: 9.593265\n",
      "batch_idx:  1000, valid_loss: 10.505857\n",
      "batch_idx:  2000, valid_loss: 11.081745\n",
      "batch_idx:  3000, valid_loss: 11.248277\n",
      "batch_idx:  4000, valid_loss: 11.307717\n",
      "batch_idx:  5000, valid_loss: 11.056791\n",
      "batch_idx:  6000, valid_loss: 10.684216\n",
      "Training Loss: 3.735064 \tValidation Loss: 10.248174\n",
      "epoch:   326\n",
      "batch_idx:     0, train_loss: 12.223096\n",
      "batch_idx:  1000, train_loss: 1.998212\n",
      "batch_idx:  2000, train_loss: 2.403207\n",
      "batch_idx:  3000, train_loss: 2.385747\n",
      "batch_idx:  4000, train_loss: 2.243129\n",
      "batch_idx:  5000, train_loss: 2.285977\n",
      "batch_idx:  6000, train_loss: 2.586257\n",
      "final train_loss: 2.839343\n",
      "batch_idx:     0, valid_loss: 4.925760\n",
      "batch_idx:  1000, valid_loss: 4.892517\n",
      "batch_idx:  2000, valid_loss: 4.852506\n",
      "batch_idx:  3000, valid_loss: 4.812954\n",
      "batch_idx:  4000, valid_loss: 4.837422\n",
      "batch_idx:  5000, valid_loss: 4.865555\n",
      "batch_idx:  6000, valid_loss: 4.873035\n",
      "Training Loss: 2.839343 \tValidation Loss: 4.875767\n",
      "epoch:   327\n",
      "batch_idx:     0, train_loss: 4.925760\n",
      "batch_idx:  1000, train_loss: 4.640380\n",
      "batch_idx:  2000, train_loss: 4.655720\n",
      "batch_idx:  3000, train_loss: 4.665310\n",
      "batch_idx:  4000, train_loss: 4.727858\n",
      "batch_idx:  5000, train_loss: 4.785302\n",
      "batch_idx:  6000, train_loss: 4.828464\n",
      "final train_loss: 4.855197\n",
      "batch_idx:     0, valid_loss: 4.920677\n",
      "batch_idx:  1000, valid_loss: 4.906854\n",
      "batch_idx:  2000, valid_loss: 4.883432\n",
      "batch_idx:  3000, valid_loss: 4.845996\n",
      "batch_idx:  4000, valid_loss: 4.854644\n",
      "batch_idx:  5000, valid_loss: 4.863002\n",
      "batch_idx:  6000, valid_loss: 4.867960\n",
      "Training Loss: 4.855197 \tValidation Loss: 4.872697\n",
      "epoch:   328\n",
      "batch_idx:     0, train_loss: 4.920677\n",
      "batch_idx:  1000, train_loss: 4.653684\n",
      "batch_idx:  2000, train_loss: 4.682423\n",
      "batch_idx:  3000, train_loss: 4.691837\n",
      "batch_idx:  4000, train_loss: 4.740649\n",
      "batch_idx:  5000, train_loss: 4.783397\n",
      "batch_idx:  6000, train_loss: 4.825352\n",
      "final train_loss: 4.853781\n",
      "batch_idx:     0, valid_loss: 4.921799\n",
      "batch_idx:  1000, valid_loss: 4.917546\n",
      "batch_idx:  2000, valid_loss: 4.903178\n",
      "batch_idx:  3000, valid_loss: 4.866750\n",
      "batch_idx:  4000, valid_loss: 4.865794\n",
      "batch_idx:  5000, valid_loss: 4.862252\n",
      "batch_idx:  6000, valid_loss: 4.865924\n",
      "Training Loss: 4.853781 \tValidation Loss: 4.872198\n",
      "Training loss decreased (4.872631 --> 4.872198).  Saving model ...\n",
      "epoch:   329\n",
      "batch_idx:     0, train_loss: 4.921799\n",
      "batch_idx:  1000, train_loss: 4.663631\n",
      "batch_idx:  2000, train_loss: 4.699539\n",
      "batch_idx:  3000, train_loss: 4.708647\n",
      "batch_idx:  4000, train_loss: 4.749045\n",
      "batch_idx:  5000, train_loss: 4.782968\n",
      "batch_idx:  6000, train_loss: 4.824226\n",
      "final train_loss: 4.853833\n",
      "batch_idx:     0, valid_loss: 4.923961\n",
      "batch_idx:  1000, valid_loss: 4.924500\n",
      "batch_idx:  2000, valid_loss: 4.915270\n",
      "batch_idx:  3000, valid_loss: 4.879402\n",
      "batch_idx:  4000, valid_loss: 4.872546\n",
      "batch_idx:  5000, valid_loss: 4.861927\n",
      "batch_idx:  6000, valid_loss: 4.864864\n",
      "Training Loss: 4.853833 \tValidation Loss: 4.872250\n",
      "epoch:   330\n",
      "batch_idx:     0, train_loss: 4.923961\n",
      "batch_idx:  1000, train_loss: 4.670135\n",
      "batch_idx:  2000, train_loss: 4.709946\n",
      "batch_idx:  3000, train_loss: 4.718792\n",
      "batch_idx:  4000, train_loss: 4.754122\n",
      "batch_idx:  5000, train_loss: 4.782806\n",
      "batch_idx:  6000, train_loss: 4.823686\n",
      "final train_loss: 4.854120\n",
      "batch_idx:     0, valid_loss: 4.925814\n",
      "batch_idx:  1000, valid_loss: 4.928792\n",
      "batch_idx:  2000, valid_loss: 4.922554\n",
      "batch_idx:  3000, valid_loss: 4.886935\n",
      "batch_idx:  4000, valid_loss: 4.876616\n",
      "batch_idx:  5000, valid_loss: 4.861794\n",
      "batch_idx:  6000, valid_loss: 4.864359\n",
      "Training Loss: 4.854120 \tValidation Loss: 4.872542\n",
      "epoch:   331\n",
      "batch_idx:     0, train_loss: 4.925814\n",
      "batch_idx:  1000, train_loss: 4.673939\n",
      "batch_idx:  2000, train_loss: 3.994589\n",
      "batch_idx:  3000, train_loss: 3.309537\n",
      "batch_idx:  4000, train_loss: 2.950496\n",
      "batch_idx:  5000, train_loss: 2.715414\n",
      "batch_idx:  6000, train_loss: 2.633598\n",
      "final train_loss: 2.629762\n",
      "batch_idx:     0, valid_loss: 9.710763\n",
      "batch_idx:  1000, valid_loss: 10.928192\n",
      "batch_idx:  2000, valid_loss: 11.002651\n",
      "batch_idx:  3000, valid_loss: 10.953087\n",
      "batch_idx:  4000, valid_loss: 10.775639\n",
      "batch_idx:  5000, valid_loss: 10.545737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  6000, valid_loss: 10.212828\n",
      "Training Loss: 2.629762 \tValidation Loss: 9.812587\n",
      "epoch:   332\n",
      "batch_idx:     0, train_loss: 12.714636\n",
      "batch_idx:  1000, train_loss: 2.222173\n",
      "batch_idx:  2000, train_loss: 2.477890\n",
      "batch_idx:  3000, train_loss: 2.229430\n",
      "batch_idx:  4000, train_loss: 2.175928\n",
      "batch_idx:  5000, train_loss: 2.242645\n",
      "batch_idx:  6000, train_loss: 2.252115\n",
      "final train_loss: 2.417308\n",
      "batch_idx:     0, valid_loss: 4.680094\n",
      "batch_idx:  1000, valid_loss: 4.683194\n",
      "batch_idx:  2000, valid_loss: 4.733537\n",
      "batch_idx:  3000, valid_loss: 4.800536\n",
      "batch_idx:  4000, valid_loss: 4.840321\n",
      "batch_idx:  5000, valid_loss: 4.854205\n",
      "batch_idx:  6000, valid_loss: 4.876966\n",
      "Training Loss: 2.417308 \tValidation Loss: 4.876142\n",
      "epoch:   333\n",
      "batch_idx:     0, train_loss: 4.707430\n",
      "batch_idx:  1000, train_loss: 3.242573\n",
      "batch_idx:  2000, train_loss: 2.372017\n",
      "batch_idx:  3000, train_loss: 2.130555\n",
      "batch_idx:  4000, train_loss: 2.066437\n",
      "batch_idx:  5000, train_loss: 2.269127\n",
      "batch_idx:  6000, train_loss: 2.516387\n",
      "final train_loss: 2.509115\n",
      "batch_idx:     0, valid_loss: 9.945990\n",
      "batch_idx:  1000, valid_loss: 11.394888\n",
      "batch_idx:  2000, valid_loss: 11.436073\n",
      "batch_idx:  3000, valid_loss: 11.192966\n",
      "batch_idx:  4000, valid_loss: 10.850415\n",
      "batch_idx:  5000, valid_loss: 10.532956\n",
      "batch_idx:  6000, valid_loss: 10.172239\n",
      "Training Loss: 2.509115 \tValidation Loss: 9.708689\n",
      "epoch:   334\n",
      "batch_idx:     0, train_loss: 12.831791\n",
      "batch_idx:  1000, train_loss: 1.011470\n",
      "batch_idx:  2000, train_loss: 1.030313\n",
      "batch_idx:  3000, train_loss: 1.449108\n",
      "batch_idx:  4000, train_loss: 1.720402\n",
      "batch_idx:  5000, train_loss: 1.922951\n",
      "batch_idx:  6000, train_loss: 2.037110\n",
      "final train_loss: 1.985239\n",
      "batch_idx:     0, valid_loss: 9.456773\n",
      "batch_idx:  1000, valid_loss: 15.957264\n",
      "batch_idx:  2000, valid_loss: 17.648039\n",
      "batch_idx:  3000, valid_loss: 17.981827\n",
      "batch_idx:  4000, valid_loss: 17.897348\n",
      "batch_idx:  5000, valid_loss: 17.631952\n",
      "batch_idx:  6000, valid_loss: 16.943146\n",
      "Training Loss: 1.985239 \tValidation Loss: 16.111410\n",
      "epoch:   335\n",
      "batch_idx:     0, train_loss: 14.258078\n",
      "batch_idx:  1000, train_loss: 1.506047\n",
      "batch_idx:  2000, train_loss: 1.754409\n",
      "batch_idx:  3000, train_loss: 1.888873\n",
      "batch_idx:  4000, train_loss: 2.035257\n",
      "batch_idx:  5000, train_loss: 1.989032\n",
      "batch_idx:  6000, train_loss: 1.970218\n",
      "final train_loss: 1.899707\n",
      "batch_idx:     0, valid_loss: 10.335543\n",
      "batch_idx:  1000, valid_loss: 14.609149\n",
      "batch_idx:  2000, valid_loss: 16.934317\n",
      "batch_idx:  3000, valid_loss: 17.453819\n",
      "batch_idx:  4000, valid_loss: 17.381235\n",
      "batch_idx:  5000, valid_loss: 17.120438\n",
      "batch_idx:  6000, valid_loss: 16.476776\n",
      "Training Loss: 1.899707 \tValidation Loss: 15.610475\n",
      "epoch:   336\n",
      "batch_idx:     0, train_loss: 11.478178\n",
      "batch_idx:  1000, train_loss: 0.777263\n",
      "batch_idx:  2000, train_loss: 1.457016\n",
      "batch_idx:  3000, train_loss: 1.570023\n",
      "batch_idx:  4000, train_loss: 1.623698\n",
      "batch_idx:  5000, train_loss: 1.716422\n",
      "batch_idx:  6000, train_loss: 1.749282\n",
      "final train_loss: 1.767033\n",
      "batch_idx:     0, valid_loss: 7.128174\n",
      "batch_idx:  1000, valid_loss: 10.409478\n",
      "batch_idx:  2000, valid_loss: 11.335402\n",
      "batch_idx:  3000, valid_loss: 11.631089\n",
      "batch_idx:  4000, valid_loss: 11.711568\n",
      "batch_idx:  5000, valid_loss: 11.516838\n",
      "batch_idx:  6000, valid_loss: 11.263547\n",
      "Training Loss: 1.767033 \tValidation Loss: 10.755057\n",
      "epoch:   337\n",
      "batch_idx:     0, train_loss: 9.374057\n",
      "batch_idx:  1000, train_loss: 2.030360\n",
      "batch_idx:  2000, train_loss: 2.482723\n",
      "batch_idx:  3000, train_loss: 2.361549\n",
      "batch_idx:  4000, train_loss: 2.520859\n",
      "batch_idx:  5000, train_loss: 2.534054\n",
      "batch_idx:  6000, train_loss: 2.383013\n",
      "final train_loss: 2.520953\n",
      "batch_idx:     0, valid_loss: 4.729692\n",
      "batch_idx:  1000, valid_loss: 4.872077\n",
      "batch_idx:  2000, valid_loss: 4.846307\n",
      "batch_idx:  3000, valid_loss: 4.861251\n",
      "batch_idx:  4000, valid_loss: 4.853105\n",
      "batch_idx:  5000, valid_loss: 4.831263\n",
      "batch_idx:  6000, valid_loss: 4.856857\n",
      "Training Loss: 2.520953 \tValidation Loss: 4.874137\n",
      "epoch:   338\n",
      "batch_idx:     0, train_loss: 4.729692\n",
      "batch_idx:  1000, train_loss: 4.621583\n",
      "batch_idx:  2000, train_loss: 4.650684\n",
      "batch_idx:  3000, train_loss: 4.703835\n",
      "batch_idx:  4000, train_loss: 4.738381\n",
      "batch_idx:  5000, train_loss: 4.759862\n",
      "batch_idx:  6000, train_loss: 4.817257\n",
      "final train_loss: 4.854272\n",
      "batch_idx:     0, valid_loss: 4.823425\n",
      "batch_idx:  1000, valid_loss: 4.896149\n",
      "batch_idx:  2000, valid_loss: 4.880792\n",
      "batch_idx:  3000, valid_loss: 4.873238\n",
      "batch_idx:  4000, valid_loss: 4.861719\n",
      "batch_idx:  5000, valid_loss: 4.840095\n",
      "batch_idx:  6000, valid_loss: 4.856883\n",
      "Training Loss: 4.854272 \tValidation Loss: 4.871437\n",
      "Training loss decreased (4.872198 --> 4.871437).  Saving model ...\n",
      "epoch:   339\n",
      "batch_idx:     0, train_loss: 4.823425\n",
      "batch_idx:  1000, train_loss: 4.643870\n",
      "batch_idx:  2000, train_loss: 4.680356\n",
      "batch_idx:  3000, train_loss: 4.713696\n",
      "batch_idx:  4000, train_loss: 4.745396\n",
      "batch_idx:  5000, train_loss: 4.766758\n",
      "batch_idx:  6000, train_loss: 4.817842\n",
      "final train_loss: 4.853125\n",
      "batch_idx:     0, valid_loss: 4.872644\n",
      "batch_idx:  1000, valid_loss: 4.911814\n",
      "batch_idx:  2000, valid_loss: 4.901901\n",
      "batch_idx:  3000, valid_loss: 4.882066\n",
      "batch_idx:  4000, valid_loss: 4.868732\n",
      "batch_idx:  5000, valid_loss: 4.847274\n",
      "batch_idx:  6000, valid_loss: 4.858531\n",
      "Training Loss: 4.853125 \tValidation Loss: 4.871344\n",
      "Training loss decreased (4.871437 --> 4.871344).  Saving model ...\n",
      "epoch:   340\n",
      "batch_idx:     0, train_loss: 4.872644\n",
      "batch_idx:  1000, train_loss: 4.658292\n",
      "batch_idx:  2000, train_loss: 4.698411\n",
      "batch_idx:  3000, train_loss: 4.720813\n",
      "batch_idx:  4000, train_loss: 4.750823\n",
      "batch_idx:  5000, train_loss: 4.772059\n",
      "batch_idx:  6000, train_loss: 4.819202\n",
      "final train_loss: 4.853370\n",
      "batch_idx:     0, valid_loss: 4.898805\n",
      "batch_idx:  1000, valid_loss: 4.921255\n",
      "batch_idx:  2000, valid_loss: 4.914437\n",
      "batch_idx:  3000, valid_loss: 4.887841\n",
      "batch_idx:  4000, valid_loss: 4.873466\n",
      "batch_idx:  5000, valid_loss: 4.852187\n",
      "batch_idx:  6000, valid_loss: 4.860014\n",
      "Training Loss: 4.853370 \tValidation Loss: 4.871744\n",
      "epoch:   341\n",
      "batch_idx:     0, train_loss: 4.898805\n",
      "batch_idx:  1000, train_loss: 4.666981\n",
      "batch_idx:  2000, train_loss: 4.709113\n",
      "batch_idx:  3000, train_loss: 4.725482\n",
      "batch_idx:  4000, train_loss: 4.754447\n",
      "batch_idx:  5000, train_loss: 4.209414\n",
      "batch_idx:  6000, train_loss: 3.876126\n",
      "final train_loss: 3.673256\n",
      "batch_idx:     0, valid_loss: 8.266463\n",
      "batch_idx:  1000, valid_loss: 10.538437\n",
      "batch_idx:  2000, valid_loss: 11.300309\n",
      "batch_idx:  3000, valid_loss: 11.477071\n",
      "batch_idx:  4000, valid_loss: 11.503760\n",
      "batch_idx:  5000, valid_loss: 11.431348\n",
      "batch_idx:  6000, valid_loss: 11.224629\n",
      "Training Loss: 3.673256 \tValidation Loss: 10.758605\n",
      "epoch:   342\n",
      "batch_idx:     0, train_loss: 4.789379\n",
      "batch_idx:  1000, train_loss: 2.020167\n",
      "batch_idx:  2000, train_loss: 2.512326\n",
      "batch_idx:  3000, train_loss: 3.184400\n",
      "batch_idx:  4000, train_loss: 3.550513\n",
      "batch_idx:  5000, train_loss: 3.495178\n",
      "batch_idx:  6000, train_loss: 3.228324\n",
      "final train_loss: 3.040751\n",
      "batch_idx:     0, valid_loss: 6.460104\n",
      "batch_idx:  1000, valid_loss: 8.113516\n",
      "batch_idx:  2000, valid_loss: 9.317644\n",
      "batch_idx:  3000, valid_loss: 9.981470\n",
      "batch_idx:  4000, valid_loss: 10.370461\n",
      "batch_idx:  5000, valid_loss: 10.611078\n",
      "batch_idx:  6000, valid_loss: 10.538437\n",
      "Training Loss: 3.040751 \tValidation Loss: 10.064758\n",
      "epoch:   343\n",
      "batch_idx:     0, train_loss: 6.927464\n",
      "batch_idx:  1000, train_loss: 0.995179\n",
      "batch_idx:  2000, train_loss: 1.023487\n",
      "batch_idx:  3000, train_loss: 1.045860\n",
      "batch_idx:  4000, train_loss: 1.071114\n",
      "batch_idx:  5000, train_loss: 1.086630\n",
      "batch_idx:  6000, train_loss: 1.141033\n",
      "final train_loss: 1.182522\n",
      "batch_idx:     0, valid_loss: 5.052721\n",
      "batch_idx:  1000, valid_loss: 6.615180\n",
      "batch_idx:  2000, valid_loss: 8.037147\n",
      "batch_idx:  3000, valid_loss: 8.878970\n",
      "batch_idx:  4000, valid_loss: 9.411500\n",
      "batch_idx:  5000, valid_loss: 9.766037\n",
      "batch_idx:  6000, valid_loss: 9.678464\n",
      "Training Loss: 1.182522 \tValidation Loss: 9.224447\n",
      "epoch:   344\n",
      "batch_idx:     0, train_loss: 4.786607\n",
      "batch_idx:  1000, train_loss: 0.989791\n",
      "batch_idx:  2000, train_loss: 0.684008\n",
      "batch_idx:  3000, train_loss: 0.562275\n",
      "batch_idx:  4000, train_loss: 0.511085\n",
      "batch_idx:  5000, train_loss: 0.484318\n",
      "batch_idx:  6000, train_loss: 0.481284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final train_loss: 0.489428\n",
      "batch_idx:     0, valid_loss: 5.787465\n",
      "batch_idx:  1000, valid_loss: 15.581213\n",
      "batch_idx:  2000, valid_loss: 18.108515\n",
      "batch_idx:  3000, valid_loss: 19.796890\n",
      "batch_idx:  4000, valid_loss: 20.556442\n",
      "batch_idx:  5000, valid_loss: 20.781488\n",
      "batch_idx:  6000, valid_loss: 20.307957\n",
      "Training Loss: 0.489428 \tValidation Loss: 19.261929\n",
      "epoch:   345\n",
      "batch_idx:     0, train_loss: 5.854295\n",
      "batch_idx:  1000, train_loss: 0.566304\n",
      "batch_idx:  2000, train_loss: 0.779678\n",
      "batch_idx:  3000, train_loss: 0.647464\n",
      "batch_idx:  4000, train_loss: 0.597052\n",
      "batch_idx:  5000, train_loss: 0.575811\n",
      "batch_idx:  6000, train_loss: 0.596362\n",
      "final train_loss: 0.627061\n",
      "batch_idx:     0, valid_loss: 10.920990\n",
      "batch_idx:  1000, valid_loss: 16.158489\n",
      "batch_idx:  2000, valid_loss: 18.589069\n",
      "batch_idx:  3000, valid_loss: 19.732765\n",
      "batch_idx:  4000, valid_loss: 20.263668\n",
      "batch_idx:  5000, valid_loss: 20.470556\n",
      "batch_idx:  6000, valid_loss: 20.119358\n",
      "Training Loss: 0.627061 \tValidation Loss: 19.273396\n",
      "epoch:   346\n",
      "batch_idx:     0, train_loss: 11.949759\n",
      "batch_idx:  1000, train_loss: 0.693627\n",
      "batch_idx:  2000, train_loss: 0.859924\n",
      "batch_idx:  3000, train_loss: 0.853183\n",
      "batch_idx:  4000, train_loss: 0.959625\n",
      "batch_idx:  5000, train_loss: 0.946934\n",
      "batch_idx:  6000, train_loss: 0.956686\n",
      "final train_loss: 0.979598\n",
      "batch_idx:     0, valid_loss: 12.203316\n",
      "batch_idx:  1000, valid_loss: 17.232555\n",
      "batch_idx:  2000, valid_loss: 19.140785\n",
      "batch_idx:  3000, valid_loss: 19.535952\n",
      "batch_idx:  4000, valid_loss: 19.485743\n",
      "batch_idx:  5000, valid_loss: 19.444700\n",
      "batch_idx:  6000, valid_loss: 18.924694\n",
      "Training Loss: 0.979598 \tValidation Loss: 18.155439\n",
      "epoch:   347\n",
      "batch_idx:     0, train_loss: 15.779277\n",
      "batch_idx:  1000, train_loss: 1.160876\n",
      "batch_idx:  2000, train_loss: 1.328576\n",
      "batch_idx:  3000, train_loss: 1.220387\n",
      "batch_idx:  4000, train_loss: 1.106761\n",
      "batch_idx:  5000, train_loss: 1.091977\n",
      "batch_idx:  6000, train_loss: 1.086803\n",
      "final train_loss: 1.174629\n",
      "batch_idx:     0, valid_loss: 9.462761\n",
      "batch_idx:  1000, valid_loss: 9.662318\n",
      "batch_idx:  2000, valid_loss: 9.418953\n",
      "batch_idx:  3000, valid_loss: 9.409572\n",
      "batch_idx:  4000, valid_loss: 9.423250\n",
      "batch_idx:  5000, valid_loss: 9.465631\n",
      "batch_idx:  6000, valid_loss: 9.574288\n",
      "Training Loss: 1.174629 \tValidation Loss: 9.373528\n",
      "epoch:   348\n",
      "batch_idx:     0, train_loss: 13.062311\n",
      "batch_idx:  1000, train_loss: 1.578655\n",
      "batch_idx:  2000, train_loss: 1.461786\n",
      "batch_idx:  3000, train_loss: 1.459653\n",
      "batch_idx:  4000, train_loss: 1.416065\n",
      "batch_idx:  5000, train_loss: 1.351214\n",
      "batch_idx:  6000, train_loss: 1.312835\n",
      "final train_loss: 1.297857\n",
      "batch_idx:     0, valid_loss: 12.530087\n",
      "batch_idx:  1000, valid_loss: 12.720718\n",
      "batch_idx:  2000, valid_loss: 13.520897\n",
      "batch_idx:  3000, valid_loss: 13.705501\n",
      "batch_idx:  4000, valid_loss: 13.788515\n",
      "batch_idx:  5000, valid_loss: 13.960937\n",
      "batch_idx:  6000, valid_loss: 13.850727\n",
      "Training Loss: 1.297857 \tValidation Loss: 13.396822\n",
      "epoch:   349\n",
      "batch_idx:     0, train_loss: 15.497224\n",
      "batch_idx:  1000, train_loss: 1.052833\n",
      "batch_idx:  2000, train_loss: 1.294849\n",
      "batch_idx:  3000, train_loss: 1.179163\n",
      "batch_idx:  4000, train_loss: 1.206842\n",
      "batch_idx:  5000, train_loss: 1.144909\n",
      "batch_idx:  6000, train_loss: 1.176653\n",
      "final train_loss: 1.223530\n",
      "batch_idx:     0, valid_loss: 9.006760\n",
      "batch_idx:  1000, valid_loss: 9.432302\n",
      "batch_idx:  2000, valid_loss: 9.447408\n",
      "batch_idx:  3000, valid_loss: 9.433698\n",
      "batch_idx:  4000, valid_loss: 9.469575\n",
      "batch_idx:  5000, valid_loss: 9.516212\n",
      "batch_idx:  6000, valid_loss: 9.297904\n",
      "Training Loss: 1.223530 \tValidation Loss: 9.011060\n",
      "epoch:   350\n",
      "batch_idx:     0, train_loss: 12.222056\n",
      "batch_idx:  1000, train_loss: 1.726109\n",
      "batch_idx:  2000, train_loss: 1.872109\n",
      "batch_idx:  3000, train_loss: 1.760028\n",
      "batch_idx:  4000, train_loss: 1.638188\n",
      "batch_idx:  5000, train_loss: 1.439003\n",
      "batch_idx:  6000, train_loss: 1.346173\n",
      "final train_loss: 1.338143\n",
      "batch_idx:     0, valid_loss: 8.791500\n",
      "batch_idx:  1000, valid_loss: 9.899545\n",
      "batch_idx:  2000, valid_loss: 10.816080\n",
      "batch_idx:  3000, valid_loss: 11.092776\n",
      "batch_idx:  4000, valid_loss: 11.129122\n",
      "batch_idx:  5000, valid_loss: 11.148033\n",
      "batch_idx:  6000, valid_loss: 10.945808\n",
      "Training Loss: 1.338143 \tValidation Loss: 10.484208\n",
      "epoch:   351\n",
      "batch_idx:     0, train_loss: 11.219067\n",
      "batch_idx:  1000, train_loss: 1.955331\n",
      "batch_idx:  2000, train_loss: 1.784933\n",
      "batch_idx:  3000, train_loss: 1.665967\n",
      "batch_idx:  4000, train_loss: 1.631947\n",
      "batch_idx:  5000, train_loss: 1.528684\n",
      "batch_idx:  6000, train_loss: 1.488095\n",
      "final train_loss: 1.497294\n",
      "batch_idx:     0, valid_loss: 9.693475\n",
      "batch_idx:  1000, valid_loss: 8.545256\n",
      "batch_idx:  2000, valid_loss: 11.004520\n",
      "batch_idx:  3000, valid_loss: 12.163394\n",
      "batch_idx:  4000, valid_loss: 12.616076\n",
      "batch_idx:  5000, valid_loss: 12.839336\n",
      "batch_idx:  6000, valid_loss: 12.772913\n",
      "Training Loss: 1.497294 \tValidation Loss: 12.371882\n",
      "epoch:   352\n",
      "batch_idx:     0, train_loss: 10.958199\n",
      "batch_idx:  1000, train_loss: 1.121301\n",
      "batch_idx:  2000, train_loss: 1.246429\n",
      "batch_idx:  3000, train_loss: 0.997839\n",
      "batch_idx:  4000, train_loss: 0.955072\n",
      "batch_idx:  5000, train_loss: 0.912382\n",
      "batch_idx:  6000, train_loss: 0.963147\n",
      "final train_loss: 1.033631\n",
      "batch_idx:     0, valid_loss: 7.655994\n",
      "batch_idx:  1000, valid_loss: 9.008545\n",
      "batch_idx:  2000, valid_loss: 10.938407\n",
      "batch_idx:  3000, valid_loss: 11.410172\n",
      "batch_idx:  4000, valid_loss: 11.477912\n",
      "batch_idx:  5000, valid_loss: 11.482622\n",
      "batch_idx:  6000, valid_loss: 11.402827\n",
      "Training Loss: 1.033631 \tValidation Loss: 10.974518\n",
      "epoch:   353\n",
      "batch_idx:     0, train_loss: 4.850960\n",
      "batch_idx:  1000, train_loss: 1.685281\n",
      "batch_idx:  2000, train_loss: 1.453510\n",
      "batch_idx:  3000, train_loss: 1.223355\n",
      "batch_idx:  4000, train_loss: 1.092203\n",
      "batch_idx:  5000, train_loss: 1.031998\n",
      "batch_idx:  6000, train_loss: 1.119037\n",
      "final train_loss: 1.166136\n",
      "batch_idx:     0, valid_loss: 9.971404\n",
      "batch_idx:  1000, valid_loss: 10.911107\n",
      "batch_idx:  2000, valid_loss: 10.868962\n",
      "batch_idx:  3000, valid_loss: 10.847691\n",
      "batch_idx:  4000, valid_loss: 10.895710\n",
      "batch_idx:  5000, valid_loss: 10.965416\n",
      "batch_idx:  6000, valid_loss: 10.820891\n",
      "Training Loss: 1.166136 \tValidation Loss: 10.460353\n",
      "epoch:   354\n",
      "batch_idx:     0, train_loss: 11.763042\n",
      "batch_idx:  1000, train_loss: 1.548291\n",
      "batch_idx:  2000, train_loss: 1.467664\n",
      "batch_idx:  3000, train_loss: 1.327632\n",
      "batch_idx:  4000, train_loss: 1.158880\n",
      "batch_idx:  5000, train_loss: 1.101667\n",
      "batch_idx:  6000, train_loss: 1.112131\n",
      "final train_loss: 1.144979\n",
      "batch_idx:     0, valid_loss: 12.257404\n",
      "batch_idx:  1000, valid_loss: 12.455598\n",
      "batch_idx:  2000, valid_loss: 11.996130\n",
      "batch_idx:  3000, valid_loss: 11.660767\n",
      "batch_idx:  4000, valid_loss: 11.361427\n",
      "batch_idx:  5000, valid_loss: 11.175042\n",
      "batch_idx:  6000, valid_loss: 10.773660\n",
      "Training Loss: 1.144979 \tValidation Loss: 10.262576\n",
      "epoch:   355\n",
      "batch_idx:     0, train_loss: 14.677444\n",
      "batch_idx:  1000, train_loss: 1.310984\n",
      "batch_idx:  2000, train_loss: 1.307615\n",
      "batch_idx:  3000, train_loss: 1.213338\n",
      "batch_idx:  4000, train_loss: 0.982993\n",
      "batch_idx:  5000, train_loss: 0.848019\n",
      "batch_idx:  6000, train_loss: 0.775085\n",
      "final train_loss: 0.744955\n",
      "batch_idx:     0, valid_loss: 6.210396\n",
      "batch_idx:  1000, valid_loss: 16.261448\n",
      "batch_idx:  2000, valid_loss: 22.346401\n",
      "batch_idx:  3000, valid_loss: 25.905987\n",
      "batch_idx:  4000, valid_loss: 27.607243\n",
      "batch_idx:  5000, valid_loss: 28.419825\n",
      "batch_idx:  6000, valid_loss: 27.949570\n",
      "Training Loss: 0.744955 \tValidation Loss: 26.463900\n",
      "epoch:   356\n",
      "batch_idx:     0, train_loss: 2.984865\n",
      "batch_idx:  1000, train_loss: 0.336725\n",
      "batch_idx:  2000, train_loss: 0.368132\n",
      "batch_idx:  3000, train_loss: 0.468481\n",
      "batch_idx:  4000, train_loss: 0.455396\n",
      "batch_idx:  5000, train_loss: 0.493192\n",
      "batch_idx:  6000, train_loss: 0.551202\n",
      "final train_loss: 0.616091\n",
      "batch_idx:     0, valid_loss: 10.943226\n",
      "batch_idx:  1000, valid_loss: 13.795353\n",
      "batch_idx:  2000, valid_loss: 15.155679\n",
      "batch_idx:  3000, valid_loss: 15.530976\n",
      "batch_idx:  4000, valid_loss: 15.634116\n",
      "batch_idx:  5000, valid_loss: 15.705064\n",
      "batch_idx:  6000, valid_loss: 15.378623\n",
      "Training Loss: 0.616091 \tValidation Loss: 14.783718\n",
      "epoch:   357\n",
      "batch_idx:     0, train_loss: 14.412018\n",
      "batch_idx:  1000, train_loss: 1.101595\n",
      "batch_idx:  2000, train_loss: 1.408261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  3000, train_loss: 1.346431\n",
      "batch_idx:  4000, train_loss: 1.237223\n",
      "batch_idx:  5000, train_loss: 1.194858\n",
      "batch_idx:  6000, train_loss: 1.239250\n",
      "final train_loss: 1.274354\n",
      "batch_idx:     0, valid_loss: 7.195230\n",
      "batch_idx:  1000, valid_loss: 10.715956\n",
      "batch_idx:  2000, valid_loss: 11.142427\n",
      "batch_idx:  3000, valid_loss: 11.292014\n",
      "batch_idx:  4000, valid_loss: 11.346212\n",
      "batch_idx:  5000, valid_loss: 11.352058\n",
      "batch_idx:  6000, valid_loss: 11.002660\n",
      "Training Loss: 1.274354 \tValidation Loss: 10.527892\n",
      "epoch:   358\n",
      "batch_idx:     0, train_loss: 10.238842\n",
      "batch_idx:  1000, train_loss: 2.053955\n",
      "batch_idx:  2000, train_loss: 2.322799\n",
      "batch_idx:  3000, train_loss: 2.262905\n",
      "batch_idx:  4000, train_loss: 2.149853\n",
      "batch_idx:  5000, train_loss: 2.065186\n",
      "batch_idx:  6000, train_loss: 2.064979\n",
      "final train_loss: 2.129542\n",
      "batch_idx:     0, valid_loss: 4.726344\n",
      "batch_idx:  1000, valid_loss: 4.808963\n",
      "batch_idx:  2000, valid_loss: 4.767081\n",
      "batch_idx:  3000, valid_loss: 4.805691\n",
      "batch_idx:  4000, valid_loss: 4.844233\n",
      "batch_idx:  5000, valid_loss: 4.901883\n",
      "batch_idx:  6000, valid_loss: 4.923178\n",
      "Training Loss: 2.129542 \tValidation Loss: 4.899016\n",
      "epoch:   359\n",
      "batch_idx:     0, train_loss: 4.543571\n",
      "batch_idx:  1000, train_loss: 2.422340\n",
      "batch_idx:  2000, train_loss: 2.358579\n",
      "batch_idx:  3000, train_loss: 2.228197\n",
      "batch_idx:  4000, train_loss: 2.158145\n",
      "batch_idx:  5000, train_loss: 2.164873\n",
      "batch_idx:  6000, train_loss: 2.104609\n",
      "final train_loss: 2.252761\n",
      "batch_idx:     0, valid_loss: 6.426572\n",
      "batch_idx:  1000, valid_loss: 6.109227\n",
      "batch_idx:  2000, valid_loss: 5.986700\n",
      "batch_idx:  3000, valid_loss: 5.957530\n",
      "batch_idx:  4000, valid_loss: 5.959618\n",
      "batch_idx:  5000, valid_loss: 5.920227\n",
      "batch_idx:  6000, valid_loss: 5.755959\n",
      "Training Loss: 2.252761 \tValidation Loss: 5.546352\n",
      "epoch:   360\n",
      "batch_idx:     0, train_loss: 4.442855\n",
      "batch_idx:  1000, train_loss: 2.100829\n",
      "batch_idx:  2000, train_loss: 1.918305\n",
      "batch_idx:  3000, train_loss: 1.763477\n",
      "batch_idx:  4000, train_loss: 1.887120\n",
      "batch_idx:  5000, train_loss: 1.836594\n",
      "batch_idx:  6000, train_loss: 1.869312\n",
      "final train_loss: 1.935716\n",
      "batch_idx:     0, valid_loss: 9.795341\n",
      "batch_idx:  1000, valid_loss: 9.821078\n",
      "batch_idx:  2000, valid_loss: 9.801572\n",
      "batch_idx:  3000, valid_loss: 9.804969\n",
      "batch_idx:  4000, valid_loss: 9.639071\n",
      "batch_idx:  5000, valid_loss: 9.564239\n",
      "batch_idx:  6000, valid_loss: 9.317311\n",
      "Training Loss: 1.935716 \tValidation Loss: 8.910625\n",
      "epoch:   361\n",
      "batch_idx:     0, train_loss: 11.869903\n",
      "batch_idx:  1000, train_loss: 1.790379\n",
      "batch_idx:  2000, train_loss: 1.834991\n",
      "batch_idx:  3000, train_loss: 1.799174\n",
      "batch_idx:  4000, train_loss: 1.891375\n",
      "batch_idx:  5000, train_loss: 1.989752\n",
      "batch_idx:  6000, train_loss: 1.913573\n",
      "final train_loss: 2.054514\n",
      "batch_idx:     0, valid_loss: 8.398703\n",
      "batch_idx:  1000, valid_loss: 9.238523\n",
      "batch_idx:  2000, valid_loss: 9.269058\n",
      "batch_idx:  3000, valid_loss: 9.274532\n",
      "batch_idx:  4000, valid_loss: 9.186565\n",
      "batch_idx:  5000, valid_loss: 9.005448\n",
      "batch_idx:  6000, valid_loss: 8.677919\n",
      "Training Loss: 2.054514 \tValidation Loss: 8.299866\n",
      "epoch:   362\n",
      "batch_idx:     0, train_loss: 10.986078\n",
      "batch_idx:  1000, train_loss: 2.459005\n",
      "batch_idx:  2000, train_loss: 2.147710\n",
      "batch_idx:  3000, train_loss: 2.986300\n",
      "batch_idx:  4000, train_loss: 3.437279\n",
      "batch_idx:  5000, train_loss: 3.043559\n",
      "batch_idx:  6000, train_loss: 2.899495\n",
      "final train_loss: 2.850661\n",
      "batch_idx:     0, valid_loss: 9.777502\n",
      "batch_idx:  1000, valid_loss: 10.446973\n",
      "batch_idx:  2000, valid_loss: 10.578582\n",
      "batch_idx:  3000, valid_loss: 10.435612\n",
      "batch_idx:  4000, valid_loss: 10.280565\n",
      "batch_idx:  5000, valid_loss: 10.085723\n",
      "batch_idx:  6000, valid_loss: 9.795517\n",
      "Training Loss: 2.850661 \tValidation Loss: 9.433425\n",
      "epoch:   363\n",
      "batch_idx:     0, train_loss: 4.512728\n",
      "batch_idx:  1000, train_loss: 2.196450\n",
      "batch_idx:  2000, train_loss: 2.404457\n",
      "batch_idx:  3000, train_loss: 2.099136\n",
      "batch_idx:  4000, train_loss: 2.059937\n",
      "batch_idx:  5000, train_loss: 1.904576\n",
      "batch_idx:  6000, train_loss: 1.884653\n",
      "final train_loss: 1.929343\n",
      "batch_idx:     0, valid_loss: 8.319884\n",
      "batch_idx:  1000, valid_loss: 9.767894\n",
      "batch_idx:  2000, valid_loss: 9.826327\n",
      "batch_idx:  3000, valid_loss: 9.812349\n",
      "batch_idx:  4000, valid_loss: 9.711346\n",
      "batch_idx:  5000, valid_loss: 9.519274\n",
      "batch_idx:  6000, valid_loss: 9.260211\n",
      "Training Loss: 1.929343 \tValidation Loss: 8.924419\n",
      "epoch:   364\n",
      "batch_idx:     0, train_loss: 12.304907\n",
      "batch_idx:  1000, train_loss: 2.110580\n",
      "batch_idx:  2000, train_loss: 1.900213\n",
      "batch_idx:  3000, train_loss: 1.862883\n",
      "batch_idx:  4000, train_loss: 1.750738\n",
      "batch_idx:  5000, train_loss: 1.910849\n",
      "batch_idx:  6000, train_loss: 2.048589\n",
      "final train_loss: 2.165510\n",
      "batch_idx:     0, valid_loss: 9.003947\n",
      "batch_idx:  1000, valid_loss: 10.456301\n",
      "batch_idx:  2000, valid_loss: 10.282446\n",
      "batch_idx:  3000, valid_loss: 9.951351\n",
      "batch_idx:  4000, valid_loss: 9.717565\n",
      "batch_idx:  5000, valid_loss: 9.420294\n",
      "batch_idx:  6000, valid_loss: 9.082477\n",
      "Training Loss: 2.165510 \tValidation Loss: 8.693357\n",
      "epoch:   365\n",
      "batch_idx:     0, train_loss: 11.148832\n",
      "batch_idx:  1000, train_loss: 2.970190\n",
      "batch_idx:  2000, train_loss: 2.541248\n",
      "batch_idx:  3000, train_loss: 2.172640\n",
      "batch_idx:  4000, train_loss: 1.836015\n",
      "batch_idx:  5000, train_loss: 1.605749\n",
      "batch_idx:  6000, train_loss: 1.653718\n",
      "final train_loss: 1.667809\n",
      "batch_idx:     0, valid_loss: 9.509924\n",
      "batch_idx:  1000, valid_loss: 10.186794\n",
      "batch_idx:  2000, valid_loss: 10.182253\n",
      "batch_idx:  3000, valid_loss: 10.191991\n",
      "batch_idx:  4000, valid_loss: 10.126699\n",
      "batch_idx:  5000, valid_loss: 10.070281\n",
      "batch_idx:  6000, valid_loss: 9.814793\n",
      "Training Loss: 1.667809 \tValidation Loss: 9.376026\n",
      "epoch:   366\n",
      "batch_idx:     0, train_loss: 12.443382\n",
      "batch_idx:  1000, train_loss: 2.446873\n",
      "batch_idx:  2000, train_loss: 2.275985\n",
      "batch_idx:  3000, train_loss: 2.087102\n",
      "batch_idx:  4000, train_loss: 1.997218\n",
      "batch_idx:  5000, train_loss: 2.068714\n",
      "batch_idx:  6000, train_loss: 2.183759\n",
      "final train_loss: 2.296239\n",
      "batch_idx:     0, valid_loss: 7.874457\n",
      "batch_idx:  1000, valid_loss: 8.606984\n",
      "batch_idx:  2000, valid_loss: 8.496895\n",
      "batch_idx:  3000, valid_loss: 8.383109\n",
      "batch_idx:  4000, valid_loss: 8.222051\n",
      "batch_idx:  5000, valid_loss: 8.020474\n",
      "batch_idx:  6000, valid_loss: 7.757400\n",
      "Training Loss: 2.296239 \tValidation Loss: 7.434272\n",
      "epoch:   367\n",
      "batch_idx:     0, train_loss: 9.598111\n",
      "batch_idx:  1000, train_loss: 1.672799\n",
      "batch_idx:  2000, train_loss: 2.014315\n",
      "batch_idx:  3000, train_loss: 1.859344\n",
      "batch_idx:  4000, train_loss: 1.707396\n",
      "batch_idx:  5000, train_loss: 1.666498\n",
      "batch_idx:  6000, train_loss: 1.695641\n",
      "final train_loss: 1.764181\n",
      "batch_idx:     0, valid_loss: 9.364952\n",
      "batch_idx:  1000, valid_loss: 10.250807\n",
      "batch_idx:  2000, valid_loss: 10.237443\n",
      "batch_idx:  3000, valid_loss: 10.276830\n",
      "batch_idx:  4000, valid_loss: 10.243318\n",
      "batch_idx:  5000, valid_loss: 10.076783\n",
      "batch_idx:  6000, valid_loss: 9.757956\n",
      "Training Loss: 1.764181 \tValidation Loss: 9.388905\n",
      "epoch:   368\n",
      "batch_idx:     0, train_loss: 4.419541\n",
      "batch_idx:  1000, train_loss: 2.016766\n",
      "batch_idx:  2000, train_loss: 1.711898\n",
      "batch_idx:  3000, train_loss: 1.577423\n",
      "batch_idx:  4000, train_loss: 1.435513\n",
      "batch_idx:  5000, train_loss: 1.256909\n",
      "batch_idx:  6000, train_loss: 1.240282\n",
      "final train_loss: 1.368764\n",
      "batch_idx:     0, valid_loss: 8.440328\n",
      "batch_idx:  1000, valid_loss: 8.951057\n",
      "batch_idx:  2000, valid_loss: 9.067000\n",
      "batch_idx:  3000, valid_loss: 9.135122\n",
      "batch_idx:  4000, valid_loss: 9.093802\n",
      "batch_idx:  5000, valid_loss: 9.034833\n",
      "batch_idx:  6000, valid_loss: 8.909856\n",
      "Training Loss: 1.368764 \tValidation Loss: 8.530444\n",
      "epoch:   369\n",
      "batch_idx:     0, train_loss: 4.424835\n",
      "batch_idx:  1000, train_loss: 2.400706\n",
      "batch_idx:  2000, train_loss: 2.148888\n",
      "batch_idx:  3000, train_loss: 1.788285\n",
      "batch_idx:  4000, train_loss: 1.634562\n",
      "batch_idx:  5000, train_loss: 1.595025\n",
      "batch_idx:  6000, train_loss: 1.653330\n",
      "final train_loss: 1.742900\n",
      "batch_idx:     0, valid_loss: 9.657555\n",
      "batch_idx:  1000, valid_loss: 10.283946\n",
      "batch_idx:  2000, valid_loss: 10.345971\n",
      "batch_idx:  3000, valid_loss: 10.409518\n",
      "batch_idx:  4000, valid_loss: 10.350090\n",
      "batch_idx:  5000, valid_loss: 10.178555\n",
      "batch_idx:  6000, valid_loss: 9.866440\n",
      "Training Loss: 1.742900 \tValidation Loss: 9.489726\n",
      "epoch:   370\n",
      "batch_idx:     0, train_loss: 11.929237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  1000, train_loss: 2.387457\n",
      "batch_idx:  2000, train_loss: 2.465652\n",
      "batch_idx:  3000, train_loss: 2.146252\n",
      "batch_idx:  4000, train_loss: 1.906412\n",
      "batch_idx:  5000, train_loss: 1.962906\n",
      "batch_idx:  6000, train_loss: 1.970076\n",
      "final train_loss: 2.041450\n",
      "batch_idx:     0, valid_loss: 11.031732\n",
      "batch_idx:  1000, valid_loss: 9.638285\n",
      "batch_idx:  2000, valid_loss: 9.569582\n",
      "batch_idx:  3000, valid_loss: 9.673795\n",
      "batch_idx:  4000, valid_loss: 9.628314\n",
      "batch_idx:  5000, valid_loss: 9.474266\n",
      "batch_idx:  6000, valid_loss: 9.181670\n",
      "Training Loss: 2.041450 \tValidation Loss: 8.824373\n",
      "epoch:   371\n",
      "batch_idx:     0, train_loss: 4.510154\n",
      "batch_idx:  1000, train_loss: 2.333820\n",
      "batch_idx:  2000, train_loss: 2.009630\n",
      "batch_idx:  3000, train_loss: 1.764990\n",
      "batch_idx:  4000, train_loss: 1.602908\n",
      "batch_idx:  5000, train_loss: 1.544335\n",
      "batch_idx:  6000, train_loss: 1.691686\n",
      "final train_loss: 1.846253\n",
      "batch_idx:     0, valid_loss: 8.435823\n",
      "batch_idx:  1000, valid_loss: 8.351549\n",
      "batch_idx:  2000, valid_loss: 8.196429\n",
      "batch_idx:  3000, valid_loss: 8.042473\n",
      "batch_idx:  4000, valid_loss: 7.948046\n",
      "batch_idx:  5000, valid_loss: 7.811697\n",
      "batch_idx:  6000, valid_loss: 7.558581\n",
      "Training Loss: 1.846253 \tValidation Loss: 7.238238\n",
      "epoch:   372\n",
      "batch_idx:     0, train_loss: 4.523245\n",
      "batch_idx:  1000, train_loss: 3.069703\n",
      "batch_idx:  2000, train_loss: 2.461240\n",
      "batch_idx:  3000, train_loss: 2.170921\n",
      "batch_idx:  4000, train_loss: 2.104891\n",
      "batch_idx:  5000, train_loss: 2.159747\n",
      "batch_idx:  6000, train_loss: 2.161080\n",
      "final train_loss: 2.181573\n",
      "batch_idx:     0, valid_loss: 8.657475\n",
      "batch_idx:  1000, valid_loss: 9.552272\n",
      "batch_idx:  2000, valid_loss: 9.770035\n",
      "batch_idx:  3000, valid_loss: 9.953326\n",
      "batch_idx:  4000, valid_loss: 10.141515\n",
      "batch_idx:  5000, valid_loss: 10.259712\n",
      "batch_idx:  6000, valid_loss: 9.995070\n",
      "Training Loss: 2.181573 \tValidation Loss: 9.602867\n",
      "epoch:   373\n",
      "batch_idx:     0, train_loss: 11.697814\n",
      "batch_idx:  1000, train_loss: 1.927447\n",
      "batch_idx:  2000, train_loss: 1.905213\n",
      "batch_idx:  3000, train_loss: 1.908669\n",
      "batch_idx:  4000, train_loss: 1.887037\n",
      "batch_idx:  5000, train_loss: 1.849657\n",
      "batch_idx:  6000, train_loss: 1.890103\n",
      "final train_loss: 1.917885\n",
      "batch_idx:     0, valid_loss: 9.165363\n",
      "batch_idx:  1000, valid_loss: 9.942923\n",
      "batch_idx:  2000, valid_loss: 10.046496\n",
      "batch_idx:  3000, valid_loss: 10.141764\n",
      "batch_idx:  4000, valid_loss: 10.216903\n",
      "batch_idx:  5000, valid_loss: 10.332052\n",
      "batch_idx:  6000, valid_loss: 10.105894\n",
      "Training Loss: 1.917885 \tValidation Loss: 9.734278\n",
      "epoch:   374\n",
      "batch_idx:     0, train_loss: 4.481518\n",
      "batch_idx:  1000, train_loss: 1.905022\n",
      "batch_idx:  2000, train_loss: 1.882992\n",
      "batch_idx:  3000, train_loss: 2.017821\n",
      "batch_idx:  4000, train_loss: 1.778995\n",
      "batch_idx:  5000, train_loss: 1.725929\n",
      "batch_idx:  6000, train_loss: 1.777772\n",
      "final train_loss: 1.831597\n",
      "batch_idx:     0, valid_loss: 7.189219\n",
      "batch_idx:  1000, valid_loss: 9.508859\n",
      "batch_idx:  2000, valid_loss: 10.177135\n",
      "batch_idx:  3000, valid_loss: 10.469543\n",
      "batch_idx:  4000, valid_loss: 10.725513\n",
      "batch_idx:  5000, valid_loss: 10.881611\n",
      "batch_idx:  6000, valid_loss: 10.868620\n",
      "Training Loss: 1.831597 \tValidation Loss: 10.523234\n",
      "epoch:   375\n",
      "batch_idx:     0, train_loss: 8.522278\n",
      "batch_idx:  1000, train_loss: 1.830038\n",
      "batch_idx:  2000, train_loss: 2.086319\n",
      "batch_idx:  3000, train_loss: 2.233600\n",
      "batch_idx:  4000, train_loss: 2.331466\n",
      "batch_idx:  5000, train_loss: 2.433550\n",
      "batch_idx:  6000, train_loss: 2.465522\n",
      "final train_loss: 2.618089\n",
      "batch_idx:     0, valid_loss: 5.008625\n",
      "batch_idx:  1000, valid_loss: 4.827208\n",
      "batch_idx:  2000, valid_loss: 4.873895\n",
      "batch_idx:  3000, valid_loss: 4.941603\n",
      "batch_idx:  4000, valid_loss: 5.034849\n",
      "batch_idx:  5000, valid_loss: 5.053426\n",
      "batch_idx:  6000, valid_loss: 5.025661\n",
      "Training Loss: 2.618089 \tValidation Loss: 4.954786\n",
      "epoch:   376\n",
      "batch_idx:     0, train_loss: 5.066580\n",
      "batch_idx:  1000, train_loss: 2.682868\n",
      "batch_idx:  2000, train_loss: 2.428043\n",
      "batch_idx:  3000, train_loss: 2.418825\n",
      "batch_idx:  4000, train_loss: 2.446845\n",
      "batch_idx:  5000, train_loss: 2.479054\n",
      "batch_idx:  6000, train_loss: 2.468198\n",
      "final train_loss: 2.438755\n",
      "batch_idx:     0, valid_loss: 8.331856\n",
      "batch_idx:  1000, valid_loss: 10.129825\n",
      "batch_idx:  2000, valid_loss: 10.326170\n",
      "batch_idx:  3000, valid_loss: 10.364425\n",
      "batch_idx:  4000, valid_loss: 10.370034\n",
      "batch_idx:  5000, valid_loss: 10.235087\n",
      "batch_idx:  6000, valid_loss: 9.912722\n",
      "Training Loss: 2.438755 \tValidation Loss: 9.488764\n",
      "epoch:   377\n",
      "batch_idx:     0, train_loss: 11.800972\n",
      "batch_idx:  1000, train_loss: 2.552803\n",
      "batch_idx:  2000, train_loss: 2.858330\n",
      "batch_idx:  3000, train_loss: 2.483012\n",
      "batch_idx:  4000, train_loss: 2.156135\n",
      "batch_idx:  5000, train_loss: 1.892441\n",
      "batch_idx:  6000, train_loss: 1.835405\n",
      "final train_loss: 1.849738\n",
      "batch_idx:     0, valid_loss: 9.836780\n",
      "batch_idx:  1000, valid_loss: 9.702122\n",
      "batch_idx:  2000, valid_loss: 9.844911\n",
      "batch_idx:  3000, valid_loss: 9.903563\n",
      "batch_idx:  4000, valid_loss: 10.065681\n",
      "batch_idx:  5000, valid_loss: 9.934879\n",
      "batch_idx:  6000, valid_loss: 9.968718\n",
      "Training Loss: 1.849738 \tValidation Loss: 9.635892\n",
      "epoch:   378\n",
      "batch_idx:     0, train_loss: 11.258938\n",
      "batch_idx:  1000, train_loss: 1.699883\n",
      "batch_idx:  2000, train_loss: 1.669143\n",
      "batch_idx:  3000, train_loss: 1.572165\n",
      "batch_idx:  4000, train_loss: 1.645158\n",
      "batch_idx:  5000, train_loss: 1.657668\n",
      "batch_idx:  6000, train_loss: 1.656795\n",
      "final train_loss: 1.745020\n",
      "batch_idx:     0, valid_loss: 8.446624\n",
      "batch_idx:  1000, valid_loss: 8.569183\n",
      "batch_idx:  2000, valid_loss: 8.732448\n",
      "batch_idx:  3000, valid_loss: 8.829905\n",
      "batch_idx:  4000, valid_loss: 8.939271\n",
      "batch_idx:  5000, valid_loss: 8.973991\n",
      "batch_idx:  6000, valid_loss: 8.742477\n",
      "Training Loss: 1.745020 \tValidation Loss: 8.347483\n",
      "epoch:   379\n",
      "batch_idx:     0, train_loss: 9.895341\n",
      "batch_idx:  1000, train_loss: 2.126961\n",
      "batch_idx:  2000, train_loss: 1.964690\n",
      "batch_idx:  3000, train_loss: 2.144444\n",
      "batch_idx:  4000, train_loss: 2.259251\n",
      "batch_idx:  5000, train_loss: 2.193762\n",
      "batch_idx:  6000, train_loss: 2.216032\n",
      "final train_loss: 2.294205\n",
      "batch_idx:     0, valid_loss: 8.769535\n",
      "batch_idx:  1000, valid_loss: 9.847008\n",
      "batch_idx:  2000, valid_loss: 9.769957\n",
      "batch_idx:  3000, valid_loss: 9.568028\n",
      "batch_idx:  4000, valid_loss: 9.377959\n",
      "batch_idx:  5000, valid_loss: 9.145993\n",
      "batch_idx:  6000, valid_loss: 8.849780\n",
      "Training Loss: 2.294205 \tValidation Loss: 8.485014\n",
      "epoch:   380\n",
      "batch_idx:     0, train_loss: 10.899759\n",
      "batch_idx:  1000, train_loss: 2.281029\n",
      "batch_idx:  2000, train_loss: 2.687031\n",
      "batch_idx:  3000, train_loss: 2.616942\n",
      "batch_idx:  4000, train_loss: 2.362388\n",
      "batch_idx:  5000, train_loss: 2.254984\n",
      "batch_idx:  6000, train_loss: 2.401975\n",
      "final train_loss: 2.417407\n",
      "batch_idx:     0, valid_loss: 8.143435\n",
      "batch_idx:  1000, valid_loss: 9.766764\n",
      "batch_idx:  2000, valid_loss: 11.245084\n",
      "batch_idx:  3000, valid_loss: 11.494365\n",
      "batch_idx:  4000, valid_loss: 11.677295\n",
      "batch_idx:  5000, valid_loss: 11.704473\n",
      "batch_idx:  6000, valid_loss: 11.518875\n",
      "Training Loss: 2.417407 \tValidation Loss: 11.143332\n",
      "epoch:   381\n",
      "batch_idx:     0, train_loss: 9.693970\n",
      "batch_idx:  1000, train_loss: 1.817075\n",
      "batch_idx:  2000, train_loss: 1.993419\n",
      "batch_idx:  3000, train_loss: 1.960866\n",
      "batch_idx:  4000, train_loss: 2.131009\n",
      "batch_idx:  5000, train_loss: 2.125631\n",
      "batch_idx:  6000, train_loss: 2.007981\n",
      "final train_loss: 2.145892\n",
      "batch_idx:     0, valid_loss: 4.548599\n",
      "batch_idx:  1000, valid_loss: 4.568097\n",
      "batch_idx:  2000, valid_loss: 4.621150\n",
      "batch_idx:  3000, valid_loss: 4.696146\n",
      "batch_idx:  4000, valid_loss: 4.793917\n",
      "batch_idx:  5000, valid_loss: 4.856288\n",
      "batch_idx:  6000, valid_loss: 4.899662\n",
      "Training Loss: 2.145892 \tValidation Loss: 4.893111\n",
      "epoch:   382\n",
      "batch_idx:     0, train_loss: 4.601374\n",
      "batch_idx:  1000, train_loss: 2.196437\n",
      "batch_idx:  2000, train_loss: 3.337539\n",
      "batch_idx:  3000, train_loss: 2.841543\n",
      "batch_idx:  4000, train_loss: 2.790760\n",
      "batch_idx:  5000, train_loss: 2.813995\n",
      "batch_idx:  6000, train_loss: 2.816217\n",
      "final train_loss: 2.823423\n",
      "batch_idx:     0, valid_loss: 6.021168\n",
      "batch_idx:  1000, valid_loss: 5.764174\n",
      "batch_idx:  2000, valid_loss: 7.136839\n",
      "batch_idx:  3000, valid_loss: 7.605518\n",
      "batch_idx:  4000, valid_loss: 7.880415\n",
      "batch_idx:  5000, valid_loss: 7.842192\n",
      "batch_idx:  6000, valid_loss: 7.633143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.823423 \tValidation Loss: 7.319324\n",
      "epoch:   383\n",
      "batch_idx:     0, train_loss: 5.759406\n",
      "batch_idx:  1000, train_loss: 1.642974\n",
      "batch_idx:  2000, train_loss: 2.940823\n",
      "batch_idx:  3000, train_loss: 3.573613\n",
      "batch_idx:  4000, train_loss: 3.933125\n",
      "batch_idx:  5000, train_loss: 3.830971\n",
      "batch_idx:  6000, train_loss: 3.497968\n",
      "final train_loss: 3.378896\n",
      "batch_idx:     0, valid_loss: 7.777269\n",
      "batch_idx:  1000, valid_loss: 9.477156\n",
      "batch_idx:  2000, valid_loss: 9.490541\n",
      "batch_idx:  3000, valid_loss: 9.490710\n",
      "batch_idx:  4000, valid_loss: 9.530851\n",
      "batch_idx:  5000, valid_loss: 9.450625\n",
      "batch_idx:  6000, valid_loss: 9.187299\n",
      "Training Loss: 3.378896 \tValidation Loss: 8.849090\n",
      "epoch:   384\n",
      "batch_idx:     0, train_loss: 10.698545\n",
      "batch_idx:  1000, train_loss: 1.847833\n",
      "batch_idx:  2000, train_loss: 2.025804\n",
      "batch_idx:  3000, train_loss: 1.894429\n",
      "batch_idx:  4000, train_loss: 2.229993\n",
      "batch_idx:  5000, train_loss: 2.183684\n",
      "batch_idx:  6000, train_loss: 2.181694\n",
      "final train_loss: 2.224077\n",
      "batch_idx:     0, valid_loss: 10.203707\n",
      "batch_idx:  1000, valid_loss: 10.422288\n",
      "batch_idx:  2000, valid_loss: 10.420445\n",
      "batch_idx:  3000, valid_loss: 10.256757\n",
      "batch_idx:  4000, valid_loss: 10.120409\n",
      "batch_idx:  5000, valid_loss: 9.933931\n",
      "batch_idx:  6000, valid_loss: 9.642640\n",
      "Training Loss: 2.224077 \tValidation Loss: 9.280543\n",
      "epoch:   385\n",
      "batch_idx:     0, train_loss: 4.857776\n",
      "batch_idx:  1000, train_loss: 2.182513\n",
      "batch_idx:  2000, train_loss: 2.235566\n",
      "batch_idx:  3000, train_loss: 2.042393\n",
      "batch_idx:  4000, train_loss: 1.967385\n",
      "batch_idx:  5000, train_loss: 1.911923\n",
      "batch_idx:  6000, train_loss: 1.938027\n",
      "final train_loss: 1.978603\n",
      "batch_idx:     0, valid_loss: 9.170801\n",
      "batch_idx:  1000, valid_loss: 9.763820\n",
      "batch_idx:  2000, valid_loss: 9.738254\n",
      "batch_idx:  3000, valid_loss: 9.743813\n",
      "batch_idx:  4000, valid_loss: 9.683054\n",
      "batch_idx:  5000, valid_loss: 9.528049\n",
      "batch_idx:  6000, valid_loss: 9.254761\n",
      "Training Loss: 1.978603 \tValidation Loss: 8.903860\n",
      "epoch:   386\n",
      "batch_idx:     0, train_loss: 11.217577\n",
      "batch_idx:  1000, train_loss: 2.159200\n",
      "batch_idx:  2000, train_loss: 2.110804\n",
      "batch_idx:  3000, train_loss: 1.920905\n",
      "batch_idx:  4000, train_loss: 2.037317\n",
      "batch_idx:  5000, train_loss: 2.087267\n",
      "batch_idx:  6000, train_loss: 2.256486\n",
      "final train_loss: 2.383244\n",
      "batch_idx:     0, valid_loss: 7.017782\n",
      "batch_idx:  1000, valid_loss: 7.744642\n",
      "batch_idx:  2000, valid_loss: 7.671257\n",
      "batch_idx:  3000, valid_loss: 7.527923\n",
      "batch_idx:  4000, valid_loss: 7.411611\n",
      "batch_idx:  5000, valid_loss: 7.196433\n",
      "batch_idx:  6000, valid_loss: 6.940455\n",
      "Training Loss: 2.383244 \tValidation Loss: 6.634753\n",
      "epoch:   387\n",
      "batch_idx:     0, train_loss: 8.901279\n",
      "batch_idx:  1000, train_loss: 2.021854\n",
      "batch_idx:  2000, train_loss: 1.721159\n",
      "batch_idx:  3000, train_loss: 1.674202\n",
      "batch_idx:  4000, train_loss: 1.687636\n",
      "batch_idx:  5000, train_loss: 1.807298\n",
      "batch_idx:  6000, train_loss: 2.096969\n",
      "final train_loss: 2.266850\n",
      "batch_idx:     0, valid_loss: 6.443578\n",
      "batch_idx:  1000, valid_loss: 6.972642\n",
      "batch_idx:  2000, valid_loss: 6.779106\n",
      "batch_idx:  3000, valid_loss: 6.644925\n",
      "batch_idx:  4000, valid_loss: 6.536633\n",
      "batch_idx:  5000, valid_loss: 6.383865\n",
      "batch_idx:  6000, valid_loss: 6.178533\n",
      "Training Loss: 2.266850 \tValidation Loss: 5.927063\n",
      "epoch:   388\n",
      "batch_idx:     0, train_loss: 7.584246\n",
      "batch_idx:  1000, train_loss: 3.529710\n",
      "batch_idx:  2000, train_loss: 2.933452\n",
      "batch_idx:  3000, train_loss: 2.596949\n",
      "batch_idx:  4000, train_loss: 2.517945\n",
      "batch_idx:  5000, train_loss: 2.496566\n",
      "batch_idx:  6000, train_loss: 2.559572\n",
      "final train_loss: 2.562917\n",
      "batch_idx:     0, valid_loss: 6.006419\n",
      "batch_idx:  1000, valid_loss: 11.028550\n",
      "batch_idx:  2000, valid_loss: 11.501945\n",
      "batch_idx:  3000, valid_loss: 11.664448\n",
      "batch_idx:  4000, valid_loss: 11.778315\n",
      "batch_idx:  5000, valid_loss: 11.564916\n",
      "batch_idx:  6000, valid_loss: 11.225719\n",
      "Training Loss: 2.562917 \tValidation Loss: 10.801263\n",
      "epoch:   389\n",
      "batch_idx:     0, train_loss: 4.671359\n",
      "batch_idx:  1000, train_loss: 2.443187\n",
      "batch_idx:  2000, train_loss: 2.051450\n",
      "batch_idx:  3000, train_loss: 1.839230\n",
      "batch_idx:  4000, train_loss: 1.957071\n",
      "batch_idx:  5000, train_loss: 1.741212\n",
      "batch_idx:  6000, train_loss: 1.512629\n",
      "final train_loss: 1.407729\n",
      "batch_idx:     0, valid_loss: 6.198202\n",
      "batch_idx:  1000, valid_loss: 16.970503\n",
      "batch_idx:  2000, valid_loss: 22.564472\n",
      "batch_idx:  3000, valid_loss: 25.942284\n",
      "batch_idx:  4000, valid_loss: 27.511934\n",
      "batch_idx:  5000, valid_loss: 28.381042\n",
      "batch_idx:  6000, valid_loss: 27.926567\n",
      "Training Loss: 1.407729 \tValidation Loss: 26.407768\n",
      "epoch:   390\n",
      "batch_idx:     0, train_loss: 6.685272\n",
      "batch_idx:  1000, train_loss: 0.320194\n",
      "batch_idx:  2000, train_loss: 0.335439\n",
      "batch_idx:  3000, train_loss: 0.348144\n",
      "batch_idx:  4000, train_loss: 0.379078\n",
      "batch_idx:  5000, train_loss: 0.464539\n",
      "batch_idx:  6000, train_loss: 0.553714\n",
      "final train_loss: 0.577283\n",
      "batch_idx:     0, valid_loss: 11.523792\n",
      "batch_idx:  1000, valid_loss: 15.716063\n",
      "batch_idx:  2000, valid_loss: 17.061228\n",
      "batch_idx:  3000, valid_loss: 17.620731\n",
      "batch_idx:  4000, valid_loss: 17.882832\n",
      "batch_idx:  5000, valid_loss: 17.942911\n",
      "batch_idx:  6000, valid_loss: 17.575722\n",
      "Training Loss: 0.577283 \tValidation Loss: 16.916729\n",
      "epoch:   391\n",
      "batch_idx:     0, train_loss: 6.817921\n",
      "batch_idx:  1000, train_loss: 1.750232\n",
      "batch_idx:  2000, train_loss: 2.211857\n",
      "batch_idx:  3000, train_loss: 1.927175\n",
      "batch_idx:  4000, train_loss: 2.007017\n",
      "batch_idx:  5000, train_loss: 2.002338\n",
      "batch_idx:  6000, train_loss: 1.947841\n",
      "final train_loss: 2.008136\n",
      "batch_idx:     0, valid_loss: 9.602544\n",
      "batch_idx:  1000, valid_loss: 9.507699\n",
      "batch_idx:  2000, valid_loss: 9.464107\n",
      "batch_idx:  3000, valid_loss: 9.543894\n",
      "batch_idx:  4000, valid_loss: 9.543637\n",
      "batch_idx:  5000, valid_loss: 9.604033\n",
      "batch_idx:  6000, valid_loss: 9.336669\n",
      "Training Loss: 2.008136 \tValidation Loss: 8.974999\n",
      "epoch:   392\n",
      "batch_idx:     0, train_loss: 4.830441\n",
      "batch_idx:  1000, train_loss: 2.008312\n",
      "batch_idx:  2000, train_loss: 2.189476\n",
      "batch_idx:  3000, train_loss: 2.206228\n",
      "batch_idx:  4000, train_loss: 2.085848\n",
      "batch_idx:  5000, train_loss: 2.609596\n",
      "batch_idx:  6000, train_loss: 2.449250\n",
      "final train_loss: 2.508816\n",
      "batch_idx:     0, valid_loss: 5.530828\n",
      "batch_idx:  1000, valid_loss: 10.178823\n",
      "batch_idx:  2000, valid_loss: 10.621102\n",
      "batch_idx:  3000, valid_loss: 10.823376\n",
      "batch_idx:  4000, valid_loss: 10.840988\n",
      "batch_idx:  5000, valid_loss: 10.936713\n",
      "batch_idx:  6000, valid_loss: 10.787725\n",
      "Training Loss: 2.508816 \tValidation Loss: 10.450361\n",
      "epoch:   393\n",
      "batch_idx:     0, train_loss: 7.189100\n",
      "batch_idx:  1000, train_loss: 1.859029\n",
      "batch_idx:  2000, train_loss: 2.139102\n",
      "batch_idx:  3000, train_loss: 2.309437\n",
      "batch_idx:  4000, train_loss: 2.416813\n",
      "batch_idx:  5000, train_loss: 2.332804\n",
      "batch_idx:  6000, train_loss: 2.245572\n",
      "final train_loss: 2.181853\n",
      "batch_idx:     0, valid_loss: 6.168271\n",
      "batch_idx:  1000, valid_loss: 7.726333\n",
      "batch_idx:  2000, valid_loss: 9.397345\n",
      "batch_idx:  3000, valid_loss: 10.173387\n",
      "batch_idx:  4000, valid_loss: 10.509296\n",
      "batch_idx:  5000, valid_loss: 10.657270\n",
      "batch_idx:  6000, valid_loss: 10.474462\n",
      "Training Loss: 2.181853 \tValidation Loss: 9.940437\n",
      "epoch:   394\n",
      "batch_idx:     0, train_loss: 6.737838\n",
      "batch_idx:  1000, train_loss: 1.153943\n",
      "batch_idx:  2000, train_loss: 1.180853\n",
      "batch_idx:  3000, train_loss: 1.206650\n",
      "batch_idx:  4000, train_loss: 1.244441\n",
      "batch_idx:  5000, train_loss: 1.225826\n",
      "batch_idx:  6000, train_loss: 1.216175\n",
      "final train_loss: 1.182070\n",
      "batch_idx:     0, valid_loss: 11.583335\n",
      "batch_idx:  1000, valid_loss: 17.666260\n",
      "batch_idx:  2000, valid_loss: 20.774105\n",
      "batch_idx:  3000, valid_loss: 21.761801\n",
      "batch_idx:  4000, valid_loss: 22.182674\n",
      "batch_idx:  5000, valid_loss: 22.404709\n",
      "batch_idx:  6000, valid_loss: 21.881046\n",
      "Training Loss: 1.182070 \tValidation Loss: 20.983452\n",
      "epoch:   395\n",
      "batch_idx:     0, train_loss: 10.995343\n",
      "batch_idx:  1000, train_loss: 1.236473\n",
      "batch_idx:  2000, train_loss: 2.137859\n",
      "batch_idx:  3000, train_loss: 2.048419\n",
      "batch_idx:  4000, train_loss: 2.286099\n",
      "batch_idx:  5000, train_loss: 2.119682\n",
      "batch_idx:  6000, train_loss: 2.163842\n",
      "final train_loss: 2.141022\n",
      "batch_idx:     0, valid_loss: 10.059434\n",
      "batch_idx:  1000, valid_loss: 10.083858\n",
      "batch_idx:  2000, valid_loss: 10.056230\n",
      "batch_idx:  3000, valid_loss: 10.133052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  4000, valid_loss: 10.140915\n",
      "batch_idx:  5000, valid_loss: 10.240645\n",
      "batch_idx:  6000, valid_loss: 9.988730\n",
      "Training Loss: 2.141022 \tValidation Loss: 9.590854\n",
      "epoch:   396\n",
      "batch_idx:     0, train_loss: 5.094903\n",
      "batch_idx:  1000, train_loss: 2.082027\n",
      "batch_idx:  2000, train_loss: 1.988913\n",
      "batch_idx:  3000, train_loss: 1.991791\n",
      "batch_idx:  4000, train_loss: 1.846787\n",
      "batch_idx:  5000, train_loss: 1.841650\n",
      "batch_idx:  6000, train_loss: 1.893910\n",
      "final train_loss: 1.902302\n",
      "batch_idx:     0, valid_loss: 10.182772\n",
      "batch_idx:  1000, valid_loss: 10.050553\n",
      "batch_idx:  2000, valid_loss: 10.003337\n",
      "batch_idx:  3000, valid_loss: 10.041236\n",
      "batch_idx:  4000, valid_loss: 10.076223\n",
      "batch_idx:  5000, valid_loss: 10.096632\n",
      "batch_idx:  6000, valid_loss: 9.835955\n",
      "Training Loss: 1.902302 \tValidation Loss: 9.446580\n",
      "epoch:   397\n",
      "batch_idx:     0, train_loss: 5.029304\n",
      "batch_idx:  1000, train_loss: 1.952297\n",
      "batch_idx:  2000, train_loss: 1.599988\n",
      "batch_idx:  3000, train_loss: 1.591652\n",
      "batch_idx:  4000, train_loss: 1.657694\n",
      "batch_idx:  5000, train_loss: 1.750010\n",
      "batch_idx:  6000, train_loss: 1.875167\n",
      "final train_loss: 1.845638\n",
      "batch_idx:     0, valid_loss: 5.971448\n",
      "batch_idx:  1000, valid_loss: 10.246670\n",
      "batch_idx:  2000, valid_loss: 10.653238\n",
      "batch_idx:  3000, valid_loss: 10.816139\n",
      "batch_idx:  4000, valid_loss: 10.858384\n",
      "batch_idx:  5000, valid_loss: 10.934101\n",
      "batch_idx:  6000, valid_loss: 10.860104\n",
      "Training Loss: 1.845638 \tValidation Loss: 10.548698\n",
      "epoch:   398\n",
      "batch_idx:     0, train_loss: 7.902464\n",
      "batch_idx:  1000, train_loss: 1.550940\n",
      "batch_idx:  2000, train_loss: 1.951961\n",
      "batch_idx:  3000, train_loss: 1.798621\n",
      "batch_idx:  4000, train_loss: 1.783481\n",
      "batch_idx:  5000, train_loss: 1.843626\n",
      "batch_idx:  6000, train_loss: 1.951037\n",
      "final train_loss: 2.090567\n",
      "batch_idx:     0, valid_loss: 10.195151\n",
      "batch_idx:  1000, valid_loss: 9.405120\n",
      "batch_idx:  2000, valid_loss: 9.059593\n",
      "batch_idx:  3000, valid_loss: 8.933930\n",
      "batch_idx:  4000, valid_loss: 8.780595\n",
      "batch_idx:  5000, valid_loss: 8.552155\n",
      "batch_idx:  6000, valid_loss: 8.247838\n",
      "Training Loss: 2.090567 \tValidation Loss: 7.883670\n",
      "epoch:   399\n",
      "batch_idx:     0, train_loss: 11.805243\n",
      "batch_idx:  1000, train_loss: 2.972289\n",
      "batch_idx:  2000, train_loss: 2.483600\n",
      "batch_idx:  3000, train_loss: 2.383740\n",
      "batch_idx:  4000, train_loss: 2.253047\n",
      "batch_idx:  5000, train_loss: 2.170934\n",
      "batch_idx:  6000, train_loss: 2.218490\n",
      "final train_loss: 2.230360\n",
      "batch_idx:     0, valid_loss: 8.924006\n",
      "batch_idx:  1000, valid_loss: 9.559609\n",
      "batch_idx:  2000, valid_loss: 9.762412\n",
      "batch_idx:  3000, valid_loss: 9.830286\n",
      "batch_idx:  4000, valid_loss: 9.917111\n",
      "batch_idx:  5000, valid_loss: 9.939456\n",
      "batch_idx:  6000, valid_loss: 9.656020\n",
      "Training Loss: 2.230360 \tValidation Loss: 9.243770\n",
      "epoch:   400\n",
      "batch_idx:     0, train_loss: 11.424321\n",
      "batch_idx:  1000, train_loss: 2.084051\n",
      "batch_idx:  2000, train_loss: 2.355661\n",
      "batch_idx:  3000, train_loss: 2.042513\n",
      "batch_idx:  4000, train_loss: 2.569765\n",
      "batch_idx:  5000, train_loss: 3.027261\n",
      "batch_idx:  6000, train_loss: 3.366841\n",
      "final train_loss: 3.552197\n",
      "batch_idx:     0, valid_loss: 5.104752\n",
      "batch_idx:  1000, valid_loss: 4.909631\n",
      "batch_idx:  2000, valid_loss: 4.854270\n",
      "batch_idx:  3000, valid_loss: 4.928527\n",
      "batch_idx:  4000, valid_loss: 4.886543\n",
      "batch_idx:  5000, valid_loss: 4.861914\n",
      "batch_idx:  6000, valid_loss: 4.869111\n",
      "Training Loss: 3.552197 \tValidation Loss: 4.883253\n",
      "epoch:   401\n",
      "batch_idx:     0, train_loss: 5.104752\n",
      "batch_idx:  1000, train_loss: 3.153520\n",
      "batch_idx:  2000, train_loss: 2.467544\n",
      "batch_idx:  3000, train_loss: 2.264294\n",
      "batch_idx:  4000, train_loss: 2.649018\n",
      "batch_idx:  5000, train_loss: 3.062887\n",
      "batch_idx:  6000, train_loss: 3.375703\n",
      "final train_loss: 3.549227\n",
      "batch_idx:     0, valid_loss: 4.898182\n",
      "batch_idx:  1000, valid_loss: 4.940029\n",
      "batch_idx:  2000, valid_loss: 4.978329\n",
      "batch_idx:  3000, valid_loss: 5.057918\n",
      "batch_idx:  4000, valid_loss: 4.989293\n",
      "batch_idx:  5000, valid_loss: 4.924929\n",
      "batch_idx:  6000, valid_loss: 4.903971\n",
      "Training Loss: 3.549227 \tValidation Loss: 4.904119\n",
      "epoch:   402\n",
      "batch_idx:     0, train_loss: 4.898182\n",
      "batch_idx:  1000, train_loss: 4.684548\n",
      "batch_idx:  2000, train_loss: 4.763631\n",
      "batch_idx:  3000, train_loss: 4.860325\n",
      "batch_idx:  4000, train_loss: 4.447680\n",
      "batch_idx:  5000, train_loss: 4.507137\n",
      "batch_idx:  6000, train_loss: 4.579599\n",
      "final train_loss: 4.630066\n",
      "batch_idx:     0, valid_loss: 4.881366\n",
      "batch_idx:  1000, valid_loss: 4.908974\n",
      "batch_idx:  2000, valid_loss: 4.929821\n",
      "batch_idx:  3000, valid_loss: 4.969415\n",
      "batch_idx:  4000, valid_loss: 4.964118\n",
      "batch_idx:  5000, valid_loss: 4.909348\n",
      "batch_idx:  6000, valid_loss: 4.891588\n",
      "Training Loss: 4.630066 \tValidation Loss: 4.892389\n",
      "epoch:   403\n",
      "batch_idx:     0, train_loss: 4.881366\n",
      "batch_idx:  1000, train_loss: 4.655592\n",
      "batch_idx:  2000, train_loss: 4.721905\n",
      "batch_idx:  3000, train_loss: 4.791032\n",
      "batch_idx:  4000, train_loss: 4.302944\n",
      "batch_idx:  5000, train_loss: 3.759234\n",
      "batch_idx:  6000, train_loss: 3.266240\n",
      "final train_loss: 2.981467\n",
      "batch_idx:     0, valid_loss: 7.024864\n",
      "batch_idx:  1000, valid_loss: 14.434023\n",
      "batch_idx:  2000, valid_loss: 20.328850\n",
      "batch_idx:  3000, valid_loss: 24.305607\n",
      "batch_idx:  4000, valid_loss: 26.141834\n",
      "batch_idx:  5000, valid_loss: 26.878668\n",
      "batch_idx:  6000, valid_loss: 26.414957\n",
      "Training Loss: 2.981467 \tValidation Loss: 25.009794\n",
      "epoch:   404\n",
      "batch_idx:     0, train_loss: 7.292364\n",
      "batch_idx:  1000, train_loss: 0.311880\n",
      "batch_idx:  2000, train_loss: 0.327191\n",
      "batch_idx:  3000, train_loss: 0.344530\n",
      "batch_idx:  4000, train_loss: 0.381200\n",
      "batch_idx:  5000, train_loss: 0.403230\n",
      "batch_idx:  6000, train_loss: 0.463986\n",
      "final train_loss: 0.482237\n",
      "batch_idx:     0, valid_loss: 8.022601\n",
      "batch_idx:  1000, valid_loss: 14.087582\n",
      "batch_idx:  2000, valid_loss: 17.563910\n",
      "batch_idx:  3000, valid_loss: 20.146744\n",
      "batch_idx:  4000, valid_loss: 21.791281\n",
      "batch_idx:  5000, valid_loss: 22.453781\n",
      "batch_idx:  6000, valid_loss: 21.978233\n",
      "Training Loss: 0.482237 \tValidation Loss: 20.880699\n",
      "epoch:   405\n",
      "batch_idx:     0, train_loss: 7.905070\n",
      "batch_idx:  1000, train_loss: 0.459625\n",
      "batch_idx:  2000, train_loss: 0.479152\n",
      "batch_idx:  3000, train_loss: 0.501269\n",
      "batch_idx:  4000, train_loss: 0.562428\n",
      "batch_idx:  5000, train_loss: 0.615247\n",
      "batch_idx:  6000, train_loss: 0.644473\n",
      "final train_loss: 0.731927\n",
      "batch_idx:     0, valid_loss: 8.175220\n",
      "batch_idx:  1000, valid_loss: 10.922565\n",
      "batch_idx:  2000, valid_loss: 12.738596\n",
      "batch_idx:  3000, valid_loss: 13.258318\n",
      "batch_idx:  4000, valid_loss: 13.437138\n",
      "batch_idx:  5000, valid_loss: 13.286065\n",
      "batch_idx:  6000, valid_loss: 13.043710\n",
      "Training Loss: 0.731927 \tValidation Loss: 12.472092\n",
      "epoch:   406\n",
      "batch_idx:     0, train_loss: 4.688818\n",
      "batch_idx:  1000, train_loss: 1.305109\n",
      "batch_idx:  2000, train_loss: 1.489857\n",
      "batch_idx:  3000, train_loss: 1.527102\n",
      "batch_idx:  4000, train_loss: 1.600432\n",
      "batch_idx:  5000, train_loss: 1.667692\n",
      "batch_idx:  6000, train_loss: 1.669552\n",
      "final train_loss: 1.710829\n",
      "batch_idx:     0, valid_loss: 8.274101\n",
      "batch_idx:  1000, valid_loss: 12.541048\n",
      "batch_idx:  2000, valid_loss: 12.376574\n",
      "batch_idx:  3000, valid_loss: 12.220241\n",
      "batch_idx:  4000, valid_loss: 12.022797\n",
      "batch_idx:  5000, valid_loss: 11.754618\n",
      "batch_idx:  6000, valid_loss: 11.298436\n",
      "Training Loss: 1.710829 \tValidation Loss: 10.839273\n",
      "epoch:   407\n",
      "batch_idx:     0, train_loss: 10.678730\n",
      "batch_idx:  1000, train_loss: 1.214302\n",
      "batch_idx:  2000, train_loss: 1.180904\n",
      "batch_idx:  3000, train_loss: 1.101348\n",
      "batch_idx:  4000, train_loss: 1.015678\n",
      "batch_idx:  5000, train_loss: 1.033178\n",
      "batch_idx:  6000, train_loss: 0.985156\n",
      "final train_loss: 1.122748\n",
      "batch_idx:     0, valid_loss: 10.455042\n",
      "batch_idx:  1000, valid_loss: 9.459867\n",
      "batch_idx:  2000, valid_loss: 9.161700\n",
      "batch_idx:  3000, valid_loss: 10.363284\n",
      "batch_idx:  4000, valid_loss: 10.948993\n",
      "batch_idx:  5000, valid_loss: 11.208464\n",
      "batch_idx:  6000, valid_loss: 11.327641\n",
      "Training Loss: 1.122748 \tValidation Loss: 11.100714\n",
      "epoch:   408\n",
      "batch_idx:     0, train_loss: 13.015156\n",
      "batch_idx:  1000, train_loss: 1.110994\n",
      "batch_idx:  2000, train_loss: 1.193803\n",
      "batch_idx:  3000, train_loss: 1.438262\n",
      "batch_idx:  4000, train_loss: 1.545653\n",
      "batch_idx:  5000, train_loss: 1.475622\n",
      "batch_idx:  6000, train_loss: 1.518952\n",
      "final train_loss: 1.452366\n",
      "batch_idx:     0, valid_loss: 7.705420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  1000, valid_loss: 16.643597\n",
      "batch_idx:  2000, valid_loss: 19.868313\n",
      "batch_idx:  3000, valid_loss: 20.933554\n",
      "batch_idx:  4000, valid_loss: 21.175577\n",
      "batch_idx:  5000, valid_loss: 21.087624\n",
      "batch_idx:  6000, valid_loss: 20.502764\n",
      "Training Loss: 1.452366 \tValidation Loss: 19.450768\n",
      "epoch:   409\n",
      "batch_idx:     0, train_loss: 8.828176\n",
      "batch_idx:  1000, train_loss: 0.465809\n",
      "batch_idx:  2000, train_loss: 0.629937\n",
      "batch_idx:  3000, train_loss: 0.766214\n",
      "batch_idx:  4000, train_loss: 0.911621\n",
      "batch_idx:  5000, train_loss: 0.944191\n",
      "batch_idx:  6000, train_loss: 1.002748\n",
      "final train_loss: 1.005677\n",
      "batch_idx:     0, valid_loss: 8.730682\n",
      "batch_idx:  1000, valid_loss: 13.396412\n",
      "batch_idx:  2000, valid_loss: 14.062460\n",
      "batch_idx:  3000, valid_loss: 14.130757\n",
      "batch_idx:  4000, valid_loss: 14.034107\n",
      "batch_idx:  5000, valid_loss: 13.787552\n",
      "batch_idx:  6000, valid_loss: 13.306234\n",
      "Training Loss: 1.005677 \tValidation Loss: 12.823125\n",
      "epoch:   410\n",
      "batch_idx:     0, train_loss: 9.500589\n",
      "batch_idx:  1000, train_loss: 1.146477\n",
      "batch_idx:  2000, train_loss: 1.293133\n",
      "batch_idx:  3000, train_loss: 1.522511\n",
      "batch_idx:  4000, train_loss: 1.440672\n",
      "batch_idx:  5000, train_loss: 1.409696\n",
      "batch_idx:  6000, train_loss: 1.351415\n",
      "final train_loss: 1.371658\n",
      "batch_idx:     0, valid_loss: 9.581446\n",
      "batch_idx:  1000, valid_loss: 14.245296\n",
      "batch_idx:  2000, valid_loss: 17.553001\n",
      "batch_idx:  3000, valid_loss: 18.434366\n",
      "batch_idx:  4000, valid_loss: 18.690374\n",
      "batch_idx:  5000, valid_loss: 18.593410\n",
      "batch_idx:  6000, valid_loss: 18.149546\n",
      "Training Loss: 1.371658 \tValidation Loss: 17.590200\n",
      "epoch:   411\n",
      "batch_idx:     0, train_loss: 13.055563\n",
      "batch_idx:  1000, train_loss: 1.051697\n",
      "batch_idx:  2000, train_loss: 1.423850\n",
      "batch_idx:  3000, train_loss: 1.474303\n",
      "batch_idx:  4000, train_loss: 1.551091\n",
      "batch_idx:  5000, train_loss: 1.475091\n",
      "batch_idx:  6000, train_loss: 1.433082\n",
      "final train_loss: 1.372812\n",
      "batch_idx:     0, valid_loss: 11.313589\n",
      "batch_idx:  1000, valid_loss: 17.082821\n",
      "batch_idx:  2000, valid_loss: 18.218182\n",
      "batch_idx:  3000, valid_loss: 18.218941\n",
      "batch_idx:  4000, valid_loss: 18.105593\n",
      "batch_idx:  5000, valid_loss: 17.770109\n",
      "batch_idx:  6000, valid_loss: 17.152615\n",
      "Training Loss: 1.372812 \tValidation Loss: 16.467300\n",
      "epoch:   412\n",
      "batch_idx:     0, train_loss: 10.116535\n",
      "batch_idx:  1000, train_loss: 1.771295\n",
      "batch_idx:  2000, train_loss: 1.776876\n",
      "batch_idx:  3000, train_loss: 1.732108\n",
      "batch_idx:  4000, train_loss: 1.924091\n",
      "batch_idx:  5000, train_loss: 1.871673\n",
      "batch_idx:  6000, train_loss: 1.781946\n",
      "final train_loss: 1.754559\n",
      "batch_idx:     0, valid_loss: 8.549891\n",
      "batch_idx:  1000, valid_loss: 11.984662\n",
      "batch_idx:  2000, valid_loss: 12.467930\n",
      "batch_idx:  3000, valid_loss: 12.477361\n",
      "batch_idx:  4000, valid_loss: 12.319554\n",
      "batch_idx:  5000, valid_loss: 12.009128\n",
      "batch_idx:  6000, valid_loss: 11.560137\n",
      "Training Loss: 1.754559 \tValidation Loss: 11.041759\n",
      "epoch:   413\n",
      "batch_idx:     0, train_loss: 10.727876\n",
      "batch_idx:  1000, train_loss: 1.516337\n",
      "batch_idx:  2000, train_loss: 1.647160\n",
      "batch_idx:  3000, train_loss: 1.621648\n",
      "batch_idx:  4000, train_loss: 1.776285\n",
      "batch_idx:  5000, train_loss: 1.751675\n",
      "batch_idx:  6000, train_loss: 1.786468\n",
      "final train_loss: 1.730088\n",
      "batch_idx:     0, valid_loss: 7.173911\n",
      "batch_idx:  1000, valid_loss: 16.052881\n",
      "batch_idx:  2000, valid_loss: 18.810539\n",
      "batch_idx:  3000, valid_loss: 19.552422\n",
      "batch_idx:  4000, valid_loss: 19.769384\n",
      "batch_idx:  5000, valid_loss: 19.656725\n",
      "batch_idx:  6000, valid_loss: 19.111643\n",
      "Training Loss: 1.730088 \tValidation Loss: 18.233925\n",
      "epoch:   414\n",
      "batch_idx:     0, train_loss: 8.007284\n",
      "batch_idx:  1000, train_loss: 0.546226\n",
      "batch_idx:  2000, train_loss: 0.736449\n",
      "batch_idx:  3000, train_loss: 1.164480\n",
      "batch_idx:  4000, train_loss: 1.240656\n",
      "batch_idx:  5000, train_loss: 1.356314\n",
      "batch_idx:  6000, train_loss: 1.466704\n",
      "final train_loss: 1.512424\n",
      "batch_idx:     0, valid_loss: 7.360182\n",
      "batch_idx:  1000, valid_loss: 9.320610\n",
      "batch_idx:  2000, valid_loss: 9.398283\n",
      "batch_idx:  3000, valid_loss: 9.271884\n",
      "batch_idx:  4000, valid_loss: 9.236412\n",
      "batch_idx:  5000, valid_loss: 9.252431\n",
      "batch_idx:  6000, valid_loss: 9.225770\n",
      "Training Loss: 1.512424 \tValidation Loss: 9.038374\n",
      "epoch:   415\n",
      "batch_idx:     0, train_loss: 9.322870\n",
      "batch_idx:  1000, train_loss: 1.594431\n",
      "batch_idx:  2000, train_loss: 1.717589\n",
      "batch_idx:  3000, train_loss: 1.799668\n",
      "batch_idx:  4000, train_loss: 1.847396\n",
      "batch_idx:  5000, train_loss: 1.765560\n",
      "batch_idx:  6000, train_loss: 1.722882\n",
      "final train_loss: 1.811066\n",
      "batch_idx:     0, valid_loss: 10.822282\n",
      "batch_idx:  1000, valid_loss: 10.550577\n",
      "batch_idx:  2000, valid_loss: 10.503465\n",
      "batch_idx:  3000, valid_loss: 10.447014\n",
      "batch_idx:  4000, valid_loss: 10.468791\n",
      "batch_idx:  5000, valid_loss: 10.370470\n",
      "batch_idx:  6000, valid_loss: 10.092615\n",
      "Training Loss: 1.811066 \tValidation Loss: 9.696489\n",
      "epoch:   416\n",
      "batch_idx:     0, train_loss: 13.073854\n",
      "batch_idx:  1000, train_loss: 2.189975\n",
      "batch_idx:  2000, train_loss: 2.372477\n",
      "batch_idx:  3000, train_loss: 2.150702\n",
      "batch_idx:  4000, train_loss: 1.987173\n",
      "batch_idx:  5000, train_loss: 1.922509\n",
      "batch_idx:  6000, train_loss: 1.931530\n",
      "final train_loss: 1.968230\n",
      "batch_idx:     0, valid_loss: 4.229306\n",
      "batch_idx:  1000, valid_loss: 6.593812\n",
      "batch_idx:  2000, valid_loss: 8.014535\n",
      "batch_idx:  3000, valid_loss: 8.651129\n",
      "batch_idx:  4000, valid_loss: 8.976275\n",
      "batch_idx:  5000, valid_loss: 9.198788\n",
      "batch_idx:  6000, valid_loss: 8.914136\n",
      "Training Loss: 1.968230 \tValidation Loss: 8.362801\n",
      "epoch:   417\n",
      "batch_idx:     0, train_loss: 3.367419\n",
      "batch_idx:  1000, train_loss: 0.455445\n",
      "batch_idx:  2000, train_loss: 0.587848\n",
      "batch_idx:  3000, train_loss: 0.686821\n",
      "batch_idx:  4000, train_loss: 0.875259\n",
      "batch_idx:  5000, train_loss: 1.045512\n",
      "batch_idx:  6000, train_loss: 1.153559\n",
      "final train_loss: 1.153478\n",
      "batch_idx:     0, valid_loss: 10.598069\n",
      "batch_idx:  1000, valid_loss: 15.827518\n",
      "batch_idx:  2000, valid_loss: 18.396786\n",
      "batch_idx:  3000, valid_loss: 19.101158\n",
      "batch_idx:  4000, valid_loss: 19.469782\n",
      "batch_idx:  5000, valid_loss: 19.402676\n",
      "batch_idx:  6000, valid_loss: 18.665970\n",
      "Training Loss: 1.153478 \tValidation Loss: 17.883022\n",
      "epoch:   418\n",
      "batch_idx:     0, train_loss: 12.323486\n",
      "batch_idx:  1000, train_loss: 0.770118\n",
      "batch_idx:  2000, train_loss: 1.201005\n",
      "batch_idx:  3000, train_loss: 1.251956\n",
      "batch_idx:  4000, train_loss: 1.334265\n",
      "batch_idx:  5000, train_loss: 1.422871\n",
      "batch_idx:  6000, train_loss: 1.504043\n",
      "final train_loss: 1.569260\n",
      "batch_idx:     0, valid_loss: 9.268498\n",
      "batch_idx:  1000, valid_loss: 9.585645\n",
      "batch_idx:  2000, valid_loss: 9.631087\n",
      "batch_idx:  3000, valid_loss: 9.593149\n",
      "batch_idx:  4000, valid_loss: 9.590746\n",
      "batch_idx:  5000, valid_loss: 9.449909\n",
      "batch_idx:  6000, valid_loss: 9.173035\n",
      "Training Loss: 1.569260 \tValidation Loss: 8.835751\n",
      "epoch:   419\n",
      "batch_idx:     0, train_loss: 10.626053\n",
      "batch_idx:  1000, train_loss: 2.106882\n",
      "batch_idx:  2000, train_loss: 1.944144\n",
      "batch_idx:  3000, train_loss: 1.900072\n",
      "batch_idx:  4000, train_loss: 1.894493\n",
      "batch_idx:  5000, train_loss: 1.998594\n",
      "batch_idx:  6000, train_loss: 1.958804\n",
      "final train_loss: 1.938635\n",
      "batch_idx:     0, valid_loss: 4.878481\n",
      "batch_idx:  1000, valid_loss: 11.270997\n",
      "batch_idx:  2000, valid_loss: 15.104089\n",
      "batch_idx:  3000, valid_loss: 16.363417\n",
      "batch_idx:  4000, valid_loss: 16.575899\n",
      "batch_idx:  5000, valid_loss: 16.928165\n",
      "batch_idx:  6000, valid_loss: 16.786760\n",
      "Training Loss: 1.938635 \tValidation Loss: 16.028557\n",
      "epoch:   420\n",
      "batch_idx:     0, train_loss: 5.190790\n",
      "batch_idx:  1000, train_loss: 0.274542\n",
      "batch_idx:  2000, train_loss: 0.284579\n",
      "batch_idx:  3000, train_loss: 0.296523\n",
      "batch_idx:  4000, train_loss: 0.323051\n",
      "batch_idx:  5000, train_loss: 0.348526\n",
      "batch_idx:  6000, train_loss: 0.375826\n",
      "final train_loss: 0.424229\n",
      "batch_idx:     0, valid_loss: 4.983308\n",
      "batch_idx:  1000, valid_loss: 8.615127\n",
      "batch_idx:  2000, valid_loss: 10.499722\n",
      "batch_idx:  3000, valid_loss: 11.589858\n",
      "batch_idx:  4000, valid_loss: 12.271123\n",
      "batch_idx:  5000, valid_loss: 12.706796\n",
      "batch_idx:  6000, valid_loss: 12.497131\n",
      "Training Loss: 0.424229 \tValidation Loss: 11.843840\n",
      "epoch:   421\n",
      "batch_idx:     0, train_loss: 4.844414\n",
      "batch_idx:  1000, train_loss: 0.511319\n",
      "batch_idx:  2000, train_loss: 0.510771\n",
      "batch_idx:  3000, train_loss: 0.686098\n",
      "batch_idx:  4000, train_loss: 0.898436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  5000, train_loss: 1.036917\n",
      "batch_idx:  6000, train_loss: 1.068736\n",
      "final train_loss: 1.101270\n",
      "batch_idx:     0, valid_loss: 9.844763\n",
      "batch_idx:  1000, valid_loss: 21.263344\n",
      "batch_idx:  2000, valid_loss: 22.428814\n",
      "batch_idx:  3000, valid_loss: 22.259977\n",
      "batch_idx:  4000, valid_loss: 21.904554\n",
      "batch_idx:  5000, valid_loss: 21.487871\n",
      "batch_idx:  6000, valid_loss: 20.710686\n",
      "Training Loss: 1.101270 \tValidation Loss: 19.717670\n",
      "epoch:   422\n",
      "batch_idx:     0, train_loss: 13.088206\n",
      "batch_idx:  1000, train_loss: 0.909178\n",
      "batch_idx:  2000, train_loss: 0.922494\n",
      "batch_idx:  3000, train_loss: 1.141690\n",
      "batch_idx:  4000, train_loss: 1.239289\n",
      "batch_idx:  5000, train_loss: 1.344615\n",
      "batch_idx:  6000, train_loss: 1.399891\n",
      "final train_loss: 1.539253\n",
      "batch_idx:     0, valid_loss: 9.879663\n",
      "batch_idx:  1000, valid_loss: 10.366734\n",
      "batch_idx:  2000, valid_loss: 10.418282\n",
      "batch_idx:  3000, valid_loss: 10.227085\n",
      "batch_idx:  4000, valid_loss: 10.029622\n",
      "batch_idx:  5000, valid_loss: 9.801425\n",
      "batch_idx:  6000, valid_loss: 9.458715\n",
      "Training Loss: 1.539253 \tValidation Loss: 9.077413\n",
      "epoch:   423\n",
      "batch_idx:     0, train_loss: 12.945269\n",
      "batch_idx:  1000, train_loss: 1.428696\n",
      "batch_idx:  2000, train_loss: 1.262430\n",
      "batch_idx:  3000, train_loss: 1.329197\n",
      "batch_idx:  4000, train_loss: 1.515928\n",
      "batch_idx:  5000, train_loss: 1.543217\n",
      "batch_idx:  6000, train_loss: 1.659775\n",
      "final train_loss: 1.662421\n",
      "batch_idx:     0, valid_loss: 10.476184\n",
      "batch_idx:  1000, valid_loss: 15.654562\n",
      "batch_idx:  2000, valid_loss: 17.707922\n",
      "batch_idx:  3000, valid_loss: 18.110378\n",
      "batch_idx:  4000, valid_loss: 18.220882\n",
      "batch_idx:  5000, valid_loss: 18.158529\n",
      "batch_idx:  6000, valid_loss: 17.598549\n",
      "Training Loss: 1.662421 \tValidation Loss: 16.875919\n",
      "epoch:   424\n",
      "batch_idx:     0, train_loss: 13.878109\n",
      "batch_idx:  1000, train_loss: 1.034244\n",
      "batch_idx:  2000, train_loss: 1.133686\n",
      "batch_idx:  3000, train_loss: 1.170618\n",
      "batch_idx:  4000, train_loss: 1.313379\n",
      "batch_idx:  5000, train_loss: 1.506736\n",
      "batch_idx:  6000, train_loss: 1.544263\n",
      "final train_loss: 1.582641\n",
      "batch_idx:     0, valid_loss: 8.191085\n",
      "batch_idx:  1000, valid_loss: 10.008260\n",
      "batch_idx:  2000, valid_loss: 11.472750\n",
      "batch_idx:  3000, valid_loss: 11.577606\n",
      "batch_idx:  4000, valid_loss: 11.625818\n",
      "batch_idx:  5000, valid_loss: 11.442762\n",
      "batch_idx:  6000, valid_loss: 11.029643\n",
      "Training Loss: 1.582641 \tValidation Loss: 10.545230\n",
      "epoch:   425\n",
      "batch_idx:     0, train_loss: 10.998380\n",
      "batch_idx:  1000, train_loss: 1.476997\n",
      "batch_idx:  2000, train_loss: 1.573271\n",
      "batch_idx:  3000, train_loss: 1.657411\n",
      "batch_idx:  4000, train_loss: 1.830469\n",
      "batch_idx:  5000, train_loss: 1.773120\n",
      "batch_idx:  6000, train_loss: 1.848028\n",
      "final train_loss: 1.871200\n",
      "batch_idx:     0, valid_loss: 9.942002\n",
      "batch_idx:  1000, valid_loss: 10.241707\n",
      "batch_idx:  2000, valid_loss: 11.704901\n",
      "batch_idx:  3000, valid_loss: 11.841743\n",
      "batch_idx:  4000, valid_loss: 11.897896\n",
      "batch_idx:  5000, valid_loss: 11.764701\n",
      "batch_idx:  6000, valid_loss: 11.315315\n",
      "Training Loss: 1.871200 \tValidation Loss: 10.776589\n",
      "epoch:   426\n",
      "batch_idx:     0, train_loss: 10.394162\n",
      "batch_idx:  1000, train_loss: 1.521842\n",
      "batch_idx:  2000, train_loss: 1.791180\n",
      "batch_idx:  3000, train_loss: 1.891826\n",
      "batch_idx:  4000, train_loss: 1.803340\n",
      "batch_idx:  5000, train_loss: 1.981585\n",
      "batch_idx:  6000, train_loss: 2.062455\n",
      "final train_loss: 2.042264\n",
      "batch_idx:     0, valid_loss: 7.557997\n",
      "batch_idx:  1000, valid_loss: 9.664192\n",
      "batch_idx:  2000, valid_loss: 11.587070\n",
      "batch_idx:  3000, valid_loss: 11.857031\n",
      "batch_idx:  4000, valid_loss: 12.031327\n",
      "batch_idx:  5000, valid_loss: 11.949001\n",
      "batch_idx:  6000, valid_loss: 11.584949\n",
      "Training Loss: 2.042264 \tValidation Loss: 11.020250\n",
      "epoch:   427\n",
      "batch_idx:     0, train_loss: 8.817483\n",
      "batch_idx:  1000, train_loss: 1.520786\n",
      "batch_idx:  2000, train_loss: 2.021014\n",
      "batch_idx:  3000, train_loss: 1.824217\n",
      "batch_idx:  4000, train_loss: 1.878320\n",
      "batch_idx:  5000, train_loss: 1.811367\n",
      "batch_idx:  6000, train_loss: 1.914584\n",
      "final train_loss: 1.928163\n",
      "batch_idx:     0, valid_loss: 9.216067\n",
      "batch_idx:  1000, valid_loss: 8.970063\n",
      "batch_idx:  2000, valid_loss: 10.769360\n",
      "batch_idx:  3000, valid_loss: 11.283593\n",
      "batch_idx:  4000, valid_loss: 11.376699\n",
      "batch_idx:  5000, valid_loss: 11.328342\n",
      "batch_idx:  6000, valid_loss: 10.948412\n",
      "Training Loss: 1.928163 \tValidation Loss: 10.459466\n",
      "epoch:   428\n",
      "batch_idx:     0, train_loss: 5.030531\n",
      "batch_idx:  1000, train_loss: 1.969599\n",
      "batch_idx:  2000, train_loss: 2.286959\n",
      "batch_idx:  3000, train_loss: 2.147431\n",
      "batch_idx:  4000, train_loss: 2.060132\n",
      "batch_idx:  5000, train_loss: 2.069432\n",
      "batch_idx:  6000, train_loss: 2.017631\n",
      "final train_loss: 1.989655\n",
      "batch_idx:     0, valid_loss: 8.241341\n",
      "batch_idx:  1000, valid_loss: 9.799807\n",
      "batch_idx:  2000, valid_loss: 9.588436\n",
      "batch_idx:  3000, valid_loss: 9.598419\n",
      "batch_idx:  4000, valid_loss: 9.595898\n",
      "batch_idx:  5000, valid_loss: 9.636723\n",
      "batch_idx:  6000, valid_loss: 9.507074\n",
      "Training Loss: 1.989655 \tValidation Loss: 9.140068\n",
      "epoch:   429\n",
      "batch_idx:     0, train_loss: 12.095356\n",
      "batch_idx:  1000, train_loss: 1.970082\n",
      "batch_idx:  2000, train_loss: 1.990324\n",
      "batch_idx:  3000, train_loss: 2.163184\n",
      "batch_idx:  4000, train_loss: 2.155557\n",
      "batch_idx:  5000, train_loss: 2.064185\n",
      "batch_idx:  6000, train_loss: 1.989128\n",
      "final train_loss: 2.019794\n",
      "batch_idx:     0, valid_loss: 4.369342\n",
      "batch_idx:  1000, valid_loss: 7.652640\n",
      "batch_idx:  2000, valid_loss: 9.560961\n",
      "batch_idx:  3000, valid_loss: 10.773104\n",
      "batch_idx:  4000, valid_loss: 11.493156\n",
      "batch_idx:  5000, valid_loss: 11.905514\n",
      "batch_idx:  6000, valid_loss: 11.718561\n",
      "Training Loss: 2.019794 \tValidation Loss: 11.113048\n",
      "epoch:   430\n",
      "batch_idx:     0, train_loss: 4.201558\n",
      "batch_idx:  1000, train_loss: 1.598387\n",
      "batch_idx:  2000, train_loss: 2.132059\n",
      "batch_idx:  3000, train_loss: 2.331074\n",
      "batch_idx:  4000, train_loss: 2.199035\n",
      "batch_idx:  5000, train_loss: 2.161842\n",
      "batch_idx:  6000, train_loss: 2.249607\n",
      "final train_loss: 2.313389\n",
      "batch_idx:     0, valid_loss: 11.751273\n",
      "batch_idx:  1000, valid_loss: 11.038061\n",
      "batch_idx:  2000, valid_loss: 11.242355\n",
      "batch_idx:  3000, valid_loss: 11.081387\n",
      "batch_idx:  4000, valid_loss: 10.823807\n",
      "batch_idx:  5000, valid_loss: 10.556177\n",
      "batch_idx:  6000, valid_loss: 10.181666\n",
      "Training Loss: 2.313389 \tValidation Loss: 9.770357\n",
      "epoch:   431\n",
      "batch_idx:     0, train_loss: 12.641415\n",
      "batch_idx:  1000, train_loss: 2.347926\n",
      "batch_idx:  2000, train_loss: 2.349701\n",
      "batch_idx:  3000, train_loss: 2.041125\n",
      "batch_idx:  4000, train_loss: 2.132856\n",
      "batch_idx:  5000, train_loss: 2.199391\n",
      "batch_idx:  6000, train_loss: 2.655589\n",
      "final train_loss: 2.914655\n",
      "batch_idx:     0, valid_loss: 5.178601\n",
      "batch_idx:  1000, valid_loss: 5.085694\n",
      "batch_idx:  2000, valid_loss: 4.942133\n",
      "batch_idx:  3000, valid_loss: 4.905350\n",
      "batch_idx:  4000, valid_loss: 4.880404\n",
      "batch_idx:  5000, valid_loss: 4.881575\n",
      "batch_idx:  6000, valid_loss: 4.870661\n",
      "Training Loss: 2.914655 \tValidation Loss: 4.886006\n",
      "epoch:   432\n",
      "batch_idx:     0, train_loss: 5.178601\n",
      "batch_idx:  1000, train_loss: 4.819657\n",
      "batch_idx:  2000, train_loss: 4.733595\n",
      "batch_idx:  3000, train_loss: 4.740368\n",
      "batch_idx:  4000, train_loss: 4.093799\n",
      "batch_idx:  5000, train_loss: 3.812834\n",
      "batch_idx:  6000, train_loss: 3.577824\n",
      "final train_loss: 3.509028\n",
      "batch_idx:     0, valid_loss: 10.396948\n",
      "batch_idx:  1000, valid_loss: 10.214993\n",
      "batch_idx:  2000, valid_loss: 10.150798\n",
      "batch_idx:  3000, valid_loss: 10.145128\n",
      "batch_idx:  4000, valid_loss: 10.247138\n",
      "batch_idx:  5000, valid_loss: 10.139742\n",
      "batch_idx:  6000, valid_loss: 9.732932\n",
      "Training Loss: 3.509028 \tValidation Loss: 9.307108\n",
      "epoch:   433\n",
      "batch_idx:     0, train_loss: 11.745055\n",
      "batch_idx:  1000, train_loss: 2.457358\n",
      "batch_idx:  2000, train_loss: 2.718878\n",
      "batch_idx:  3000, train_loss: 2.725397\n",
      "batch_idx:  4000, train_loss: 2.486746\n",
      "batch_idx:  5000, train_loss: 2.360886\n",
      "batch_idx:  6000, train_loss: 2.343245\n",
      "final train_loss: 2.365431\n",
      "batch_idx:     0, valid_loss: 9.635732\n",
      "batch_idx:  1000, valid_loss: 10.589814\n",
      "batch_idx:  2000, valid_loss: 10.608840\n",
      "batch_idx:  3000, valid_loss: 10.645054\n",
      "batch_idx:  4000, valid_loss: 10.518218\n",
      "batch_idx:  5000, valid_loss: 10.272954\n",
      "batch_idx:  6000, valid_loss: 9.947073\n",
      "Training Loss: 2.365431 \tValidation Loss: 9.558367\n",
      "epoch:   434\n",
      "batch_idx:     0, train_loss: 12.017444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  1000, train_loss: 2.165009\n",
      "batch_idx:  2000, train_loss: 2.466832\n",
      "batch_idx:  3000, train_loss: 2.115527\n",
      "batch_idx:  4000, train_loss: 2.000234\n",
      "batch_idx:  5000, train_loss: 2.311538\n",
      "batch_idx:  6000, train_loss: 2.753646\n",
      "final train_loss: 2.995627\n",
      "batch_idx:     0, valid_loss: 4.999565\n",
      "batch_idx:  1000, valid_loss: 4.929951\n",
      "batch_idx:  2000, valid_loss: 4.818650\n",
      "batch_idx:  3000, valid_loss: 4.809360\n",
      "batch_idx:  4000, valid_loss: 4.888048\n",
      "batch_idx:  5000, valid_loss: 4.891107\n",
      "batch_idx:  6000, valid_loss: 4.878995\n",
      "Training Loss: 2.995627 \tValidation Loss: 4.886743\n",
      "epoch:   435\n",
      "batch_idx:     0, train_loss: 4.999565\n",
      "batch_idx:  1000, train_loss: 4.674671\n",
      "batch_idx:  2000, train_loss: 4.176616\n",
      "batch_idx:  3000, train_loss: 3.863001\n",
      "batch_idx:  4000, train_loss: 3.353701\n",
      "batch_idx:  5000, train_loss: 3.024923\n",
      "batch_idx:  6000, train_loss: 3.117645\n",
      "final train_loss: 3.287421\n",
      "batch_idx:     0, valid_loss: 4.753945\n",
      "batch_idx:  1000, valid_loss: 4.716319\n",
      "batch_idx:  2000, valid_loss: 4.698125\n",
      "batch_idx:  3000, valid_loss: 4.737289\n",
      "batch_idx:  4000, valid_loss: 4.859320\n",
      "batch_idx:  5000, valid_loss: 4.904470\n",
      "batch_idx:  6000, valid_loss: 4.897399\n",
      "Training Loss: 3.287421 \tValidation Loss: 4.891950\n",
      "epoch:   436\n",
      "batch_idx:     0, train_loss: 4.753945\n",
      "batch_idx:  1000, train_loss: 4.478929\n",
      "batch_idx:  2000, train_loss: 4.524986\n",
      "batch_idx:  3000, train_loss: 3.947738\n",
      "batch_idx:  4000, train_loss: 4.245797\n",
      "batch_idx:  5000, train_loss: 3.874451\n",
      "batch_idx:  6000, train_loss: 3.677885\n",
      "final train_loss: 3.497543\n",
      "batch_idx:     0, valid_loss: 4.961689\n",
      "batch_idx:  1000, valid_loss: 7.602275\n",
      "batch_idx:  2000, valid_loss: 9.583972\n",
      "batch_idx:  3000, valid_loss: 10.871348\n",
      "batch_idx:  4000, valid_loss: 11.698819\n",
      "batch_idx:  5000, valid_loss: 12.232211\n",
      "batch_idx:  6000, valid_loss: 12.041863\n",
      "Training Loss: 3.497543 \tValidation Loss: 11.397552\n",
      "epoch:   437\n",
      "batch_idx:     0, train_loss: 4.513557\n",
      "batch_idx:  1000, train_loss: 1.068976\n",
      "batch_idx:  2000, train_loss: 0.827045\n",
      "batch_idx:  3000, train_loss: 1.201795\n",
      "batch_idx:  4000, train_loss: 1.674010\n",
      "batch_idx:  5000, train_loss: 1.734799\n",
      "batch_idx:  6000, train_loss: 1.743970\n",
      "final train_loss: 1.884670\n",
      "batch_idx:     0, valid_loss: 8.338117\n",
      "batch_idx:  1000, valid_loss: 7.099108\n",
      "batch_idx:  2000, valid_loss: 8.852971\n",
      "batch_idx:  3000, valid_loss: 9.415101\n",
      "batch_idx:  4000, valid_loss: 9.690887\n",
      "batch_idx:  5000, valid_loss: 9.894139\n",
      "batch_idx:  6000, valid_loss: 10.036245\n",
      "Training Loss: 1.884670 \tValidation Loss: 9.895304\n",
      "epoch:   438\n",
      "batch_idx:     0, train_loss: 10.678381\n",
      "batch_idx:  1000, train_loss: 1.293558\n",
      "batch_idx:  2000, train_loss: 1.375860\n",
      "batch_idx:  3000, train_loss: 1.724378\n",
      "batch_idx:  4000, train_loss: 1.895952\n",
      "batch_idx:  5000, train_loss: 1.885635\n",
      "batch_idx:  6000, train_loss: 2.072434\n",
      "final train_loss: 2.196776\n",
      "batch_idx:     0, valid_loss: 4.795325\n",
      "batch_idx:  1000, valid_loss: 4.820175\n",
      "batch_idx:  2000, valid_loss: 4.825998\n",
      "batch_idx:  3000, valid_loss: 4.829152\n",
      "batch_idx:  4000, valid_loss: 4.817189\n",
      "batch_idx:  5000, valid_loss: 4.878458\n",
      "batch_idx:  6000, valid_loss: 4.885915\n",
      "Training Loss: 2.196776 \tValidation Loss: 4.884267\n",
      "epoch:   439\n",
      "batch_idx:     0, train_loss: 4.795325\n",
      "batch_idx:  1000, train_loss: 1.705276\n",
      "batch_idx:  2000, train_loss: 2.034341\n",
      "batch_idx:  3000, train_loss: 2.024734\n",
      "batch_idx:  4000, train_loss: 1.974677\n",
      "batch_idx:  5000, train_loss: 2.094774\n",
      "batch_idx:  6000, train_loss: 2.086967\n",
      "final train_loss: 2.073725\n",
      "batch_idx:     0, valid_loss: 7.472830\n",
      "batch_idx:  1000, valid_loss: 7.531396\n",
      "batch_idx:  2000, valid_loss: 9.604836\n",
      "batch_idx:  3000, valid_loss: 10.394879\n",
      "batch_idx:  4000, valid_loss: 10.607424\n",
      "batch_idx:  5000, valid_loss: 10.647005\n",
      "batch_idx:  6000, valid_loss: 10.479727\n",
      "Training Loss: 2.073725 \tValidation Loss: 10.258848\n",
      "epoch:   440\n",
      "batch_idx:     0, train_loss: 9.588760\n",
      "batch_idx:  1000, train_loss: 1.481465\n",
      "batch_idx:  2000, train_loss: 2.266445\n",
      "batch_idx:  3000, train_loss: 2.256694\n",
      "batch_idx:  4000, train_loss: 2.359666\n",
      "batch_idx:  5000, train_loss: 2.351695\n",
      "batch_idx:  6000, train_loss: 2.698039\n",
      "final train_loss: 2.925391\n",
      "batch_idx:     0, valid_loss: 4.669779\n",
      "batch_idx:  1000, valid_loss: 4.990532\n",
      "batch_idx:  2000, valid_loss: 4.910825\n",
      "batch_idx:  3000, valid_loss: 4.906929\n",
      "batch_idx:  4000, valid_loss: 4.900489\n",
      "batch_idx:  5000, valid_loss: 4.938726\n",
      "batch_idx:  6000, valid_loss: 4.911048\n",
      "Training Loss: 2.925391 \tValidation Loss: 4.896274\n",
      "epoch:   441\n",
      "batch_idx:     0, train_loss: 4.669779\n",
      "batch_idx:  1000, train_loss: 4.731407\n",
      "batch_idx:  2000, train_loss: 4.453197\n",
      "batch_idx:  3000, train_loss: 3.538975\n",
      "batch_idx:  4000, train_loss: 3.182739\n",
      "batch_idx:  5000, train_loss: 2.932089\n",
      "batch_idx:  6000, train_loss: 2.895426\n",
      "final train_loss: 2.784914\n",
      "batch_idx:     0, valid_loss: 4.719164\n",
      "batch_idx:  1000, valid_loss: 5.264907\n",
      "batch_idx:  2000, valid_loss: 7.379236\n",
      "batch_idx:  3000, valid_loss: 9.029037\n",
      "batch_idx:  4000, valid_loss: 10.002974\n",
      "batch_idx:  5000, valid_loss: 10.689759\n",
      "batch_idx:  6000, valid_loss: 10.696194\n",
      "Training Loss: 2.784914 \tValidation Loss: 10.245927\n",
      "epoch:   442\n",
      "batch_idx:     0, train_loss: 4.615942\n",
      "batch_idx:  1000, train_loss: 1.046541\n",
      "batch_idx:  2000, train_loss: 0.920745\n",
      "batch_idx:  3000, train_loss: 0.846088\n",
      "batch_idx:  4000, train_loss: 1.020729\n",
      "batch_idx:  5000, train_loss: 1.316669\n",
      "batch_idx:  6000, train_loss: 1.438593\n",
      "final train_loss: 1.475497\n",
      "batch_idx:     0, valid_loss: 6.480503\n",
      "batch_idx:  1000, valid_loss: 7.414661\n",
      "batch_idx:  2000, valid_loss: 7.819193\n",
      "batch_idx:  3000, valid_loss: 8.853330\n",
      "batch_idx:  4000, valid_loss: 9.361670\n",
      "batch_idx:  5000, valid_loss: 9.524001\n",
      "batch_idx:  6000, valid_loss: 9.457403\n",
      "Training Loss: 1.475497 \tValidation Loss: 9.241220\n",
      "epoch:   443\n",
      "batch_idx:     0, train_loss: 8.636522\n",
      "batch_idx:  1000, train_loss: 1.629696\n",
      "batch_idx:  2000, train_loss: 1.501404\n",
      "batch_idx:  3000, train_loss: 1.434883\n",
      "batch_idx:  4000, train_loss: 1.574096\n",
      "batch_idx:  5000, train_loss: 1.729632\n",
      "batch_idx:  6000, train_loss: 1.816472\n",
      "final train_loss: 1.789640\n",
      "batch_idx:     0, valid_loss: 12.245777\n",
      "batch_idx:  1000, valid_loss: 11.460276\n",
      "batch_idx:  2000, valid_loss: 11.286094\n",
      "batch_idx:  3000, valid_loss: 11.316488\n",
      "batch_idx:  4000, valid_loss: 11.148769\n",
      "batch_idx:  5000, valid_loss: 10.876683\n",
      "batch_idx:  6000, valid_loss: 10.365119\n",
      "Training Loss: 1.789640 \tValidation Loss: 9.806452\n",
      "epoch:   444\n",
      "batch_idx:     0, train_loss: 4.587946\n",
      "batch_idx:  1000, train_loss: 4.554427\n",
      "batch_idx:  2000, train_loss: 4.599972\n",
      "batch_idx:  3000, train_loss: 4.719887\n",
      "batch_idx:  4000, train_loss: 4.768413\n",
      "batch_idx:  5000, train_loss: 4.632654\n",
      "batch_idx:  6000, train_loss: 4.218381\n",
      "final train_loss: 3.942739\n",
      "batch_idx:     0, valid_loss: 10.452661\n",
      "batch_idx:  1000, valid_loss: 14.559107\n",
      "batch_idx:  2000, valid_loss: 14.410407\n",
      "batch_idx:  3000, valid_loss: 14.422036\n",
      "batch_idx:  4000, valid_loss: 14.243883\n",
      "batch_idx:  5000, valid_loss: 13.939216\n",
      "batch_idx:  6000, valid_loss: 13.430384\n",
      "Training Loss: 3.942739 \tValidation Loss: 12.722746\n",
      "epoch:   445\n",
      "batch_idx:     0, train_loss: 11.408722\n",
      "batch_idx:  1000, train_loss: 4.343751\n",
      "batch_idx:  2000, train_loss: 4.469883\n",
      "batch_idx:  3000, train_loss: 4.576467\n",
      "batch_idx:  4000, train_loss: 4.633693\n",
      "batch_idx:  5000, train_loss: 4.698487\n",
      "batch_idx:  6000, train_loss: 4.210116\n",
      "final train_loss: 4.070639\n",
      "batch_idx:     0, valid_loss: 4.030824\n",
      "batch_idx:  1000, valid_loss: 9.398479\n",
      "batch_idx:  2000, valid_loss: 9.528174\n",
      "batch_idx:  3000, valid_loss: 9.582092\n",
      "batch_idx:  4000, valid_loss: 9.474936\n",
      "batch_idx:  5000, valid_loss: 9.234024\n",
      "batch_idx:  6000, valid_loss: 8.888880\n",
      "Training Loss: 4.070639 \tValidation Loss: 8.390143\n",
      "epoch:   446\n",
      "batch_idx:     0, train_loss: 3.997517\n",
      "batch_idx:  1000, train_loss: 4.023223\n",
      "batch_idx:  2000, train_loss: 3.433164\n",
      "batch_idx:  3000, train_loss: 3.371717\n",
      "batch_idx:  4000, train_loss: 3.604934\n",
      "batch_idx:  5000, train_loss: 3.731492\n",
      "batch_idx:  6000, train_loss: 3.510128\n",
      "final train_loss: 3.695203\n",
      "batch_idx:     0, valid_loss: 5.043784\n",
      "batch_idx:  1000, valid_loss: 4.729569\n",
      "batch_idx:  2000, valid_loss: 4.787777\n",
      "batch_idx:  3000, valid_loss: 4.796335\n",
      "batch_idx:  4000, valid_loss: 4.763097\n",
      "batch_idx:  5000, valid_loss: 4.754406\n",
      "batch_idx:  6000, valid_loss: 4.850586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.695203 \tValidation Loss: 4.885957\n",
      "epoch:   447\n",
      "batch_idx:     0, train_loss: 5.043784\n",
      "batch_idx:  1000, train_loss: 4.490205\n",
      "batch_idx:  2000, train_loss: 4.599432\n",
      "batch_idx:  3000, train_loss: 4.649269\n",
      "batch_idx:  4000, train_loss: 4.667384\n",
      "batch_idx:  5000, train_loss: 4.701305\n",
      "batch_idx:  6000, train_loss: 4.811792\n",
      "final train_loss: 4.860440\n",
      "batch_idx:     0, valid_loss: 4.968073\n",
      "batch_idx:  1000, valid_loss: 4.808880\n",
      "batch_idx:  2000, valid_loss: 4.835978\n",
      "batch_idx:  3000, valid_loss: 4.824033\n",
      "batch_idx:  4000, valid_loss: 4.797229\n",
      "batch_idx:  5000, valid_loss: 4.782808\n",
      "batch_idx:  6000, valid_loss: 4.847041\n",
      "Training Loss: 4.860440 \tValidation Loss: 4.873149\n",
      "epoch:   448\n",
      "batch_idx:     0, train_loss: 4.968073\n",
      "batch_idx:  1000, train_loss: 4.563059\n",
      "batch_idx:  2000, train_loss: 4.641163\n",
      "batch_idx:  3000, train_loss: 4.672806\n",
      "batch_idx:  4000, train_loss: 4.694915\n",
      "batch_idx:  5000, train_loss: 4.723725\n",
      "batch_idx:  6000, train_loss: 4.495576\n",
      "final train_loss: 4.525809\n",
      "batch_idx:     0, valid_loss: 4.907709\n",
      "batch_idx:  1000, valid_loss: 4.823182\n",
      "batch_idx:  2000, valid_loss: 4.836110\n",
      "batch_idx:  3000, valid_loss: 4.812600\n",
      "batch_idx:  4000, valid_loss: 4.789275\n",
      "batch_idx:  5000, valid_loss: 4.772600\n",
      "batch_idx:  6000, valid_loss: 4.850016\n",
      "Training Loss: 4.525809 \tValidation Loss: 4.873768\n",
      "epoch:   449\n",
      "batch_idx:     0, train_loss: 4.907709\n",
      "batch_idx:  1000, train_loss: 4.575761\n",
      "batch_idx:  2000, train_loss: 4.640759\n",
      "batch_idx:  3000, train_loss: 4.663292\n",
      "batch_idx:  4000, train_loss: 4.688585\n",
      "batch_idx:  5000, train_loss: 4.716081\n",
      "batch_idx:  6000, train_loss: 4.812773\n",
      "final train_loss: 4.853799\n",
      "batch_idx:     0, valid_loss: 4.905526\n",
      "batch_idx:  1000, valid_loss: 4.863428\n",
      "batch_idx:  2000, valid_loss: 4.868387\n",
      "batch_idx:  3000, valid_loss: 4.839386\n",
      "batch_idx:  4000, valid_loss: 4.818559\n",
      "batch_idx:  5000, valid_loss: 4.799601\n",
      "batch_idx:  6000, valid_loss: 4.851503\n",
      "Training Loss: 4.853799 \tValidation Loss: 4.869980\n",
      "Training loss decreased (4.871344 --> 4.869980).  Saving model ...\n",
      "epoch:   450\n",
      "batch_idx:     0, train_loss: 4.905526\n",
      "batch_idx:  1000, train_loss: 4.613647\n",
      "batch_idx:  2000, train_loss: 4.669239\n",
      "batch_idx:  3000, train_loss: 4.685823\n",
      "batch_idx:  4000, train_loss: 4.711873\n",
      "batch_idx:  5000, train_loss: 4.736742\n",
      "batch_idx:  6000, train_loss: 4.814106\n",
      "final train_loss: 4.851844\n",
      "batch_idx:     0, valid_loss: 4.910206\n",
      "batch_idx:  1000, valid_loss: 4.889900\n",
      "batch_idx:  2000, valid_loss: 4.891020\n",
      "batch_idx:  3000, valid_loss: 4.859145\n",
      "batch_idx:  4000, valid_loss: 4.839830\n",
      "batch_idx:  5000, valid_loss: 4.819705\n",
      "batch_idx:  6000, valid_loss: 4.854317\n",
      "Training Loss: 4.851844 \tValidation Loss: 4.869529\n",
      "Training loss decreased (4.869980 --> 4.869529).  Saving model ...\n",
      "epoch:   451\n",
      "batch_idx:     0, train_loss: 4.910206\n",
      "batch_idx:  1000, train_loss: 4.637870\n",
      "batch_idx:  2000, train_loss: 4.688796\n",
      "batch_idx:  3000, train_loss: 4.702044\n",
      "batch_idx:  4000, train_loss: 4.728441\n",
      "batch_idx:  5000, train_loss: 4.751738\n",
      "batch_idx:  6000, train_loss: 4.816215\n",
      "final train_loss: 4.851904\n",
      "batch_idx:     0, valid_loss: 4.915596\n",
      "batch_idx:  1000, valid_loss: 4.906818\n",
      "batch_idx:  2000, valid_loss: 4.906158\n",
      "batch_idx:  3000, valid_loss: 4.872780\n",
      "batch_idx:  4000, valid_loss: 4.854447\n",
      "batch_idx:  5000, valid_loss: 4.833786\n",
      "batch_idx:  6000, valid_loss: 4.856975\n",
      "Training Loss: 4.851904 \tValidation Loss: 4.870175\n",
      "epoch:   452\n",
      "batch_idx:     0, train_loss: 4.915596\n",
      "batch_idx:  1000, train_loss: 4.653536\n",
      "batch_idx:  2000, train_loss: 4.701924\n",
      "batch_idx:  3000, train_loss: 4.041949\n",
      "batch_idx:  4000, train_loss: 3.438027\n",
      "batch_idx:  5000, train_loss: 3.151308\n",
      "batch_idx:  6000, train_loss: 3.041545\n",
      "final train_loss: 3.006530\n",
      "batch_idx:     0, valid_loss: 10.100653\n",
      "batch_idx:  1000, valid_loss: 10.462846\n",
      "batch_idx:  2000, valid_loss: 10.491207\n",
      "batch_idx:  3000, valid_loss: 10.292681\n",
      "batch_idx:  4000, valid_loss: 10.040663\n",
      "batch_idx:  5000, valid_loss: 9.785229\n",
      "batch_idx:  6000, valid_loss: 9.461972\n",
      "Training Loss: 3.006530 \tValidation Loss: 9.074274\n",
      "epoch:   453\n",
      "batch_idx:     0, train_loss: 11.209468\n",
      "batch_idx:  1000, train_loss: 2.471187\n",
      "batch_idx:  2000, train_loss: 2.914837\n",
      "batch_idx:  3000, train_loss: 3.267274\n",
      "batch_idx:  4000, train_loss: 3.669851\n",
      "batch_idx:  5000, train_loss: 3.913205\n",
      "batch_idx:  6000, train_loss: 4.112042\n",
      "final train_loss: 4.219602\n",
      "batch_idx:     0, valid_loss: 4.966561\n",
      "batch_idx:  1000, valid_loss: 4.923040\n",
      "batch_idx:  2000, valid_loss: 4.879864\n",
      "batch_idx:  3000, valid_loss: 4.854791\n",
      "batch_idx:  4000, valid_loss: 4.853080\n",
      "batch_idx:  5000, valid_loss: 4.839541\n",
      "batch_idx:  6000, valid_loss: 4.856893\n",
      "Training Loss: 4.219602 \tValidation Loss: 4.870358\n",
      "epoch:   454\n",
      "batch_idx:     0, train_loss: 4.966561\n",
      "batch_idx:  1000, train_loss: 4.668397\n",
      "batch_idx:  2000, train_loss: 4.679272\n",
      "batch_idx:  3000, train_loss: 4.699412\n",
      "batch_idx:  4000, train_loss: 4.739225\n",
      "batch_idx:  5000, train_loss: 4.766463\n",
      "batch_idx:  6000, train_loss: 4.817945\n",
      "final train_loss: 4.852463\n",
      "batch_idx:     0, valid_loss: 4.946504\n",
      "batch_idx:  1000, valid_loss: 4.925178\n",
      "batch_idx:  2000, valid_loss: 4.900509\n",
      "batch_idx:  3000, valid_loss: 4.871896\n",
      "batch_idx:  4000, valid_loss: 4.863741\n",
      "batch_idx:  5000, valid_loss: 4.846970\n",
      "batch_idx:  6000, valid_loss: 4.858725\n",
      "Training Loss: 4.852463 \tValidation Loss: 4.870806\n",
      "epoch:   455\n",
      "batch_idx:     0, train_loss: 4.946504\n",
      "batch_idx:  1000, train_loss: 4.670480\n",
      "batch_idx:  2000, train_loss: 4.697060\n",
      "batch_idx:  3000, train_loss: 4.712874\n",
      "batch_idx:  4000, train_loss: 4.747169\n",
      "batch_idx:  5000, train_loss: 4.771846\n",
      "batch_idx:  6000, train_loss: 4.819348\n",
      "final train_loss: 4.853028\n",
      "batch_idx:     0, valid_loss: 4.937119\n",
      "batch_idx:  1000, valid_loss: 4.927929\n",
      "batch_idx:  2000, valid_loss: 4.913229\n",
      "batch_idx:  3000, valid_loss: 4.882195\n",
      "batch_idx:  4000, valid_loss: 4.870512\n",
      "batch_idx:  5000, valid_loss: 4.851956\n",
      "batch_idx:  6000, valid_loss: 4.860186\n",
      "Training Loss: 4.853028 \tValidation Loss: 4.871451\n",
      "epoch:   456\n",
      "batch_idx:     0, train_loss: 4.937119\n",
      "batch_idx:  1000, train_loss: 4.673077\n",
      "batch_idx:  2000, train_loss: 4.708040\n",
      "batch_idx:  3000, train_loss: 4.721025\n",
      "batch_idx:  4000, train_loss: 4.752246\n",
      "batch_idx:  5000, train_loss: 4.775465\n",
      "batch_idx:  6000, train_loss: 4.820469\n",
      "final train_loss: 4.853627\n",
      "batch_idx:     0, valid_loss: 4.932708\n",
      "batch_idx:  1000, valid_loss: 4.930134\n",
      "batch_idx:  2000, valid_loss: 4.920946\n",
      "batch_idx:  3000, valid_loss: 4.888314\n",
      "batch_idx:  4000, valid_loss: 4.874708\n",
      "batch_idx:  5000, valid_loss: 4.855163\n",
      "batch_idx:  6000, valid_loss: 4.861248\n",
      "Training Loss: 4.853627 \tValidation Loss: 4.872019\n",
      "epoch:   457\n",
      "batch_idx:     0, train_loss: 4.932708\n",
      "batch_idx:  1000, train_loss: 4.675143\n",
      "batch_idx:  2000, train_loss: 4.714731\n",
      "batch_idx:  3000, train_loss: 4.725944\n",
      "batch_idx:  4000, train_loss: 4.755447\n",
      "batch_idx:  5000, train_loss: 4.777823\n",
      "batch_idx:  6000, train_loss: 4.821232\n",
      "final train_loss: 4.854071\n",
      "batch_idx:     0, valid_loss: 4.930599\n",
      "batch_idx:  1000, valid_loss: 4.931693\n",
      "batch_idx:  2000, valid_loss: 4.925629\n",
      "batch_idx:  3000, valid_loss: 4.892011\n",
      "batch_idx:  4000, valid_loss: 4.877278\n",
      "batch_idx:  5000, valid_loss: 4.857180\n",
      "batch_idx:  6000, valid_loss: 4.861882\n",
      "Training Loss: 4.854071 \tValidation Loss: 4.872365\n",
      "epoch:   458\n",
      "batch_idx:     0, train_loss: 4.930599\n",
      "batch_idx:  1000, train_loss: 4.676610\n",
      "batch_idx:  2000, train_loss: 4.718792\n",
      "batch_idx:  3000, train_loss: 4.728919\n",
      "batch_idx:  4000, train_loss: 4.757407\n",
      "batch_idx:  5000, train_loss: 4.779285\n",
      "batch_idx:  6000, train_loss: 4.821709\n",
      "final train_loss: 4.854352\n",
      "batch_idx:     0, valid_loss: 4.929634\n",
      "batch_idx:  1000, valid_loss: 4.932756\n",
      "batch_idx:  2000, valid_loss: 4.928484\n",
      "batch_idx:  3000, valid_loss: 4.894251\n",
      "batch_idx:  4000, valid_loss: 4.878905\n",
      "batch_idx:  5000, valid_loss: 4.858467\n",
      "batch_idx:  6000, valid_loss: 4.862309\n",
      "Training Loss: 4.854352 \tValidation Loss: 4.872613\n",
      "epoch:   459\n",
      "batch_idx:     0, train_loss: 4.929634\n",
      "batch_idx:  1000, train_loss: 4.677558\n",
      "batch_idx:  2000, train_loss: 4.721287\n",
      "batch_idx:  3000, train_loss: 4.730747\n",
      "batch_idx:  4000, train_loss: 4.758658\n",
      "batch_idx:  5000, train_loss: 4.780241\n",
      "batch_idx:  6000, train_loss: 4.822030\n",
      "final train_loss: 4.854553\n",
      "batch_idx:     0, valid_loss: 4.929205\n",
      "batch_idx:  1000, valid_loss: 4.933466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  2000, valid_loss: 4.930242\n",
      "batch_idx:  3000, valid_loss: 4.895600\n",
      "batch_idx:  4000, valid_loss: 4.879883\n",
      "batch_idx:  5000, valid_loss: 4.859248\n",
      "batch_idx:  6000, valid_loss: 4.862534\n",
      "Training Loss: 4.854553 \tValidation Loss: 4.872730\n",
      "epoch:   460\n",
      "batch_idx:     0, train_loss: 4.929205\n",
      "batch_idx:  1000, train_loss: 4.678244\n",
      "batch_idx:  2000, train_loss: 4.722815\n",
      "batch_idx:  3000, train_loss: 4.731845\n",
      "batch_idx:  4000, train_loss: 4.759413\n",
      "batch_idx:  5000, train_loss: 4.780818\n",
      "batch_idx:  6000, train_loss: 4.822202\n",
      "final train_loss: 4.854654\n",
      "batch_idx:     0, valid_loss: 4.929027\n",
      "batch_idx:  1000, valid_loss: 4.933933\n",
      "batch_idx:  2000, valid_loss: 4.931306\n",
      "batch_idx:  3000, valid_loss: 4.896435\n",
      "batch_idx:  4000, valid_loss: 4.880521\n",
      "batch_idx:  5000, valid_loss: 4.859796\n",
      "batch_idx:  6000, valid_loss: 4.862718\n",
      "Training Loss: 4.854654 \tValidation Loss: 4.872834\n",
      "epoch:   461\n",
      "batch_idx:     0, train_loss: 4.929027\n",
      "batch_idx:  1000, train_loss: 4.678658\n",
      "batch_idx:  2000, train_loss: 4.723736\n",
      "batch_idx:  3000, train_loss: 4.732519\n",
      "batch_idx:  4000, train_loss: 4.759895\n",
      "batch_idx:  5000, train_loss: 4.781204\n",
      "batch_idx:  6000, train_loss: 4.822334\n",
      "final train_loss: 4.854739\n",
      "batch_idx:     0, valid_loss: 4.928955\n",
      "batch_idx:  1000, valid_loss: 4.934237\n",
      "batch_idx:  2000, valid_loss: 4.931991\n",
      "batch_idx:  3000, valid_loss: 4.896966\n",
      "batch_idx:  4000, valid_loss: 4.880920\n",
      "batch_idx:  5000, valid_loss: 4.860137\n",
      "batch_idx:  6000, valid_loss: 4.862851\n",
      "Training Loss: 4.854739 \tValidation Loss: 4.872926\n",
      "epoch:   462\n",
      "batch_idx:     0, train_loss: 4.928955\n",
      "batch_idx:  1000, train_loss: 4.678943\n",
      "batch_idx:  2000, train_loss: 4.724333\n",
      "batch_idx:  3000, train_loss: 4.732938\n",
      "batch_idx:  4000, train_loss: 4.760190\n",
      "batch_idx:  5000, train_loss: 4.781443\n",
      "batch_idx:  6000, train_loss: 4.822385\n",
      "final train_loss: 4.854769\n",
      "batch_idx:     0, valid_loss: 4.928957\n",
      "batch_idx:  1000, valid_loss: 4.934454\n",
      "batch_idx:  2000, valid_loss: 4.932427\n",
      "batch_idx:  3000, valid_loss: 4.897304\n",
      "batch_idx:  4000, valid_loss: 4.881190\n",
      "batch_idx:  5000, valid_loss: 4.860343\n",
      "batch_idx:  6000, valid_loss: 4.862908\n",
      "Training Loss: 4.854769 \tValidation Loss: 4.872953\n",
      "epoch:   463\n",
      "batch_idx:     0, train_loss: 4.928957\n",
      "batch_idx:  1000, train_loss: 4.679244\n",
      "batch_idx:  2000, train_loss: 4.724757\n",
      "batch_idx:  3000, train_loss: 4.733234\n",
      "batch_idx:  4000, train_loss: 4.760428\n",
      "batch_idx:  5000, train_loss: 4.781634\n",
      "batch_idx:  6000, train_loss: 4.822464\n",
      "final train_loss: 4.642166\n",
      "batch_idx:     0, valid_loss: 6.892663\n",
      "batch_idx:  1000, valid_loss: 12.293450\n",
      "batch_idx:  2000, valid_loss: 12.532398\n",
      "batch_idx:  3000, valid_loss: 12.632953\n",
      "batch_idx:  4000, valid_loss: 12.519794\n",
      "batch_idx:  5000, valid_loss: 12.353018\n",
      "batch_idx:  6000, valid_loss: 12.183868\n",
      "Training Loss: 4.642166 \tValidation Loss: 11.637039\n",
      "epoch:   464\n",
      "batch_idx:     0, train_loss: 8.151630\n",
      "batch_idx:  1000, train_loss: 2.626130\n",
      "batch_idx:  2000, train_loss: 2.815348\n",
      "batch_idx:  3000, train_loss: 3.143771\n",
      "batch_idx:  4000, train_loss: 3.534142\n",
      "batch_idx:  5000, train_loss: 3.776721\n",
      "batch_idx:  6000, train_loss: 3.969520\n",
      "final train_loss: 4.099265\n",
      "batch_idx:     0, valid_loss: 5.288501\n",
      "batch_idx:  1000, valid_loss: 5.094697\n",
      "batch_idx:  2000, valid_loss: 5.075364\n",
      "batch_idx:  3000, valid_loss: 5.001232\n",
      "batch_idx:  4000, valid_loss: 4.936954\n",
      "batch_idx:  5000, valid_loss: 4.886324\n",
      "batch_idx:  6000, valid_loss: 4.870012\n",
      "Training Loss: 4.099265 \tValidation Loss: 4.889555\n",
      "epoch:   465\n",
      "batch_idx:     0, train_loss: 5.288501\n",
      "batch_idx:  1000, train_loss: 4.826952\n",
      "batch_idx:  2000, train_loss: 4.847270\n",
      "batch_idx:  3000, train_loss: 4.816074\n",
      "batch_idx:  4000, train_loss: 4.802522\n",
      "batch_idx:  5000, train_loss: 4.801085\n",
      "batch_idx:  6000, train_loss: 4.828596\n",
      "final train_loss: 4.867670\n",
      "batch_idx:     0, valid_loss: 5.132617\n",
      "batch_idx:  1000, valid_loss: 5.022347\n",
      "batch_idx:  2000, valid_loss: 5.011683\n",
      "batch_idx:  3000, valid_loss: 4.954100\n",
      "batch_idx:  4000, valid_loss: 4.910746\n",
      "batch_idx:  5000, valid_loss: 4.872780\n",
      "batch_idx:  6000, valid_loss: 4.863807\n",
      "Training Loss: 4.867670 \tValidation Loss: 4.880796\n",
      "epoch:   466\n",
      "batch_idx:     0, train_loss: 5.132617\n",
      "batch_idx:  1000, train_loss: 4.760173\n",
      "batch_idx:  2000, train_loss: 4.792606\n",
      "batch_idx:  3000, train_loss: 4.778543\n",
      "batch_idx:  4000, train_loss: 4.782731\n",
      "batch_idx:  5000, train_loss: 4.791023\n",
      "batch_idx:  6000, train_loss: 4.823721\n",
      "final train_loss: 4.861081\n",
      "batch_idx:     0, valid_loss: 5.041288\n",
      "batch_idx:  1000, valid_loss: 4.982681\n",
      "batch_idx:  2000, valid_loss: 4.976789\n",
      "batch_idx:  3000, valid_loss: 4.928751\n",
      "batch_idx:  4000, valid_loss: 4.897052\n",
      "batch_idx:  5000, valid_loss: 4.866356\n",
      "batch_idx:  6000, valid_loss: 4.861696\n",
      "Training Loss: 4.861081 \tValidation Loss: 4.876757\n",
      "epoch:   467\n",
      "batch_idx:     0, train_loss: 5.041288\n",
      "batch_idx:  1000, train_loss: 4.723549\n",
      "batch_idx:  2000, train_loss: 4.762738\n",
      "batch_idx:  3000, train_loss: 4.758338\n",
      "batch_idx:  4000, train_loss: 4.772387\n",
      "batch_idx:  5000, train_loss: 4.786158\n",
      "batch_idx:  6000, train_loss: 4.822004\n",
      "final train_loss: 4.857995\n",
      "batch_idx:     0, valid_loss: 4.989889\n",
      "batch_idx:  1000, valid_loss: 4.961052\n",
      "batch_idx:  2000, valid_loss: 4.957459\n",
      "batch_idx:  3000, valid_loss: 4.914919\n",
      "batch_idx:  4000, valid_loss: 4.889802\n",
      "batch_idx:  5000, valid_loss: 4.863233\n",
      "batch_idx:  6000, valid_loss: 4.861249\n",
      "Training Loss: 4.857995 \tValidation Loss: 4.874893\n",
      "epoch:   468\n",
      "batch_idx:     0, train_loss: 4.989889\n",
      "batch_idx:  1000, train_loss: 4.703638\n",
      "batch_idx:  2000, train_loss: 4.746198\n",
      "batch_idx:  3000, train_loss: 4.747342\n",
      "batch_idx:  4000, train_loss: 4.766951\n",
      "batch_idx:  5000, train_loss: 4.783835\n",
      "batch_idx:  6000, train_loss: 4.406973\n",
      "final train_loss: 4.271610\n",
      "batch_idx:     0, valid_loss: 9.188921\n",
      "batch_idx:  1000, valid_loss: 9.474851\n",
      "batch_idx:  2000, valid_loss: 9.452965\n",
      "batch_idx:  3000, valid_loss: 9.270427\n",
      "batch_idx:  4000, valid_loss: 9.098610\n",
      "batch_idx:  5000, valid_loss: 8.879658\n",
      "batch_idx:  6000, valid_loss: 8.541348\n",
      "Training Loss: 4.271610 \tValidation Loss: 8.171082\n",
      "epoch:   469\n",
      "batch_idx:     0, train_loss: 11.584516\n",
      "batch_idx:  1000, train_loss: 2.533817\n",
      "batch_idx:  2000, train_loss: 2.792043\n",
      "batch_idx:  3000, train_loss: 2.932892\n",
      "batch_idx:  4000, train_loss: 3.020654\n",
      "batch_idx:  5000, train_loss: 3.055777\n",
      "batch_idx:  6000, train_loss: 2.878617\n",
      "final train_loss: 3.083581\n",
      "batch_idx:     0, valid_loss: 4.967562\n",
      "batch_idx:  1000, valid_loss: 4.917755\n",
      "batch_idx:  2000, valid_loss: 4.900464\n",
      "batch_idx:  3000, valid_loss: 4.851426\n",
      "batch_idx:  4000, valid_loss: 4.825753\n",
      "batch_idx:  5000, valid_loss: 4.797976\n",
      "batch_idx:  6000, valid_loss: 4.860191\n",
      "Training Loss: 3.083581 \tValidation Loss: 4.874947\n",
      "epoch:   470\n",
      "batch_idx:     0, train_loss: 4.967562\n",
      "batch_idx:  1000, train_loss: 4.663026\n",
      "batch_idx:  2000, train_loss: 4.696051\n",
      "batch_idx:  3000, train_loss: 4.694965\n",
      "batch_idx:  4000, train_loss: 4.717090\n",
      "batch_idx:  5000, train_loss: 4.735714\n",
      "batch_idx:  6000, train_loss: 4.375913\n",
      "final train_loss: 4.182000\n",
      "batch_idx:     0, valid_loss: 5.706697\n",
      "batch_idx:  1000, valid_loss: 13.790725\n",
      "batch_idx:  2000, valid_loss: 14.458479\n",
      "batch_idx:  3000, valid_loss: 14.615553\n",
      "batch_idx:  4000, valid_loss: 14.507277\n",
      "batch_idx:  5000, valid_loss: 14.224074\n",
      "batch_idx:  6000, valid_loss: 13.833785\n",
      "Training Loss: 4.182000 \tValidation Loss: 13.165661\n",
      "epoch:   471\n",
      "batch_idx:     0, train_loss: 4.909824\n",
      "batch_idx:  1000, train_loss: 3.757581\n",
      "batch_idx:  2000, train_loss: 4.212132\n",
      "batch_idx:  3000, train_loss: 4.353825\n",
      "batch_idx:  4000, train_loss: 4.449728\n",
      "batch_idx:  5000, train_loss: 4.513675\n",
      "batch_idx:  6000, train_loss: 4.429110\n",
      "final train_loss: 4.248574\n",
      "batch_idx:     0, valid_loss: 10.320820\n",
      "batch_idx:  1000, valid_loss: 10.600233\n",
      "batch_idx:  2000, valid_loss: 10.603230\n",
      "batch_idx:  3000, valid_loss: 10.616120\n",
      "batch_idx:  4000, valid_loss: 10.466908\n",
      "batch_idx:  5000, valid_loss: 10.169333\n",
      "batch_idx:  6000, valid_loss: 9.878490\n",
      "Training Loss: 4.248574 \tValidation Loss: 9.489433\n",
      "epoch:   472\n",
      "batch_idx:     0, train_loss: 12.840500\n",
      "batch_idx:  1000, train_loss: 2.182390\n",
      "batch_idx:  2000, train_loss: 2.406031\n",
      "batch_idx:  3000, train_loss: 2.651290\n",
      "batch_idx:  4000, train_loss: 2.757616\n",
      "batch_idx:  5000, train_loss: 2.484365\n",
      "batch_idx:  6000, train_loss: 2.525793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final train_loss: 2.611453\n",
      "batch_idx:     0, valid_loss: 10.556499\n",
      "batch_idx:  1000, valid_loss: 11.469547\n",
      "batch_idx:  2000, valid_loss: 11.249238\n",
      "batch_idx:  3000, valid_loss: 10.935770\n",
      "batch_idx:  4000, valid_loss: 10.688326\n",
      "batch_idx:  5000, valid_loss: 10.413344\n",
      "batch_idx:  6000, valid_loss: 10.005546\n",
      "Training Loss: 2.611453 \tValidation Loss: 9.572366\n",
      "epoch:   473\n",
      "batch_idx:     0, train_loss: 15.978015\n",
      "batch_idx:  1000, train_loss: 2.915412\n",
      "batch_idx:  2000, train_loss: 3.114439\n",
      "batch_idx:  3000, train_loss: 2.631025\n",
      "batch_idx:  4000, train_loss: 2.408993\n",
      "batch_idx:  5000, train_loss: 2.592592\n",
      "batch_idx:  6000, train_loss: 2.672423\n",
      "final train_loss: 2.742848\n",
      "batch_idx:     0, valid_loss: 9.985999\n",
      "batch_idx:  1000, valid_loss: 10.097927\n",
      "batch_idx:  2000, valid_loss: 10.031682\n",
      "batch_idx:  3000, valid_loss: 9.786434\n",
      "batch_idx:  4000, valid_loss: 9.592085\n",
      "batch_idx:  5000, valid_loss: 9.385864\n",
      "batch_idx:  6000, valid_loss: 9.044003\n",
      "Training Loss: 2.742848 \tValidation Loss: 8.653225\n",
      "epoch:   474\n",
      "batch_idx:     0, train_loss: 11.155260\n",
      "batch_idx:  1000, train_loss: 2.638052\n",
      "batch_idx:  2000, train_loss: 3.020487\n",
      "batch_idx:  3000, train_loss: 2.878968\n",
      "batch_idx:  4000, train_loss: 2.706937\n",
      "batch_idx:  5000, train_loss: 2.649626\n",
      "batch_idx:  6000, train_loss: 2.678978\n",
      "final train_loss: 2.709856\n",
      "batch_idx:     0, valid_loss: 10.203449\n",
      "batch_idx:  1000, valid_loss: 10.541477\n",
      "batch_idx:  2000, valid_loss: 10.350657\n",
      "batch_idx:  3000, valid_loss: 10.160842\n",
      "batch_idx:  4000, valid_loss: 9.945173\n",
      "batch_idx:  5000, valid_loss: 9.721138\n",
      "batch_idx:  6000, valid_loss: 9.382300\n",
      "Training Loss: 2.709856 \tValidation Loss: 8.995489\n",
      "epoch:   475\n",
      "batch_idx:     0, train_loss: 13.098164\n",
      "batch_idx:  1000, train_loss: 2.530069\n",
      "batch_idx:  2000, train_loss: 2.325699\n",
      "batch_idx:  3000, train_loss: 2.308995\n",
      "batch_idx:  4000, train_loss: 2.666266\n",
      "batch_idx:  5000, train_loss: 2.689722\n",
      "batch_idx:  6000, train_loss: 2.798576\n",
      "final train_loss: 2.859696\n",
      "batch_idx:     0, valid_loss: 8.955969\n",
      "batch_idx:  1000, valid_loss: 8.777985\n",
      "batch_idx:  2000, valid_loss: 8.556321\n",
      "batch_idx:  3000, valid_loss: 8.355684\n",
      "batch_idx:  4000, valid_loss: 8.221917\n",
      "batch_idx:  5000, valid_loss: 8.031583\n",
      "batch_idx:  6000, valid_loss: 7.739118\n",
      "Training Loss: 2.859696 \tValidation Loss: 7.398593\n",
      "epoch:   476\n",
      "batch_idx:     0, train_loss: 5.065948\n",
      "batch_idx:  1000, train_loss: 2.423142\n",
      "batch_idx:  2000, train_loss: 2.142750\n",
      "batch_idx:  3000, train_loss: 1.631296\n",
      "batch_idx:  4000, train_loss: 1.403374\n",
      "batch_idx:  5000, train_loss: 1.374417\n",
      "batch_idx:  6000, train_loss: 1.707294\n",
      "final train_loss: 1.764664\n",
      "batch_idx:     0, valid_loss: 7.979634\n",
      "batch_idx:  1000, valid_loss: 8.803190\n",
      "batch_idx:  2000, valid_loss: 9.035373\n",
      "batch_idx:  3000, valid_loss: 9.923401\n",
      "batch_idx:  4000, valid_loss: 10.244745\n",
      "batch_idx:  5000, valid_loss: 10.398948\n",
      "batch_idx:  6000, valid_loss: 10.204526\n",
      "Training Loss: 1.764664 \tValidation Loss: 9.752550\n",
      "epoch:   477\n",
      "batch_idx:     0, train_loss: 10.947474\n",
      "batch_idx:  1000, train_loss: 1.783031\n",
      "batch_idx:  2000, train_loss: 1.646981\n",
      "batch_idx:  3000, train_loss: 1.482809\n",
      "batch_idx:  4000, train_loss: 1.654966\n",
      "batch_idx:  5000, train_loss: 1.664762\n",
      "batch_idx:  6000, train_loss: 1.749525\n",
      "final train_loss: 1.802569\n",
      "batch_idx:     0, valid_loss: 9.163848\n",
      "batch_idx:  1000, valid_loss: 9.010508\n",
      "batch_idx:  2000, valid_loss: 9.395711\n",
      "batch_idx:  3000, valid_loss: 10.624181\n",
      "batch_idx:  4000, valid_loss: 11.139306\n",
      "batch_idx:  5000, valid_loss: 11.414553\n",
      "batch_idx:  6000, valid_loss: 11.326268\n",
      "Training Loss: 1.802569 \tValidation Loss: 10.801991\n",
      "epoch:   478\n",
      "batch_idx:     0, train_loss: 4.860578\n",
      "batch_idx:  1000, train_loss: 1.637502\n",
      "batch_idx:  2000, train_loss: 1.355295\n",
      "batch_idx:  3000, train_loss: 1.419257\n",
      "batch_idx:  4000, train_loss: 1.520728\n",
      "batch_idx:  5000, train_loss: 1.733510\n",
      "batch_idx:  6000, train_loss: 1.778334\n",
      "final train_loss: 1.842775\n",
      "batch_idx:     0, valid_loss: 8.228992\n",
      "batch_idx:  1000, valid_loss: 8.924900\n",
      "batch_idx:  2000, valid_loss: 11.224339\n",
      "batch_idx:  3000, valid_loss: 11.816155\n",
      "batch_idx:  4000, valid_loss: 12.057264\n",
      "batch_idx:  5000, valid_loss: 12.044958\n",
      "batch_idx:  6000, valid_loss: 11.818852\n",
      "Training Loss: 1.842775 \tValidation Loss: 11.360460\n",
      "epoch:   479\n",
      "batch_idx:     0, train_loss: 10.055700\n",
      "batch_idx:  1000, train_loss: 1.731706\n",
      "batch_idx:  2000, train_loss: 2.286493\n",
      "batch_idx:  3000, train_loss: 2.241420\n",
      "batch_idx:  4000, train_loss: 2.302945\n",
      "batch_idx:  5000, train_loss: 2.193628\n",
      "batch_idx:  6000, train_loss: 2.249671\n",
      "final train_loss: 2.494740\n",
      "batch_idx:     0, valid_loss: 4.898751\n",
      "batch_idx:  1000, valid_loss: 4.796503\n",
      "batch_idx:  2000, valid_loss: 4.802399\n",
      "batch_idx:  3000, valid_loss: 4.853521\n",
      "batch_idx:  4000, valid_loss: 4.837454\n",
      "batch_idx:  5000, valid_loss: 4.845944\n",
      "batch_idx:  6000, valid_loss: 4.873762\n",
      "Training Loss: 2.494740 \tValidation Loss: 4.879468\n",
      "epoch:   480\n",
      "batch_idx:     0, train_loss: 4.898751\n",
      "batch_idx:  1000, train_loss: 4.551980\n",
      "batch_idx:  2000, train_loss: 3.350275\n",
      "batch_idx:  3000, train_loss: 2.972089\n",
      "batch_idx:  4000, train_loss: 2.609252\n",
      "batch_idx:  5000, train_loss: 2.446714\n",
      "batch_idx:  6000, train_loss: 2.407753\n",
      "final train_loss: 2.370361\n",
      "batch_idx:     0, valid_loss: 7.804512\n",
      "batch_idx:  1000, valid_loss: 8.040713\n",
      "batch_idx:  2000, valid_loss: 10.818219\n",
      "batch_idx:  3000, valid_loss: 11.592043\n",
      "batch_idx:  4000, valid_loss: 11.973378\n",
      "batch_idx:  5000, valid_loss: 12.068207\n",
      "batch_idx:  6000, valid_loss: 11.930094\n",
      "Training Loss: 2.370361 \tValidation Loss: 11.460751\n",
      "epoch:   481\n",
      "batch_idx:     0, train_loss: 10.480780\n",
      "batch_idx:  1000, train_loss: 1.533280\n",
      "batch_idx:  2000, train_loss: 1.329707\n",
      "batch_idx:  3000, train_loss: 1.207014\n",
      "batch_idx:  4000, train_loss: 1.086104\n",
      "batch_idx:  5000, train_loss: 1.094497\n",
      "batch_idx:  6000, train_loss: 1.232224\n",
      "final train_loss: 1.311003\n",
      "batch_idx:     0, valid_loss: 7.594812\n",
      "batch_idx:  1000, valid_loss: 7.551292\n",
      "batch_idx:  2000, valid_loss: 10.487925\n",
      "batch_idx:  3000, valid_loss: 11.156959\n",
      "batch_idx:  4000, valid_loss: 11.872167\n",
      "batch_idx:  5000, valid_loss: 12.174555\n",
      "batch_idx:  6000, valid_loss: 12.145488\n",
      "Training Loss: 1.311003 \tValidation Loss: 11.807745\n",
      "epoch:   482\n",
      "batch_idx:     0, train_loss: 9.708864\n",
      "batch_idx:  1000, train_loss: 1.416021\n",
      "batch_idx:  2000, train_loss: 1.479330\n",
      "batch_idx:  3000, train_loss: 1.339418\n",
      "batch_idx:  4000, train_loss: 1.263343\n",
      "batch_idx:  5000, train_loss: 1.382350\n",
      "batch_idx:  6000, train_loss: 1.462498\n",
      "final train_loss: 1.570740\n",
      "batch_idx:     0, valid_loss: 9.103898\n",
      "batch_idx:  1000, valid_loss: 10.084310\n",
      "batch_idx:  2000, valid_loss: 9.919311\n",
      "batch_idx:  3000, valid_loss: 9.813675\n",
      "batch_idx:  4000, valid_loss: 9.926262\n",
      "batch_idx:  5000, valid_loss: 9.849943\n",
      "batch_idx:  6000, valid_loss: 9.570767\n",
      "Training Loss: 1.570740 \tValidation Loss: 9.188793\n",
      "epoch:   483\n",
      "batch_idx:     0, train_loss: 12.757253\n",
      "batch_idx:  1000, train_loss: 1.674747\n",
      "batch_idx:  2000, train_loss: 1.607471\n",
      "batch_idx:  3000, train_loss: 1.481869\n",
      "batch_idx:  4000, train_loss: 1.415142\n",
      "batch_idx:  5000, train_loss: 1.684459\n",
      "batch_idx:  6000, train_loss: 1.922047\n",
      "final train_loss: 1.915029\n",
      "batch_idx:     0, valid_loss: 11.503088\n",
      "batch_idx:  1000, valid_loss: 17.143248\n",
      "batch_idx:  2000, valid_loss: 18.016079\n",
      "batch_idx:  3000, valid_loss: 18.213623\n",
      "batch_idx:  4000, valid_loss: 18.328230\n",
      "batch_idx:  5000, valid_loss: 18.060472\n",
      "batch_idx:  6000, valid_loss: 17.475344\n",
      "Training Loss: 1.915029 \tValidation Loss: 16.665918\n",
      "epoch:   484\n",
      "batch_idx:     0, train_loss: 6.908249\n",
      "batch_idx:  1000, train_loss: 1.274526\n",
      "batch_idx:  2000, train_loss: 1.331776\n",
      "batch_idx:  3000, train_loss: 1.228735\n",
      "batch_idx:  4000, train_loss: 1.254039\n",
      "batch_idx:  5000, train_loss: 1.151405\n",
      "batch_idx:  6000, train_loss: 1.212340\n",
      "final train_loss: 1.234030\n",
      "batch_idx:     0, valid_loss: 10.532190\n",
      "batch_idx:  1000, valid_loss: 15.806764\n",
      "batch_idx:  2000, valid_loss: 16.484272\n",
      "batch_idx:  3000, valid_loss: 17.127594\n",
      "batch_idx:  4000, valid_loss: 17.446619\n",
      "batch_idx:  5000, valid_loss: 17.287664\n",
      "batch_idx:  6000, valid_loss: 16.800320\n",
      "Training Loss: 1.234030 \tValidation Loss: 16.012682\n",
      "epoch:   485\n",
      "batch_idx:     0, train_loss: 15.044306\n",
      "batch_idx:  1000, train_loss: 1.130901\n",
      "batch_idx:  2000, train_loss: 1.384645\n",
      "batch_idx:  3000, train_loss: 1.487621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  4000, train_loss: 1.625055\n",
      "batch_idx:  5000, train_loss: 1.822983\n",
      "batch_idx:  6000, train_loss: 1.801986\n",
      "final train_loss: 1.828614\n",
      "batch_idx:     0, valid_loss: 6.507194\n",
      "batch_idx:  1000, valid_loss: 12.669589\n",
      "batch_idx:  2000, valid_loss: 12.764390\n",
      "batch_idx:  3000, valid_loss: 12.802412\n",
      "batch_idx:  4000, valid_loss: 12.759377\n",
      "batch_idx:  5000, valid_loss: 12.431860\n",
      "batch_idx:  6000, valid_loss: 12.033378\n",
      "Training Loss: 1.828614 \tValidation Loss: 11.511001\n",
      "epoch:   486\n",
      "batch_idx:     0, train_loss: 6.687698\n",
      "batch_idx:  1000, train_loss: 2.208221\n",
      "batch_idx:  2000, train_loss: 2.048616\n",
      "batch_idx:  3000, train_loss: 2.241879\n",
      "batch_idx:  4000, train_loss: 2.218773\n",
      "batch_idx:  5000, train_loss: 2.275558\n",
      "batch_idx:  6000, train_loss: 2.417292\n",
      "final train_loss: 2.501100\n",
      "batch_idx:     0, valid_loss: 11.183419\n",
      "batch_idx:  1000, valid_loss: 11.216391\n",
      "batch_idx:  2000, valid_loss: 11.030764\n",
      "batch_idx:  3000, valid_loss: 10.781239\n",
      "batch_idx:  4000, valid_loss: 10.535537\n",
      "batch_idx:  5000, valid_loss: 10.301156\n",
      "batch_idx:  6000, valid_loss: 9.936094\n",
      "Training Loss: 2.501100 \tValidation Loss: 9.517500\n",
      "epoch:   487\n",
      "batch_idx:     0, train_loss: 4.709775\n",
      "batch_idx:  1000, train_loss: 3.103628\n",
      "batch_idx:  2000, train_loss: 2.715727\n",
      "batch_idx:  3000, train_loss: 2.482430\n",
      "batch_idx:  4000, train_loss: 2.398616\n",
      "batch_idx:  5000, train_loss: 2.523204\n",
      "batch_idx:  6000, train_loss: 2.411621\n",
      "final train_loss: 2.331828\n",
      "batch_idx:     0, valid_loss: 7.941820\n",
      "batch_idx:  1000, valid_loss: 8.888653\n",
      "batch_idx:  2000, valid_loss: 9.051143\n",
      "batch_idx:  3000, valid_loss: 9.115870\n",
      "batch_idx:  4000, valid_loss: 9.201411\n",
      "batch_idx:  5000, valid_loss: 9.160425\n",
      "batch_idx:  6000, valid_loss: 9.169881\n",
      "Training Loss: 2.331828 \tValidation Loss: 8.926555\n",
      "epoch:   488\n",
      "batch_idx:     0, train_loss: 11.933602\n",
      "batch_idx:  1000, train_loss: 1.573550\n",
      "batch_idx:  2000, train_loss: 1.780091\n",
      "batch_idx:  3000, train_loss: 1.894086\n",
      "batch_idx:  4000, train_loss: 1.952878\n",
      "batch_idx:  5000, train_loss: 1.982635\n",
      "batch_idx:  6000, train_loss: 1.923098\n",
      "final train_loss: 1.846993\n",
      "batch_idx:     0, valid_loss: 8.845656\n",
      "batch_idx:  1000, valid_loss: 17.024178\n",
      "batch_idx:  2000, valid_loss: 17.500189\n",
      "batch_idx:  3000, valid_loss: 17.618793\n",
      "batch_idx:  4000, valid_loss: 17.605616\n",
      "batch_idx:  5000, valid_loss: 17.463980\n",
      "batch_idx:  6000, valid_loss: 17.149826\n",
      "Training Loss: 1.846993 \tValidation Loss: 16.477367\n",
      "epoch:   489\n",
      "batch_idx:     0, train_loss: 6.661695\n",
      "batch_idx:  1000, train_loss: 2.561278\n",
      "batch_idx:  2000, train_loss: 2.823877\n",
      "batch_idx:  3000, train_loss: 2.869417\n",
      "batch_idx:  4000, train_loss: 2.887157\n",
      "batch_idx:  5000, train_loss: 2.718297\n",
      "batch_idx:  6000, train_loss: 2.397196\n",
      "final train_loss: 2.200984\n",
      "batch_idx:     0, valid_loss: 6.048684\n",
      "batch_idx:  1000, valid_loss: 15.981366\n",
      "batch_idx:  2000, valid_loss: 18.126083\n",
      "batch_idx:  3000, valid_loss: 21.106367\n",
      "batch_idx:  4000, valid_loss: 23.208757\n",
      "batch_idx:  5000, valid_loss: 24.248688\n",
      "batch_idx:  6000, valid_loss: 23.982767\n",
      "Training Loss: 2.200984 \tValidation Loss: 22.681797\n",
      "epoch:   490\n",
      "batch_idx:     0, train_loss: 5.990013\n",
      "batch_idx:  1000, train_loss: 0.348583\n",
      "batch_idx:  2000, train_loss: 0.350864\n",
      "batch_idx:  3000, train_loss: 0.330822\n",
      "batch_idx:  4000, train_loss: 0.350985\n",
      "batch_idx:  5000, train_loss: 0.379843\n",
      "batch_idx:  6000, train_loss: 0.505908\n",
      "final train_loss: 0.630080\n",
      "batch_idx:     0, valid_loss: 5.097300\n",
      "batch_idx:  1000, valid_loss: 7.432980\n",
      "batch_idx:  2000, valid_loss: 8.579049\n",
      "batch_idx:  3000, valid_loss: 9.474666\n",
      "batch_idx:  4000, valid_loss: 10.298565\n",
      "batch_idx:  5000, valid_loss: 10.712192\n",
      "batch_idx:  6000, valid_loss: 10.543184\n",
      "Training Loss: 0.630080 \tValidation Loss: 9.994260\n",
      "epoch:   491\n",
      "batch_idx:     0, train_loss: 4.654823\n",
      "batch_idx:  1000, train_loss: 1.220795\n",
      "batch_idx:  2000, train_loss: 1.180351\n",
      "batch_idx:  3000, train_loss: 0.895696\n",
      "batch_idx:  4000, train_loss: 0.769832\n",
      "batch_idx:  5000, train_loss: 0.716041\n",
      "batch_idx:  6000, train_loss: 0.707576\n",
      "final train_loss: 0.732147\n",
      "batch_idx:     0, valid_loss: 10.209656\n",
      "batch_idx:  1000, valid_loss: 13.340306\n",
      "batch_idx:  2000, valid_loss: 16.250151\n",
      "batch_idx:  3000, valid_loss: 17.472786\n",
      "batch_idx:  4000, valid_loss: 18.063057\n",
      "batch_idx:  5000, valid_loss: 18.232080\n",
      "batch_idx:  6000, valid_loss: 17.999458\n",
      "Training Loss: 0.732147 \tValidation Loss: 17.292847\n",
      "epoch:   492\n",
      "batch_idx:     0, train_loss: 13.053104\n",
      "batch_idx:  1000, train_loss: 0.971880\n",
      "batch_idx:  2000, train_loss: 0.972480\n",
      "batch_idx:  3000, train_loss: 0.908369\n",
      "batch_idx:  4000, train_loss: 0.841398\n",
      "batch_idx:  5000, train_loss: 0.825774\n",
      "batch_idx:  6000, train_loss: 0.863378\n",
      "final train_loss: 0.899144\n",
      "batch_idx:     0, valid_loss: 5.309173\n",
      "batch_idx:  1000, valid_loss: 12.471741\n",
      "batch_idx:  2000, valid_loss: 13.208687\n",
      "batch_idx:  3000, valid_loss: 13.359015\n",
      "batch_idx:  4000, valid_loss: 13.300996\n",
      "batch_idx:  5000, valid_loss: 12.991650\n",
      "batch_idx:  6000, valid_loss: 12.618523\n",
      "Training Loss: 0.899144 \tValidation Loss: 12.087666\n",
      "epoch:   493\n",
      "batch_idx:     0, train_loss: 5.878071\n",
      "batch_idx:  1000, train_loss: 1.118559\n",
      "batch_idx:  2000, train_loss: 0.899992\n",
      "batch_idx:  3000, train_loss: 0.845514\n",
      "batch_idx:  4000, train_loss: 0.867879\n",
      "batch_idx:  5000, train_loss: 0.885500\n",
      "batch_idx:  6000, train_loss: 0.925665\n",
      "final train_loss: 0.939202\n",
      "batch_idx:     0, valid_loss: 7.340516\n",
      "batch_idx:  1000, valid_loss: 12.181729\n",
      "batch_idx:  2000, valid_loss: 13.870135\n",
      "batch_idx:  3000, valid_loss: 14.436174\n",
      "batch_idx:  4000, valid_loss: 14.852610\n",
      "batch_idx:  5000, valid_loss: 14.924793\n",
      "batch_idx:  6000, valid_loss: 14.560744\n",
      "Training Loss: 0.939202 \tValidation Loss: 13.891998\n",
      "epoch:   494\n",
      "batch_idx:     0, train_loss: 8.972794\n",
      "batch_idx:  1000, train_loss: 1.087512\n",
      "batch_idx:  2000, train_loss: 1.052948\n",
      "batch_idx:  3000, train_loss: 0.885889\n",
      "batch_idx:  4000, train_loss: 0.818121\n",
      "batch_idx:  5000, train_loss: 0.844416\n",
      "batch_idx:  6000, train_loss: 0.867189\n",
      "final train_loss: 0.937918\n",
      "batch_idx:     0, valid_loss: 8.089396\n",
      "batch_idx:  1000, valid_loss: 9.184422\n",
      "batch_idx:  2000, valid_loss: 11.930267\n",
      "batch_idx:  3000, valid_loss: 12.658257\n",
      "batch_idx:  4000, valid_loss: 13.038493\n",
      "batch_idx:  5000, valid_loss: 13.044205\n",
      "batch_idx:  6000, valid_loss: 12.789307\n",
      "Training Loss: 0.937918 \tValidation Loss: 12.136514\n",
      "epoch:   495\n",
      "batch_idx:     0, train_loss: 10.846125\n",
      "batch_idx:  1000, train_loss: 1.392713\n",
      "batch_idx:  2000, train_loss: 1.389824\n",
      "batch_idx:  3000, train_loss: 1.238799\n",
      "batch_idx:  4000, train_loss: 1.107114\n",
      "batch_idx:  5000, train_loss: 1.039905\n",
      "batch_idx:  6000, train_loss: 1.000376\n",
      "final train_loss: 1.033298\n",
      "batch_idx:     0, valid_loss: 13.722136\n",
      "batch_idx:  1000, valid_loss: 18.027685\n",
      "batch_idx:  2000, valid_loss: 21.801891\n",
      "batch_idx:  3000, valid_loss: 22.617992\n",
      "batch_idx:  4000, valid_loss: 22.955912\n",
      "batch_idx:  5000, valid_loss: 22.807947\n",
      "batch_idx:  6000, valid_loss: 22.177021\n",
      "Training Loss: 1.033298 \tValidation Loss: 21.037853\n",
      "epoch:   496\n",
      "batch_idx:     0, train_loss: 10.954863\n",
      "batch_idx:  1000, train_loss: 0.841962\n",
      "batch_idx:  2000, train_loss: 0.948444\n",
      "batch_idx:  3000, train_loss: 0.902887\n",
      "batch_idx:  4000, train_loss: 1.121183\n",
      "batch_idx:  5000, train_loss: 1.121325\n",
      "batch_idx:  6000, train_loss: 1.142258\n",
      "final train_loss: 1.139091\n",
      "batch_idx:     0, valid_loss: 6.121781\n",
      "batch_idx:  1000, valid_loss: 12.485330\n",
      "batch_idx:  2000, valid_loss: 14.179622\n",
      "batch_idx:  3000, valid_loss: 14.562931\n",
      "batch_idx:  4000, valid_loss: 14.668912\n",
      "batch_idx:  5000, valid_loss: 14.539495\n",
      "batch_idx:  6000, valid_loss: 14.170856\n",
      "Training Loss: 1.139091 \tValidation Loss: 13.478893\n",
      "epoch:   497\n",
      "batch_idx:     0, train_loss: 3.882118\n",
      "batch_idx:  1000, train_loss: 1.024954\n",
      "batch_idx:  2000, train_loss: 1.364280\n",
      "batch_idx:  3000, train_loss: 1.327184\n",
      "batch_idx:  4000, train_loss: 1.200354\n",
      "batch_idx:  5000, train_loss: 1.118200\n",
      "batch_idx:  6000, train_loss: 1.146234\n",
      "final train_loss: 1.197444\n",
      "batch_idx:     0, valid_loss: 6.152906\n",
      "batch_idx:  1000, valid_loss: 12.637053\n",
      "batch_idx:  2000, valid_loss: 12.984534\n",
      "batch_idx:  3000, valid_loss: 12.905226\n",
      "batch_idx:  4000, valid_loss: 12.812931\n",
      "batch_idx:  5000, valid_loss: 12.620181\n",
      "batch_idx:  6000, valid_loss: 12.310265\n",
      "Training Loss: 1.197444 \tValidation Loss: 11.789253\n",
      "epoch:   498\n",
      "batch_idx:     0, train_loss: 8.014534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  1000, train_loss: 1.580852\n",
      "batch_idx:  2000, train_loss: 1.571665\n",
      "batch_idx:  3000, train_loss: 1.702692\n",
      "batch_idx:  4000, train_loss: 1.756648\n",
      "batch_idx:  5000, train_loss: 1.807667\n",
      "batch_idx:  6000, train_loss: 1.851132\n",
      "final train_loss: 1.840090\n",
      "batch_idx:     0, valid_loss: 7.699742\n",
      "batch_idx:  1000, valid_loss: 12.679405\n",
      "batch_idx:  2000, valid_loss: 14.546448\n",
      "batch_idx:  3000, valid_loss: 14.933448\n",
      "batch_idx:  4000, valid_loss: 15.059837\n",
      "batch_idx:  5000, valid_loss: 14.978401\n",
      "batch_idx:  6000, valid_loss: 14.624858\n",
      "Training Loss: 1.840090 \tValidation Loss: 14.060402\n",
      "epoch:   499\n",
      "batch_idx:     0, train_loss: 9.915027\n",
      "batch_idx:  1000, train_loss: 1.122468\n",
      "batch_idx:  2000, train_loss: 1.307379\n",
      "batch_idx:  3000, train_loss: 1.363640\n",
      "batch_idx:  4000, train_loss: 1.236975\n",
      "batch_idx:  5000, train_loss: 1.319754\n",
      "batch_idx:  6000, train_loss: 1.485835\n",
      "final train_loss: 1.514948\n",
      "batch_idx:     0, valid_loss: 15.125368\n",
      "batch_idx:  1000, valid_loss: 11.022648\n",
      "batch_idx:  2000, valid_loss: 11.010059\n",
      "batch_idx:  3000, valid_loss: 10.991300\n",
      "batch_idx:  4000, valid_loss: 10.841997\n",
      "batch_idx:  5000, valid_loss: 10.646882\n",
      "batch_idx:  6000, valid_loss: 10.323002\n",
      "Training Loss: 1.514948 \tValidation Loss: 9.863569\n",
      "epoch:   500\n",
      "batch_idx:     0, train_loss: 18.332317\n",
      "batch_idx:  1000, train_loss: 2.224492\n",
      "batch_idx:  2000, train_loss: 2.151241\n",
      "batch_idx:  3000, train_loss: 1.747462\n",
      "batch_idx:  4000, train_loss: 1.548074\n",
      "batch_idx:  5000, train_loss: 1.512200\n",
      "batch_idx:  6000, train_loss: 1.488818\n",
      "final train_loss: 1.554606\n",
      "batch_idx:     0, valid_loss: 9.173332\n",
      "batch_idx:  1000, valid_loss: 11.466399\n",
      "batch_idx:  2000, valid_loss: 11.606187\n",
      "batch_idx:  3000, valid_loss: 11.707057\n",
      "batch_idx:  4000, valid_loss: 11.743828\n",
      "batch_idx:  5000, valid_loss: 11.737967\n",
      "batch_idx:  6000, valid_loss: 11.582641\n",
      "Training Loss: 1.554606 \tValidation Loss: 11.175035\n"
     ]
    }
   ],
   "source": [
    "# accomodate truncated image files in the database\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "    \n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf \n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        print ('epoch: {:5}'.format(epoch))\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "            # migrate data to proper device\n",
    "            data   = data.to(device=device)\n",
    "            target = torch.tensor([target],device=device)\n",
    "            \n",
    "            ## find the loss and update the model parameters accordingly\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            ## update running average for training loss\n",
    "            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "            \n",
    "            if ((batch_idx % 1000) == 0):\n",
    "                print ('batch_idx: {:5}, train_loss: {:.6f}'.format(batch_idx, train_loss))\n",
    "        \n",
    "        print ('final train_loss: {:.6f}'.format(train_loss))\n",
    "        \n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['valid']):\n",
    "            # move to GPU\n",
    "            data = data.to(device=device)\n",
    "            ## update the average validation loss\n",
    "            output = model(data)\n",
    "            target = torch.tensor([target],device=device)\n",
    "                \n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
    "            \n",
    "            if ((batch_idx % 1000) == 0):\n",
    "                print ('batch_idx: {:5}, valid_loss: {:.6f}'.format(batch_idx, valid_loss))\n",
    "            \n",
    "        # print training/validation statistics \n",
    "        print('Training Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(train_loss, valid_loss))\n",
    "        \n",
    "        ## TODO: save the model if validation loss has decreased\n",
    "        \n",
    "        if (valid_loss_min > valid_loss):\n",
    "            print('Training loss decreased ({:.6f} --> {:.6f}).  Saving model ...'\n",
    "                  .format(valid_loss_min, valid_loss))\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            valid_loss_min = valid_loss\n",
    "            \n",
    "    # return trained model\n",
    "    return model\n",
    "\n",
    "\n",
    "# train the model\n",
    "model_scratch = train(500, loaders_scratch, model_scratch, optimizer_scratch, \n",
    "                      criterion_scratch, (device=='cuda'), 'model_scratch.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Test the Model\n",
    "\n",
    "Try out your model on the test dataset of dog images.  Use the code cell below to calculate and print the test loss and accuracy.  Ensure that your test accuracy is greater than 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:   0, Test Accuracy:  0% ( 0/ 3)\n",
      "batch_idx: 100, Test Accuracy:  0% ( 0/303)\n",
      "batch_idx: 200, Test Accuracy:  0% ( 0/603)\n",
      "batch_idx: 300, Test Accuracy:  0% ( 0/903)\n",
      "batch_idx: 400, Test Accuracy:  0% ( 0/1203)\n",
      "batch_idx: 500, Test Accuracy:  0% ( 0/1503)\n",
      "batch_idx: 600, Test Accuracy:  0% ( 5/1803)\n",
      "batch_idx: 700, Test Accuracy:  0% ( 8/2103)\n",
      "batch_idx: 800, Test Accuracy:  0% ( 8/2403)\n",
      "Test Loss: 4.864292\n",
      "\n",
      "\n",
      "Test Accuracy:  0% ( 8/2508)\n"
     ]
    }
   ],
   "source": [
    "# accomodate truncated image files in the database\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "def test(loaders, model, criterion, use_cuda):\n",
    "\n",
    "    # monitor test loss and accuracy\n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    model.eval()\n",
    "    for batch_idx, (data, target) in enumerate(loaders['test']):\n",
    "        target = torch.tensor([target])\n",
    "        # move to GPU\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average test loss \n",
    "        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
    "        # convert output probabilities to predicted class\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        # compare predictions to true label\n",
    "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "        total += data.size(0)\n",
    "            \n",
    "        if ((batch_idx % 100) == 0):\n",
    "            print ('batch_idx: %3d, Test Accuracy: %2d%% (%2d/%2d)' % (batch_idx, 100. * correct / total, correct, total))\n",
    "            \n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
    "        100. * correct / total, correct, total))\n",
    "\n",
    "# load the model that got the best validation accuracy\n",
    "model_scratch.load_state_dict(torch.load('model_scratch.pt',map_location=device))\n",
    "\n",
    "# call test function    \n",
    "test(loaders_scratch, model_scratch, criterion_scratch, (device=='cuda'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step4'></a>\n",
    "## Step 4: Create a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "\n",
    "You will now use transfer learning to create a CNN that can identify dog breed from images.  Your CNN must attain at least 60% accuracy on the test set.\n",
    "\n",
    "### (IMPLEMENTATION) Specify Data Loaders for the Dog Dataset\n",
    "\n",
    "Use the code cell below to write three separate [data loaders](http://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader) for the training, validation, and test datasets of dog images (located at `dogImages/train`, `dogImages/valid`, and `dogImages/test`, respectively). \n",
    "\n",
    "If you like, **you are welcome to use the same data loaders from the previous step**, when you created a CNN from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Specify data loaders\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "### TODO: Write data loaders for training, validation, and test sets\n",
    "## Specify appropriate transforms, and batch_sizes\n",
    "        \n",
    "in_transform = transforms.Compose([\n",
    "                    transforms.RandomAffine(10,translate=(0.2,0.2)),\n",
    "                    transforms.RandomResizedCrop(224),\n",
    "                    transforms.ToTensor()])\n",
    "\n",
    "train_data = \"/data/dog_images/train\"\n",
    "valid_data = \"/data/dog_images/valid\"\n",
    "test_data  = \"/data/dog_images/test\"\n",
    "\n",
    "train_loader = datasets.ImageFolder(train_data, transform=in_transform)\n",
    "valid_loader = datasets.ImageFolder(train_data, transform=in_transform)\n",
    "test_loader  = datasets.ImageFolder(test_data,  transform=in_transform)\n",
    "\n",
    "loaders_transfer = {'train': train_loader, 'valid': valid_loader, 'test': test_loader}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Model Architecture\n",
    "\n",
    "Use transfer learning to create a CNN to classify dog breed.  Use the code cell below, and save your initialized model as the variable `model_transfer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "## TODO: Specify model architecture \n",
    "model_transfer = models.vgg16(pretrained=True)\n",
    "\n",
    "# freeze pretrained layers\n",
    "for param in model_transfer.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# set output layer\n",
    "model_transfer.classifier[6] = nn.Linear(4096,118)\n",
    "model_transfer.classifier[6].requires_grad = True\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    model_transfer = model_transfer.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 5:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  Describe why you think the architecture is suitable for the current problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__ \n",
    "I started by pulling in the VGG-16 classifier.  It's a good, general purpose classifier to start with.  After that, I froze the parameters for all but the last layer and set the output to 118 classifiers, the same as the number of dog breeds in the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Specify Loss Function and Optimizer\n",
    "\n",
    "Use the next code cell to specify a [loss function](http://pytorch.org/docs/master/nn.html#loss-functions) and [optimizer](http://pytorch.org/docs/master/optim.html).  Save the chosen loss function as `criterion_transfer`, and the optimizer as `optimizer_transfer` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_transfer = None\n",
    "optimizer_transfer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Train and Validate the Model\n",
    "\n",
    "Train and validate your model in the code cell below.  [Save the final model parameters](http://pytorch.org/docs/master/notes/serialization.html) at filepath `'model_transfer.pt'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "model_transfer = # train(n_epochs, loaders_transfer, model_transfer, optimizer_transfer, criterion_transfer, use_cuda, 'model_transfer.pt')\n",
    "\n",
    "# load the model that got the best validation accuracy (uncomment the line below)\n",
    "#model_transfer.load_state_dict(torch.load('model_transfer.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Test the Model\n",
    "\n",
    "Try out your model on the test dataset of dog images. Use the code cell below to calculate and print the test loss and accuracy.  Ensure that your test accuracy is greater than 60%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(loaders_transfer, model_transfer, criterion_transfer, use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Predict Dog Breed with the Model\n",
    "\n",
    "Write a function that takes an image path as input and returns the dog breed (`Affenpinscher`, `Afghan hound`, etc) that is predicted by your model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Write a function that takes a path to an image as input\n",
    "### and returns the dog breed that is predicted by the model.\n",
    "\n",
    "# list of class names by index, i.e. a name can be accessed like class_names[0]\n",
    "class_names = [item[4:].replace(\"_\", \" \") for item in data_transfer['train'].classes]\n",
    "\n",
    "def predict_breed_transfer(img_path):\n",
    "    # load the image and return the predicted breed\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step5'></a>\n",
    "## Step 5: Write your Algorithm\n",
    "\n",
    "Write an algorithm that accepts a file path to an image and first determines whether the image contains a human, dog, or neither.  Then,\n",
    "- if a __dog__ is detected in the image, return the predicted breed.\n",
    "- if a __human__ is detected in the image, return the resembling dog breed.\n",
    "- if __neither__ is detected in the image, provide output that indicates an error.\n",
    "\n",
    "You are welcome to write your own functions for detecting humans and dogs in images, but feel free to use the `face_detector` and `human_detector` functions developed above.  You are __required__ to use your CNN from Step 4 to predict dog breed.  \n",
    "\n",
    "Some sample output for our algorithm is provided below, but feel free to design your own user experience!\n",
    "\n",
    "![Sample Human Output](images/sample_human_output.png)\n",
    "\n",
    "\n",
    "### (IMPLEMENTATION) Write your Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Write your algorithm.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n",
    "def run_app(img_path):\n",
    "    ## handle cases for a human face, dog, and neither\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step6'></a>\n",
    "## Step 6: Test Your Algorithm\n",
    "\n",
    "In this section, you will take your new algorithm for a spin!  What kind of dog does the algorithm think that _you_ look like?  If you have a dog, does it predict your dog's breed accurately?  If you have a cat, does it mistakenly think that your cat is a dog?\n",
    "\n",
    "### (IMPLEMENTATION) Test Your Algorithm on Sample Images!\n",
    "\n",
    "Test your algorithm at least six images on your computer.  Feel free to use any images you like.  Use at least two human and two dog images.  \n",
    "\n",
    "__Question 6:__ Is the output better than you expected :) ?  Or worse :( ?  Provide at least three possible points of improvement for your algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__ (Three possible points for improvement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Execute your algorithm from Step 6 on\n",
    "## at least 6 images on your computer.\n",
    "## Feel free to use as many code cells as needed.\n",
    "\n",
    "## suggested code, below\n",
    "for file in np.hstack((human_files[:3], dog_files[:3])):\n",
    "    run_app(file)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
